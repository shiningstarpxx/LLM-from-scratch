# æ•°æ®åŠ è½½å™¨æ€§èƒ½ä¼˜åŒ–è¯¦è§£

## æ ¸å¿ƒå‚æ•°é…ç½®

```python
# é«˜æ€§èƒ½æ•°æ®åŠ è½½å™¨é…ç½®
dataloader = DataLoader(
    dataset,
    batch_size=32,
    num_workers=8,           # å¤šè¿›ç¨‹å¹¶è¡ŒåŠ è½½
    pin_memory=True,         # GPUè®­ç»ƒä¼˜åŒ–
    persistent_workers=True, # è·¨epochå¤ç”¨worker
    prefetch_factor=2,       # é¢„å–å€æ•°
    drop_last=True,          # ä¿æŒæ‰¹æ¬¡å¤§å°ä¸€è‡´
    shuffle=True             # è®­ç»ƒæ—¶æ‰“ä¹±æ•°æ®
)
```

## å‚æ•°è¯¦è§£

### 1. num_workers è®¾ç½®ç­–ç•¥
```python
def optimal_num_workers():
    cpu_count = os.cpu_count()
    # ç»éªŒå…¬å¼ï¼šCPUæ ¸å¿ƒæ•°çš„1-4å€
    return min(cpu_count * 2, 8)  # ä¸è¶…è¿‡8ä¸ª

# ä¸åŒåœºæ™¯çš„æ¨èå€¼
# å°æ•°æ®é›† (<1GB): 0-2 workers
# ä¸­ç­‰æ•°æ®é›† (1-10GB): 2-4 workers
# å¤§æ•°æ®é›† (>10GB): 4-8 workers
```

### 2. pin_memory ä½¿ç”¨åœºæ™¯
```python
# GPUè®­ç»ƒæ—¶å¿…é¡»å¼€å¯
if torch.cuda.is_available():
    dataloader = DataLoader(dataset, pin_memory=True)
else:
    # CPUè®­ç»ƒæ—¶ä¸å¼€å¯ï¼Œæµªè´¹å†…å­˜
    dataloader = DataLoader(dataset, pin_memory=False)
```

### 3. persistent_workers é…åˆæ¡ä»¶
```python
# persistent_workers=True çš„å‰ææ¡ä»¶
dataloader = DataLoader(
    dataset,
    num_workers=4,           # å¿…é¡» > 0
    persistent_workers=True, # åªæœ‰åœ¨å¤šè¿›ç¨‹æ—¶æœ‰æ•ˆ
    prefetch_factor=2        # å»ºè®®é…åˆé¢„å–
)
```

## æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
import time
import torch
from torch.utils.data import DataLoader, TensorDataset

def benchmark_dataloader_configs():
    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    dataset = TensorDataset(torch.randn(10000, 100))

    configs = [
        {"pin_memory": False, "persistent_workers": False},
        {"pin_memory": True, "persistent_workers": False},
        {"pin_memory": False, "persistent_workers": True},
        {"pin_memory": True, "persistent_workers": True}
    ]

    results = {}

    for config in configs:
        dataloader = DataLoader(
            dataset,
            batch_size=32,
            num_workers=4,
            **config
        )

        # æµ‹é‡åŠ è½½æ—¶é—´
        start_time = time.time()
        for batch in dataloader:
            pass
        end_time = time.time()

        config_name = f"pin={config['pin_memory']}, persist={config['persistent_workers']}"
        results[config_name] = end_time - start_time

    return results

# è¿è¡ŒåŸºå‡†æµ‹è¯•
if __name__ == "__main__":
    results = benchmark_dataloader_configs()
    for config, time_taken in results.items():
        print(f"{config}: {time_taken:.3f}s")
```

## å†…å­˜ä½¿ç”¨åˆ†æ

```python
def analyze_memory_usage():
    """åˆ†æä¸åŒé…ç½®çš„å†…å­˜ä½¿ç”¨æƒ…å†µ"""

    # å›ºå®šå†…å­˜ä½¿ç”¨é‡ä¼°ç®—
    def estimate_pinned_memory(batch_size, seq_len, hidden_dim):
        # å•ä¸ªæ ·æœ¬å¤§å° (float32 = 4 bytes)
        sample_size = seq_len * hidden_dim * 4
        # æ‰¹æ¬¡å¤§å°
        batch_memory = batch_size * sample_size
        # é¢„å–ç¼“å†²åŒº (prefetch_factor * num_workers)
        buffer_memory = batch_memory * 2 * 4  # å‡è®¾prefetch=2, workers=4

        return {
            "sample_size_mb": sample_size / 1024**2,
            "batch_memory_mb": batch_memory / 1024**2,
            "buffer_memory_mb": buffer_memory / 1024**2,
            "total_pinned_mb": buffer_memory / 1024**2
        }

    return estimate_pinned_memory(32, 512, 768)

# ç¤ºä¾‹ï¼šBERT-baseé…ç½®
memory_info = analyze_memory_usage()
print(f"å›ºå®šå†…å­˜ä½¿ç”¨: {memory_info['total_pinned_mb']:.1f} MB")
```

## æ•…éšœæ’æŸ¥æŒ‡å—

### å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

```python
# 1. Workeræ­»é”é—®é¢˜
# ç—‡çŠ¶ï¼šæ•°æ®åŠ è½½å™¨å¡ä½ï¼Œæ— å“åº”
# è§£å†³ï¼šæ£€æŸ¥æ•°æ®é¢„å¤„ç†æ˜¯å¦æœ‰å…±äº«èµ„æºç«äº‰

# 2. å†…å­˜æ³„æ¼
# ç—‡çŠ¶ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­å†…å­˜æŒç»­å¢é•¿
# è§£å†³ï¼šç¡®ä¿persistent_workers=Trueæ—¶æ­£ç¡®å¤„ç†èµ„æº

# 3. CPUåˆ©ç”¨ç‡ä½
# ç—‡çŠ¶ï¼šGPUç­‰å¾…æ•°æ®ï¼ŒCPUç©ºé—²
# è§£å†³ï¼šå¢åŠ num_workersæˆ–æ£€æŸ¥æ•°æ®é¢„å¤„ç†ç“¶é¢ˆ

def safe_data_loading(dataset, batch_size=32):
    """å®‰å…¨çš„æ•°æ®åŠ è½½é…ç½®"""

    # æ£€æµ‹å¯ç”¨èµ„æº
    cpu_count = os.cpu_count()
    gpu_available = torch.cuda.is_available()

    # å®‰å…¨é…ç½®
    config = {
        "batch_size": batch_size,
        "num_workers": min(cpu_count, 6),  # ä¿å®ˆè®¾ç½®
        "pin_memory": gpu_available,
        "persistent_workers": gpu_available,  # GPUè®­ç»ƒæ—¶å¯ç”¨
        "prefetch_factor": 2,
        "drop_last": True
    }

    # å°æ•°æ®é›†ç‰¹æ®Šå¤„ç†
    if len(dataset) < 1000:
        config["num_workers"] = 0
        config["pin_memory"] = False

    return DataLoader(dataset, **config)
```

## é«˜çº§ä¼˜åŒ–æŠ€å·§

```python
class OptimizedDataLoader:
    """ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨åŒ…è£…ç±»"""

    def __init__(self, dataset, batch_size, **kwargs):
        self.dataset = dataset
        self.batch_size = batch_size
        self.config = self._get_optimal_config(**kwargs)

    def _get_optimal_config(self, **kwargs):
        """æ ¹æ®ç³»ç»Ÿèµ„æºè‡ªåŠ¨ä¼˜åŒ–é…ç½®"""

        # ç³»ç»Ÿèµ„æºæ£€æµ‹
        cpu_count = os.cpu_count()
        memory_gb = psutil.virtual_memory().total / (1024**3)
        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3) if torch.cuda.is_available() else 0

        # åŠ¨æ€é…ç½®
        config = {
            "batch_size": self.batch_size,
            "num_workers": min(cpu_count, 8),
            "pin_memory": torch.cuda.is_available(),
            "persistent_workers": torch.cuda.is_available(),
            "prefetch_factor": 2 if memory_gb > 16 else 1
        }

        # å¤§å†…å­˜ç³»ç»Ÿå¯ä»¥æ›´æ¿€è¿›
        if memory_gb > 32:
            config["prefetch_factor"] = 3
            config["num_workers"] = min(cpu_count * 2, 12)

        # æ›´æ–°ç”¨æˆ·è‡ªå®šä¹‰é…ç½®
        config.update(kwargs)
        return config

    def get_dataloader(self):
        return DataLoader(self.dataset, **self.config)

    def benchmark(self, num_batches=100):
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        dataloader = self.get_dataloader()

        start_time = time.time()
        for i, batch in enumerate(dataloader):
            if i >= num_batches:
                break
        end_time = time.time()

        throughput = num_batches / (end_time - start_time)
        print(f"æ•°æ®åŠ è½½ååé‡: {throughput:.1f} batches/sec")
        return throughput

# ä½¿ç”¨ç¤ºä¾‹
optimized_loader = OptimizedDataLoader(
    dataset,
    batch_size=32,
    shuffle=True
)

dataloader = optimized_loader.get_dataloader()
optimized_loader.benchmark()
```

## æ€»ç»“

### pin_memory æ ¸å¿ƒè¦ç‚¹
- âœ… GPUè®­ç»ƒæ—¶å¿…é¡»å¼€å¯ï¼Œæ˜¾è‘—æå‡æ•°æ®ä¼ è¾“é€Ÿåº¦
- âŒ CPUè®­ç»ƒæ—¶ä¸è¦å¼€å¯ï¼Œæµªè´¹å†…å­˜èµ„æº
- âš ï¸ å¢åŠ ç³»ç»Ÿå†…å­˜å‹åŠ›ï¼Œéœ€è¦ç›‘æ§ç³»ç»Ÿå†…å­˜ä½¿ç”¨

### persistent_workers æ ¸å¿ƒè¦ç‚¹
- âœ… å¤šè¿›ç¨‹è®­ç»ƒæ—¶å¼€å¯ï¼Œæ¶ˆé™¤epoché—´å»¶è¿Ÿ
- âŒ num_workers=0æ—¶æ— æ•ˆ
- âš ï¸ éœ€è¦æ­£ç¡®å¤„ç†èµ„æºæ¸…ç†ï¼Œé¿å…å†…å­˜æ³„æ¼
- ğŸ¯ é…åˆprefetch_factorä½¿ç”¨æ•ˆæœæœ€ä½³

### å®è·µå»ºè®®
```python
# GPUè®­ç»ƒçš„æœ€ä½³å®è·µé…ç½®
dataloader = DataLoader(
    dataset,
    batch_size=32,
    num_workers=4,              # æ ¹æ®CPUè°ƒæ•´
    pin_memory=True,            # GPUå¿…é¡»
    persistent_workers=True,    # è·¨epochå¤ç”¨
    prefetch_factor=2,          # é¢„å–ä¼˜åŒ–
    drop_last=True,             # ä¿æŒæ‰¹æ¬¡ä¸€è‡´
    shuffle=True                # è®­ç»ƒæ—¶æ‰“ä¹±
)
```