# æ·±åº¦å­¦ä¹ é‡åŒ–æŠ€æœ¯å®Œå…¨è§£æ

## ğŸ¯ é‡åŒ– vs ç²¾åº¦è½¬æ¢ï¼šæœ¬è´¨åŒºåˆ«

### åŸºæœ¬æ¦‚å¿µå¯¹æ¯”

```python
# === ç²¾åº¦è½¬æ¢ ===
# FP32 â†’ FP16ï¼šä¿æŒæµ®ç‚¹ç‰¹æ€§ï¼Œé™ä½ç²¾åº¦
fp32_value = 3.14159265359
fp16_value = torch.float16(fp32_value)  # 3.140625
print(f"ç²¾åº¦è½¬æ¢: {fp32_value} â†’ {fp16_value}")
print("ç‰¹æ€§: ä»æ˜¯æµ®ç‚¹æ•°ï¼ŒåŠ¨æ€èŒƒå›´ç¼©å°ï¼Œç²¾åº¦é™ä½")

# === é‡åŒ– ===
# FP32 â†’ INT8ï¼šæ˜ å°„åˆ°æ•´æ•°ï¼Œå®Œå…¨ä¸åŒçš„æ•°å€¼ç³»ç»Ÿ
def quantize_to_int8(value, scale, zero_point):
    """é‡åŒ–å‡½æ•°ï¼šFP32 â†’ INT8"""
    return int(round(value / scale + zero_point))

# ç¤ºä¾‹ï¼šé‡åŒ–èŒƒå›´ [-1.0, 1.0] åˆ° INT8 [-128, 127]
scale = 2.0 / 255  # ç¼©æ”¾å› å­
zero_point = 128   # é›¶ç‚¹

fp32_value = 3.14159265359
clipped_value = max(-1.0, min(1.0, fp32_value))  # é™åˆ¶åˆ°é‡åŒ–èŒƒå›´
int8_value = quantize_to_int8(clipped_value, scale, zero_point)

print(f"é‡åŒ–: {fp32_value} â†’ {int8_value}")
print("ç‰¹æ€§: æ˜ å°„åˆ°æ•´æ•°ï¼Œæœ‰é™ç¦»æ•£å€¼ï¼Œéœ€è¦åé‡åŒ–æ¢å¤")
```

### æ ¸å¿ƒåŒºåˆ«æ€»ç»“

| ç‰¹æ€§ | ç²¾åº¦è½¬æ¢ (FP32â†’FP16) | é‡åŒ– (FP32â†’INT8) |
|------|---------------------|------------------|
| **æ•°å€¼ç±»å‹** | æµ®ç‚¹æ•° â†’ æµ®ç‚¹æ•° | æµ®ç‚¹æ•° â†’ æ•´æ•° |
| **åŠ¨æ€èŒƒå›´** | ç¼©å° (10^38 â†’ 10^4) | æœ‰é™ç¦»æ•£å€¼ |
| **ç²¾åº¦æŸå¤±** | ç²¾åº¦é™ä½ | èŒƒå›´å‹ç¼© + ç¦»æ•£åŒ– |
| **ç¡¬ä»¶æ”¯æŒ** | åŸç”Ÿæ”¯æŒ | éœ€è¦ä¸“ç”¨æŒ‡ä»¤ |
| **å†…å­˜èŠ‚çœ** | 50% | 75% |
| **è®¡ç®—åŠ é€Ÿ** | æœ‰é™ | æ˜¾è‘— |

## ğŸ”¬ é‡åŒ–çš„æ•°å­¦åŸç†

### 1. çº¿æ€§é‡åŒ–å…¬å¼

```python
def linear_quantize(tensor, dtype=torch.qint8):
    """çº¿æ€§é‡åŒ–çš„å®Œæ•´å®ç°"""

    # ç¡®å®šé‡åŒ–èŒƒå›´
    if dtype == torch.qint8:
        qmin, qmax = -128, 127
    elif dtype == torch.quint8:
        qmin, qmax = 0, 255

    # è®¡ç®—ç¼©æ”¾å› å­å’Œé›¶ç‚¹
    min_val, max_val = tensor.min().item(), tensor.max().item()

    # é˜²æ­¢é™¤é›¶
    if max_val == min_val:
        scale = 1.0
        zero_point = 0
    else:
        scale = (max_val - min_val) / (qmax - qmin)
        zero_point = qmin - round(min_val / scale)

        # ç¡®ä¿é›¶ç‚¹åœ¨æœ‰æ•ˆèŒƒå›´å†…
        zero_point = max(qmin, min(qmax, zero_point))

    # é‡åŒ–æ“ä½œ
    quantized = torch.round(tensor / scale + zero_point)
    quantized = torch.clamp(quantized, qmin, qmax)

    return quantized.to(torch.int8), scale, zero_point

def linear_dequantize(quantized, scale, zero_point):
    """åé‡åŒ–æ“ä½œ"""
    return (quantized.float() - zero_point) * scale

# ç¤ºä¾‹æ¼”ç¤º
if __name__ == "__main__":
    # åˆ›å»ºæµ‹è¯•å¼ é‡
    original = torch.randn(1000) * 2 + 1  # å‡å€¼1ï¼Œæ ‡å‡†å·®2

    # é‡åŒ–
    quantized, scale, zero_point = linear_quantize(original)

    # åé‡åŒ–
    dequantized = linear_dequantize(quantized, scale, zero_point)

    # è®¡ç®—è¯¯å·®
    mse_error = torch.mean((original - dequantized) ** 2).item()

    print(f"åŸå§‹èŒƒå›´: [{original.min():.3f}, {original.max():.3f}]")
    print(f"é‡åŒ–å‚æ•°: scale={scale:.6f}, zero_point={zero_point}")
    print(f"é‡åŒ–åèŒƒå›´: [{quantized.min()}, {quantized.max()}]")
    print(f"åé‡åŒ–èŒƒå›´: [{dequantized.min():.3f}, {dequantized.max():.3f}]")
    print(f"å‡æ–¹è¯¯å·®: {mse_error:.6f}")
```

### 2. å¯¹ç§°é‡åŒ– vs éå¯¹ç§°é‡åŒ–

```python
def symmetric_quantize(tensor, bits=8):
    """å¯¹ç§°é‡åŒ–ï¼šé›¶ç‚¹å›ºå®šä¸º0"""

    # è®¡ç®—æœ€å¤§ç»å¯¹å€¼
    max_abs = torch.max(torch.abs(tensor)).item()

    # ç¡®å®šé‡åŒ–èŒƒå›´
    if bits == 8:
        qmax = 127  # INT8èŒƒå›´ [-127, 127]
    elif bits == 4:
        qmax = 7    # INT4èŒƒå›´ [-7, 7]

    # è®¡ç®—ç¼©æ”¾å› å­ï¼ˆé›¶ç‚¹ä¸º0ï¼‰
    scale = max_abs / qmax if max_abs > 0 else 1.0

    # é‡åŒ–
    quantized = torch.round(tensor / scale)
    quantized = torch.clamp(quantized, -qmax, qmax)

    return quantized.to(torch.int8), scale

def asymmetric_quantize(tensor, bits=8):
    """éå¯¹ç§°é‡åŒ–ï¼šé›¶ç‚¹å¯è°ƒæ•´"""

    # ç¡®å®šé‡åŒ–èŒƒå›´
    if bits == 8:
        qmin, qmax = -128, 127
    elif bits == 4:
        qmin, qmax = -8, 7

    min_val, max_val = tensor.min().item(), tensor.max().item()

    # è®¡ç®—ç¼©æ”¾å› å­å’Œé›¶ç‚¹
    scale = (max_val - min_val) / (qmax - qmin) if max_val != min_val else 1.0
    zero_point = qmin - round(min_val / scale)
    zero_point = max(qmin, min(qmax, zero_point))

    # é‡åŒ–
    quantized = torch.round(tensor / scale + zero_point)
    quantized = torch.clamp(quantized, qmin, qmax)

    return quantized.to(torch.int8), scale, zero_point

# å¯¹æ¯”æ¼”ç¤º
def compare_quantization_methods():
    """å¯¹æ¯”å¯¹ç§°å’Œéå¯¹ç§°é‡åŒ–"""

    # åˆ›å»ºéå¯¹ç§°åˆ†å¸ƒçš„å¼ é‡
    tensor = torch.randn(1000) + 2  # å‡å€¼2ï¼Œåå‘æ­£å€¼

    # å¯¹ç§°é‡åŒ–
    sym_quant, sym_scale = symmetric_quantize(tensor)
    sym_dequant = sym_quant.float() * sym_scale

    # éå¯¹ç§°é‡åŒ–
    asym_quant, asym_scale, asym_zp = asymmetric_quantize(tensor)
    asym_dequant = (asym_quant.float() - asym_zp) * asym_scale

    # è®¡ç®—è¯¯å·®
    sym_error = torch.mean((tensor - sym_dequant) ** 2).item()
    asym_error = torch.mean((tensor - asym_dequant) ** 2).item()

    print(f"åŸå§‹å¼ é‡èŒƒå›´: [{tensor.min():.3f}, {tensor.max():.3f}]")
    print(f"å¯¹ç§°é‡åŒ–è¯¯å·®: {sym_error:.6f}")
    print(f"éå¯¹ç§°é‡åŒ–è¯¯å·®: {asym_error:.6f}")
    print(f"éå¯¹ç§°é‡åŒ–ä¼˜åŠ¿: {'æ˜¯' if asym_error < sym_error else 'å¦'}")

compare_quantization_methods()
```

## ğŸ—ï¸ é‡åŒ–ç±»å‹è¯¦è§£

### 1. è®­ç»ƒåé‡åŒ– (Post-Training Quantization)

```python
import torch
import torch.nn as nn
import torch.quantization as quant

class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(256, 10)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

def post_training_quantization_demo():
    """è®­ç»ƒåé‡åŒ–æ¼”ç¤º"""

    # 1. åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹ï¼ˆç®€åŒ–ï¼‰
    model = SimpleModel()
    model.eval()

    print("=== æ¨¡å‹å¤§å°å¯¹æ¯” ===")

    # 2. åŸå§‹FP32æ¨¡å‹
    torch.save(model.state_dict(), 'model_fp32.pth')
    fp32_size = os.path.getsize('model_fp32.pth') / 1024 / 1024  # MB
    print(f"FP32æ¨¡å‹å¤§å°: {fp32_size:.2f} MB")

    # 3. åŠ¨æ€é‡åŒ– (è¿è¡Œæ—¶é‡åŒ–)
    dynamic_quantized = quant.quantize_dynamic(
        model,
        {nn.Linear},  # åªé‡åŒ–Linearå±‚
        dtype=torch.qint8
    )

    torch.save(dynamic_quantized.state_dict(), 'model_dynamic_quantized.pth')
    dynamic_size = os.path.getsize('model_dynamic_quantized.pth') / 1024 / 1024
    print(f"åŠ¨æ€é‡åŒ–æ¨¡å‹å¤§å°: {dynamic_size:.2f} MB")
    print(f"åŠ¨æ€é‡åŒ–å‹ç¼©æ¯”: {fp32_size/dynamic_size:.2f}x")

    # 4. é™æ€é‡åŒ– (ç¦»çº¿é‡åŒ–)
    # å‡†å¤‡æ ¡å‡†æ•°æ®
    model.qconfig = quant.get_default_qconfig('fbgemm')  # x86 CPU
    model_prepared = quant.prepare(model, inplace=False)

    # æ ¡å‡†ï¼ˆä½¿ç”¨ä»£è¡¨æ€§æ•°æ®ï¼‰
    calibration_data = torch.randn(100, 784)
    with torch.no_grad():
        model_prepared(calibration_data)

    # è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
    static_quantized = quant.convert(model_prepared, inplace=False)

    torch.save(static_quantized.state_dict(), 'model_static_quantized.pth')
    static_size = os.path.getsize('model_static_quantized.pth') / 1024 / 1024
    print(f"é™æ€é‡åŒ–æ¨¡å‹å¤§å°: {static_size:.2f} MB")
    print(f"é™æ€é‡åŒ–å‹ç¼©æ¯”: {fp32_size/static_size:.2f}x")

    return model, dynamic_quantized, static_quantized
```

### 2. é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (Quantization Aware Training)

```python
def quantization_aware_training_demo():
    """é‡åŒ–æ„ŸçŸ¥è®­ç»ƒæ¼”ç¤º"""

    model = SimpleModel()

    # 1. æ’å…¥ä¼ªé‡åŒ–èŠ‚ç‚¹
    model.qconfig = quant.get_default_qat_qconfig('fbgemm')
    model_prepared = quant.prepare_qat(model, inplace=False)

    print("=== é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ ===")
    print("æ¨¡å‹ç»“æ„å˜åŒ–:")
    for name, module in model_prepared.named_modules():
        if hasattr(module, 'weight_observer'):
            print(f"  {name}: æ·»åŠ äº†æƒé‡è§‚å¯Ÿå™¨")
        if hasattr(module, 'activation_observer'):
            print(f"  {name}: æ·»åŠ äº†æ¿€æ´»è§‚å¯Ÿå™¨")

    # 2. æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼ˆç®€åŒ–ï¼‰
    optimizer = torch.optim.Adam(model_prepared.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()

    # æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤
    for epoch in range(3):  # ç®€åŒ–è®­ç»ƒ
        # æ‰¹æ¬¡æ•°æ®
        batch_data = torch.randn(32, 784)
        batch_labels = torch.randint(0, 10, (32,))

        # å‰å‘ä¼ æ’­
        outputs = model_prepared(batch_data)
        loss = criterion(outputs, batch_labels)

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")

    # 3. è½¬æ¢ä¸ºæœ€ç»ˆé‡åŒ–æ¨¡å‹
    model_prepared.eval()
    quantized_model = quant.convert(model_prepared, inplace=False)

    print("é‡åŒ–æ„ŸçŸ¥è®­ç»ƒå®Œæˆ")
    return quantized_model
```

## âš¡ æ€§èƒ½å½±å“åˆ†æ

### 1. å†…å­˜å’Œè®¡ç®—æ€§èƒ½

```python
def performance_analysis():
    """é‡åŒ–æ€§èƒ½åˆ†æ"""

    # åˆ›å»ºä¸åŒç²¾åº¦çš„å¼ é‡
    size = (1000, 1000)

    # FP32å¼ é‡
    fp32_tensor = torch.randn(size, dtype=torch.float32)
    fp32_memory = fp32_tensor.numel() * 4 / 1024 / 1024  # MB

    # FP16å¼ é‡
    fp16_tensor = fp32_tensor.half()
    fp16_memory = fp16_tensor.numel() * 2 / 1024 / 1024  # MB

    # INT8å¼ é‡ï¼ˆæ¨¡æ‹Ÿï¼‰
    int8_tensor = torch.randint(-128, 128, size, dtype=torch.int8)
    int8_memory = int8_tensor.numel() * 1 / 1024 / 1024  # MB

    print("=== å†…å­˜ä½¿ç”¨å¯¹æ¯” ===")
    print(f"FP32: {fp32_memory:.2f} MB")
    print(f"FP16: {fp16_memory:.2f} MB (èŠ‚çœ {100*(1-fp16_memory/fp32_memory):.1f}%)")
    print(f"INT8: {int8_memory:.2f} MB (èŠ‚çœ {100*(1-int8_memory/fp32_memory):.1f}%)")

    # è®¡ç®—æ€§èƒ½æµ‹è¯•
    weight = torch.randn(1000, 1000, dtype=torch.float32)

    # FP32çŸ©é˜µä¹˜æ³•
    start_time = time.time()
    for _ in range(100):
        result_fp32 = torch.matmul(fp32_tensor, weight)
    fp32_time = time.time() - start_time

    # FP16çŸ©é˜µä¹˜æ³•
    weight_fp16 = weight.half()
    start_time = time.time()
    for _ in range(100):
        result_fp16 = torch.matmul(fp16_tensor, weight_fp16)
    fp16_time = time.time() - start_time

    print("\n=== è®¡ç®—æ—¶é—´å¯¹æ¯” ===")
    print(f"FP32: {fp32_time:.3f}s")
    print(f"FP16: {fp16_time:.3f}s (åŠ é€Ÿ {fp32_time/fp16_time:.2f}x)")

    return {
        'memory': {'fp32': fp32_memory, 'fp16': fp16_memory, 'int8': int8_memory},
        'time': {'fp32': fp32_time, 'fp16': fp16_time}
    }

performance_analysis()
```

### 2. ç²¾åº¦æŸå¤±åˆ†æ

```python
def accuracy_loss_analysis():
    """é‡åŒ–ç²¾åº¦æŸå¤±åˆ†æ"""

    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    model = SimpleModel()
    model.eval()

    # æµ‹è¯•æ•°æ®
    test_data = torch.randn(1000, 784)

    # åŸå§‹æ¨¡å‹é¢„æµ‹
    with torch.no_grad():
        original_output = model(test_data)

    # é‡åŒ–æ¨¡å‹é¢„æµ‹
    quantized_model = quant.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)
    with torch.no_grad():
        quantized_output = quantized_model(test_data)

    # è®¡ç®—é¢„æµ‹å·®å¼‚
    mse_loss = nn.MSELoss()(original_output, quantized_output.float()).item()

    # è®¡ç®—é¢„æµ‹ä¸€è‡´æ€§
    original_pred = torch.argmax(original_output, dim=1)
    quantized_pred = torch.argmax(quantized_output, dim=1)
    accuracy_consistency = (original_pred == quantized_pred).float().mean().item()

    print("=== ç²¾åº¦æŸå¤±åˆ†æ ===")
    print(f"MSEæŸå¤±: {mse_loss:.6f}")
    print(f"é¢„æµ‹ä¸€è‡´æ€§: {accuracy_consistency*100:.2f}%")
    print(f"é¢„æµ‹ä¸ä¸€è‡´æ€§: {(1-accuracy_consistency)*100:.2f}%")

    # åˆ†æä¸åŒå±‚çš„é‡åŒ–å½±å“
    print("\n=== é€å±‚åˆ†æ ===")
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            weight = module.weight.data
            quantized_weight = quant.quantize_dynamic(
                module, {nn.Linear}, dtype=torch.qint8
            ).fc1.weight().data

            weight_error = torch.mean((weight.float() - quantized_weight.float()) ** 2).item()
            print(f"{name}: æƒé‡é‡åŒ–è¯¯å·® = {weight_error:.6f}")

    return mse_loss, accuracy_consistency

accuracy_loss_analysis()
```

## ğŸ”§ å®é™…åº”ç”¨ç­–ç•¥

### 1. é‡åŒ–ç­–ç•¥é€‰æ‹©æŒ‡å—

```python
def quantization_strategy_guide():
    """é‡åŒ–ç­–ç•¥é€‰æ‹©æŒ‡å—"""

    strategies = {
        'åŠ¨æ€é‡åŒ–': {
            'é€‚ç”¨åœºæ™¯': [
                'æ¨ç†æ—¶CPUèµ„æºå—é™',
                'å†…å­˜æ˜¯ä¸»è¦ç“¶é¢ˆ',
                'æ¨¡å‹åŠ è½½æ—¶é—´æ•æ„Ÿ',
                'ä¸æƒ³é‡æ–°è®­ç»ƒæ¨¡å‹'
            ],
            'ä¼˜åŠ¿': ['å®ç°ç®€å•', 'æ— éœ€æ ¡å‡†æ•°æ®', 'æ¨¡å‹ç²¾åº¦æŸå¤±ç›¸å¯¹è¾ƒå°'],
            'åŠ£åŠ¿': ['æ¨ç†åŠ é€Ÿæœ‰é™', 'ä¸æ”¯æŒæ‰€æœ‰æ“ä½œç±»å‹'],
            'æ¨èé…ç½®': {
                'ç›®æ ‡å±‚': ['Linear', 'LSTM', 'GRU'],
                'æ•°æ®ç±»å‹': 'torch.qint8',
                'ç¡¬ä»¶': 'CPUæ¨ç†'
            }
        },

        'é™æ€é‡åŒ–': {
            'é€‚ç”¨åœºæ™¯': [
                'è¿½æ±‚æœ€å¤§æ¨ç†é€Ÿåº¦',
                'æœ‰ä»£è¡¨æ€§æ•°æ®é›†',
                'å¯ä»¥æ¥å—å°‘é‡ç²¾åº¦æŸå¤±',
                'éƒ¨ç½²ç¯å¢ƒç¡®å®š'
            ],
            'ä¼˜åŠ¿': ['æ¨ç†é€Ÿåº¦æœ€å¿«', 'å†…å­˜å ç”¨æœ€å°', 'ç¡¬ä»¶æ”¯æŒæœ€å¥½'],
            'åŠ£åŠ¿': ['éœ€è¦æ ¡å‡†æ•°æ®', 'ç²¾åº¦æŸå¤±å¯èƒ½è¾ƒå¤§', 'å®ç°å¤æ‚'],
            'æ¨èé…ç½®': {
                'æ ¡å‡†æ•°æ®': '100-1000ä¸ªä»£è¡¨æ€§æ ·æœ¬',
                'æ•°æ®ç±»å‹': 'torch.qint8 (CPU) / torch.quint8 (GPU)',
                'ç¡¬ä»¶': 'ä¸“ç”¨æ¨ç†ç¡¬ä»¶'
            }
        },

        'é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ': {
            'é€‚ç”¨åœºæ™¯': [
                'ç²¾åº¦è¦æ±‚ä¸¥æ ¼',
                'å¯ä»¥é‡æ–°è®­ç»ƒ',
                'è¿½æ±‚æœ€ä½³æ€§èƒ½å¹³è¡¡',
                'ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²'
            ],
            'ä¼˜åŠ¿': ['ç²¾åº¦æŸå¤±æœ€å°', 'æ€§èƒ½æœ€ä¼˜', 'é€‚åº”æ€§å¼º'],
            'åŠ£åŠ¿': ['éœ€è¦é‡æ–°è®­ç»ƒ', 'å®ç°æœ€å¤æ‚', 'è®­ç»ƒæ—¶é—´é•¿'],
            'æ¨èé…ç½®': {
                'è®­ç»ƒè½®æ•°': 'åŸè®­ç»ƒçš„10-20%',
                'å­¦ä¹ ç‡': 'åŸå­¦ä¹ ç‡çš„0.1-0.01',
                'æ•°æ®ç±»å‹': 'torch.qint8'
            }
        }
    }

    return strategies

# å†³ç­–æ ‘å‡½æ•°
def recommend_quantization_strategy(model_size_mb, accuracy_requirement,
                                  has_training_data, deployment_hardware):
    """é‡åŒ–ç­–ç•¥æ¨è"""

    if accuracy_requirement == 'strict' and has_training_data:
        return 'é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ'

    elif deployment_hardware == 'mobile' and accuracy_requirement == 'moderate':
        return 'é™æ€é‡åŒ–'

    elif model_size_mb > 100 and not has_training_data:
        return 'åŠ¨æ€é‡åŒ–'

    elif deployment_hardware == 'server' and accuracy_requirement == 'relaxed':
        return 'é™æ€é‡åŒ–'

    else:
        return 'åŠ¨æ€é‡åŒ–'  # é»˜è®¤æ¨è

# ä½¿ç”¨ç¤ºä¾‹
print("=== é‡åŒ–ç­–ç•¥æ¨è ===")
strategy = recommend_quantization_strategy(
    model_size_mb=50,
    accuracy_requirement='moderate',
    has_training_data=False,
    deployment_hardware='cpu'
)
print(f"æ¨èç­–ç•¥: {strategy}")
```

### 2. é‡åŒ–æœ€ä½³å®è·µ

```python
class QuantizationBestPractices:
    """é‡åŒ–æœ€ä½³å®è·µæŒ‡å—"""

    @staticmethod
    def pre_quantization_checks(model):
        """é‡åŒ–å‰æ£€æŸ¥æ¸…å•"""

        checks = {
            'æ¨¡å‹å…¼å®¹æ€§': [],
            'æ•°æ®å‡†å¤‡': [],
            'ç¡¬ä»¶æ”¯æŒ': []
        }

        # æ£€æŸ¥æ¨¡å‹å…¼å®¹æ€§
        supported_layers = [nn.Linear, nn.Conv2d, nn.LSTM, nn.GRU]
        for name, module in model.named_modules():
            if isinstance(module, tuple(supported_layers)):
                checks['æ¨¡å‹å…¼å®¹æ€§'].append(f"âœ… {name}: æ”¯æŒé‡åŒ–")
            else:
                checks['æ¨¡å‹å…¼å®¹æ€§'].append(f"âš ï¸  {name}: å¯èƒ½ä¸æ”¯æŒé‡åŒ–")

        # æ£€æŸ¥æ•°æ®å‡†å¤‡
        checks['æ•°æ®å‡†å¤‡'] = [
            "âœ… å‡†å¤‡æ ¡å‡†æ•°æ®é›†ï¼ˆ100-1000ä¸ªæ ·æœ¬ï¼‰",
            "âœ… æ•°æ®åˆ†å¸ƒä¸å®é™…æ¨ç†æ•°æ®ä¸€è‡´",
            "âœ… æ•°æ®é¢„å¤„ç†æµç¨‹ä¿æŒä¸€è‡´"
        ]

        # æ£€æŸ¥ç¡¬ä»¶æ”¯æŒ
        checks['ç¡¬ä»¶æ”¯æŒ'] = [
            "âœ… CPU: æ”¯æŒINT8æŒ‡ä»¤é›†ï¼ˆAVX2, AVX512ï¼‰",
            "âœ… GPU: æ”¯æŒTensor Coreï¼ˆVoltaæ¶æ„åŠä»¥ä¸Šï¼‰",
            "âœ… ç§»åŠ¨ç«¯: æ”¯æŒNNAPI/CoreML"
        ]

        return checks

    @staticmethod
    def quantization_checklist():
        """é‡åŒ–æ£€æŸ¥æ¸…å•"""

        return {
            'é‡åŒ–å‰': [
                'â–¡ å¤‡ä»½åŸå§‹æ¨¡å‹',
                'â–¡ è¯„ä¼°åŸå§‹æ¨¡å‹ç²¾åº¦',
                'â–¡ å‡†å¤‡ä»£è¡¨æ€§æ•°æ®é›†',
                'â–¡ æ£€æŸ¥ç¡¬ä»¶æ”¯æŒ',
                'â–¡ ç¡®å®šé‡åŒ–ç­–ç•¥'
            ],

            'é‡åŒ–ä¸­': [
                'â–¡ é€‰æ‹©åˆé€‚çš„é‡åŒ–æ–¹æ³•',
                'â–¡ é…ç½®é‡åŒ–å‚æ•°',
                'â–¡ æ‰§è¡Œæ ¡å‡†è¿‡ç¨‹',
                'â–¡ ç›‘æ§ç²¾åº¦å˜åŒ–',
                'â–¡ éªŒè¯æ¨¡å‹åŠŸèƒ½'
            ],

            'é‡åŒ–å': [
                'â–¡ æµ‹è¯•é‡åŒ–æ¨¡å‹ç²¾åº¦',
                'â–¡ æµ‹è¯•æ¨ç†æ€§èƒ½',
                'â–¡ éªŒè¯éƒ¨ç½²å…¼å®¹æ€§',
                'â–¡ æ–‡æ¡£åŒ–é‡åŒ–è¿‡ç¨‹',
                'â–¡ å»ºç«‹ç›‘æ§æœºåˆ¶'
            ]
        }

    @staticmethod
    def common_pitfalls():
        """å¸¸è§é™·é˜±å’Œè§£å†³æ–¹æ¡ˆ"""

        pitfalls = {
            'ç²¾åº¦æŸå¤±è¿‡å¤§': {
                'åŸå› ': ['é‡åŒ–èŒƒå›´ä¸åˆé€‚', 'æ•°æ®åˆ†å¸ƒå¼‚å¸¸', 'æ¨¡å‹å¯¹ç²¾åº¦æ•æ„Ÿ'],
                'è§£å†³æ–¹æ¡ˆ': [
                    'ä½¿ç”¨é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ',
                    'è°ƒæ•´é‡åŒ–èŒƒå›´',
                    'åˆ†å±‚é‡åŒ–ç­–ç•¥',
                    'æ··åˆç²¾åº¦é‡åŒ–'
                ]
            },

            'æ€§èƒ½æå‡ä¸æ˜æ˜¾': {
                'åŸå› ': ['ç¡¬ä»¶ä¸æ”¯æŒ', 'é‡åŒ–ä¸å½»åº•', 'æ“ä½œç±»å‹ä¸æ”¯æŒ'],
                'è§£å†³æ–¹æ¡ˆ': [
                    'æ£€æŸ¥ç¡¬ä»¶æŒ‡ä»¤é›†æ”¯æŒ',
                    'ä½¿ç”¨é™æ€é‡åŒ–',
                    'é€‰æ‹©åˆé€‚çš„ç®—å­èåˆ',
                    'ä¼˜åŒ–æ¨¡å‹ç»“æ„'
                ]
            },

            'éƒ¨ç½²å…¼å®¹æ€§é—®é¢˜': {
                'åŸå› ': ['æ¡†æ¶ç‰ˆæœ¬ä¸åŒ¹é…', 'ç¡¬ä»¶ç¯å¢ƒå·®å¼‚', 'ä¾èµ–åº“ç¼ºå¤±'],
                'è§£å†³æ–¹æ¡ˆ': [
                    'ä½¿ç”¨å®¹å™¨åŒ–éƒ¨ç½²',
                    'éªŒè¯ç›®æ ‡ç¯å¢ƒ',
                    'æä¾›å¤‡é€‰æ–¹æ¡ˆ',
                    'å»ºç«‹æµ‹è¯•æµç¨‹'
                ]
            }
        }

        return pitfalls

# ä½¿ç”¨ç¤ºä¾‹
best_practices = QuantizationBestPractices()
print("=== é‡åŒ–æœ€ä½³å®è·µ ===")
checklist = best_practices.quantization_checklist()
for stage, items in checklist.items():
    print(f"\n{stage}:")
    for item in items:
        print(f"  {item}")
```

## ğŸ¯ æ€»ç»“ï¼šé‡åŒ–çš„æ ¸å¿ƒä»·å€¼

### é‡åŒ– vs ç²¾åº¦è½¬æ¢çš„æœ¬è´¨åŒºåˆ«

```python
# æœ€ç»ˆå¯¹æ¯”æ€»ç»“
comparison_summary = {
    'ç²¾åº¦è½¬æ¢ (FP32â†’FP16)': {
        'æœ¬è´¨': 'é™ä½æµ®ç‚¹ç²¾åº¦',
        'å†…å­˜èŠ‚çœ': '50%',
        'è®¡ç®—åŠ é€Ÿ': 'æœ‰é™ï¼ˆ2-4xï¼‰',
        'ç²¾åº¦æŸå¤±': 'ç›¸å¯¹è¾ƒå°',
        'ç¡¬ä»¶æ”¯æŒ': 'å¹¿æ³›',
        'å®ç°å¤æ‚åº¦': 'ç®€å•'
    },

    'é‡åŒ– (FP32â†’INT8)': {
        'æœ¬è´¨': 'æ•°å€¼ç³»ç»Ÿè½¬æ¢ï¼ˆæµ®ç‚¹â†’æ•´æ•°ï¼‰',
        'å†…å­˜èŠ‚çœ': '75%',
        'è®¡ç®—åŠ é€Ÿ': 'æ˜¾è‘—ï¼ˆ4-16xï¼‰',
        'ç²¾åº¦æŸå¤±': 'å¯èƒ½è¾ƒå¤§',
        'ç¡¬ä»¶æ”¯æŒ': 'éœ€è¦ä¸“ç”¨æŒ‡ä»¤',
        'å®ç°å¤æ‚åº¦': 'å¤æ‚'
    }
}

print("=== æ ¸å¿ƒå¯¹æ¯” ===")
for method, features in comparison_summary.items():
    print(f"\n{method}:")
    for feature, value in features.items():
        print(f"  {feature}: {value}")
```

### ä½•æ—¶é€‰æ‹©å“ªç§æ–¹æ³•ï¼Ÿ

```python
def choose_optimization_strategy(scenario):
    """æ ¹æ®åœºæ™¯é€‰æ‹©ä¼˜åŒ–ç­–ç•¥"""

    strategies = {
        'è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²': {
            'é¦–é€‰': 'é‡åŒ– (INT8)',
            'åŸå› ': 'å†…å­˜å’Œè®¡ç®—èµ„æºæåº¦å—é™'
        },

        'äº‘ç«¯æ¨ç†åŠ é€Ÿ': {
            'é¦–é€‰': 'æ··åˆç­–ç•¥ (FP16 + é‡åŒ–)',
            'åŸå› ': 'å¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½'
        },

        'å¿«é€ŸåŸå‹éªŒè¯': {
            'é¦–é€‰': 'ç²¾åº¦è½¬æ¢ (FP16)',
            'åŸå› ': 'å®ç°ç®€å•ï¼Œé£é™©ä½'
        },

        'ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–': {
            'é¦–é€‰': 'é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ',
            'åŸå› ': 'æœ€ä½³æ€§èƒ½å’Œç²¾åº¦å¹³è¡¡'
        }
    }

    return strategies.get(scenario, {'é¦–é€‰': 'FP16', 'åŸå› ': 'é»˜è®¤å®‰å…¨é€‰æ‹©'})

print("\n=== ç­–ç•¥é€‰æ‹©å»ºè®® ===")
for scenario, recommendation in choose_optimization_strategy.__code__.co_consts:
    if scenario and isinstance(scenario, str) and scenario in ['è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²', 'äº‘ç«¯æ¨ç†åŠ é€Ÿ', 'å¿«é€ŸåŸå‹éªŒè¯', 'ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–']:
        result = choose_optimization_strategy(scenario)
        print(f"{scenario}: {result['é¦–é€‰']} - {result['åŸå› ']}")
```

é‡åŒ–æ˜¯ä¸€é¡¹å¼ºå¤§çš„ä¼˜åŒ–æŠ€æœ¯ï¼Œä½†éœ€è¦æ·±å…¥ç†è§£å…¶åŸç†å’Œæƒè¡¡ã€‚æ­£ç¡®ä½¿ç”¨é‡åŒ–å¯ä»¥å¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè€Œè¯¯ç”¨å¯èƒ½å¯¼è‡´ä¸¥é‡çš„ç²¾åº¦é—®é¢˜ã€‚å…³é”®æ˜¯æ ¹æ®å…·ä½“åœºæ™¯é€‰æ‹©åˆé€‚çš„ç­–ç•¥ã€‚