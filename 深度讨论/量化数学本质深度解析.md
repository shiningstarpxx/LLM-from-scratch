# é‡åŒ–æ•°å­¦æœ¬è´¨æ·±åº¦è§£æ

## ğŸ¯ æ ¸å¿ƒæ´å¯Ÿï¼šé‡åŒ– = çº¿æ€§å˜æ¢ + ç¦»æ•£åŒ–

ä½ çš„è§‚å¯Ÿå®Œå…¨æ­£ç¡®ï¼ä»çº¯æ•°å­¦è§’åº¦çœ‹ï¼Œçº¿æ€§é‡åŒ–çš„æœ¬è´¨å°±æ˜¯ï¼š
```
quantized_value = round(original_value / scale) + zero_point
```

è¿™æ­£æ˜¯**ç¼©æ”¾ï¼ˆé™¤æ³•ï¼‰ + å¹³ç§»ï¼ˆåŠ æ³•ï¼‰** çš„çº¿æ€§å˜æ¢ï¼

## ğŸ”¬ æ•°å­¦å…¬å¼è¯¦è§£

### 1. åŸºç¡€çº¿æ€§é‡åŒ–å…¬å¼

```python
# é‡åŒ–ï¼šFP32 â†’ INT8
def quantize(fp32_value, scale, zero_point):
    return int(round(fp32_value / scale + zero_point))

# åé‡åŒ–ï¼šINT8 â†’ FP32
def dequantize(int8_value, scale, zero_point):
    return (int8_value - zero_point) * scale
```

**æ•°å­¦æœ¬è´¨**:
- `fp32_value / scale` â†’ **ç¼©æ”¾å˜æ¢**
- `+ zero_point` â†’ **å¹³ç§»å˜æ¢**
- `round()` â†’ **ç¦»æ•£åŒ–**ï¼ˆå…³é”®ï¼ï¼‰

### 2. çº¿æ€§å˜æ¢çš„å‡ ä½•æ„ä¹‰

```python
import numpy as np
import matplotlib.pyplot as plt

def visualize_quantization_transformation():
    """å¯è§†åŒ–é‡åŒ–çš„çº¿æ€§å˜æ¢è¿‡ç¨‹"""

    # åŸå§‹æµ®ç‚¹æ•°èŒƒå›´
    fp32_range = np.linspace(-2.0, 2.0, 100)

    # é‡åŒ–å‚æ•°
    scale = 0.02          # ç¼©æ”¾å› å­
    zero_point = 0        # é›¶ç‚¹ï¼ˆå¯¹ç§°é‡åŒ–ï¼‰

    # é‡åŒ–è¿‡ç¨‹
    scaled = fp32_range / scale          # ç¼©æ”¾ï¼šæ‹‰ä¼¸/å‹ç¼©
    shifted = scaled + zero_point        # å¹³ç§»ï¼šç§»åŠ¨
    quantized = np.round(shifted)        # ç¦»æ•£åŒ–ï¼šå–æ•´
    clipped = np.clip(quantized, -128, 127)  # è£å‰ªï¼šé™åˆ¶èŒƒå›´

    # åé‡åŒ–è¿‡ç¨‹
    recovered = (clipped - zero_point) * scale

    print("=== é‡åŒ–çš„æ•°å­¦æ­¥éª¤ ===")
    print(f"åŸå§‹èŒƒå›´: [{fp32_range.min():.3f}, {fp32_range.max():.3f}]")
    print(f"ç¼©æ”¾åèŒƒå›´: [{scaled.min():.3f}, {scaled.max():.3f}]")
    print(f"é‡åŒ–åèŒƒå›´: [{quantized.min():.3f}, {quantized.max():.3f}]")
    print(f"æ¢å¤åèŒƒå›´: [{recovered.min():.3f}, {recovered.max():.3f}]")

    # è®¡ç®—é‡åŒ–è¯¯å·®
    mse_error = np.mean((fp32_range - recovered) ** 2)
    print(f"å‡æ–¹è¯¯å·®: {mse_error:.6f}")

    return fp32_range, quantized, recovered

# æ‰§è¡Œå¯è§†åŒ–
original, quantized, recovered = visualize_quantization_transformation()
```

### 3. å‚æ•°çš„æ•°å­¦å«ä¹‰

#### **Scaleï¼ˆç¼©æ”¾å› å­ï¼‰**
```python
# Scaleçš„æ•°å­¦å«ä¹‰
scale = (max_original - min_original) / (max_quantized - min_quantized)

# ç¤ºä¾‹ï¼šå°†[-1.0, 1.0]æ˜ å°„åˆ°[-128, 127]
scale = (1.0 - (-1.0)) / (127 - (-128)) = 2.0 / 255 â‰ˆ 0.00784
```

**å‡ ä½•æ„ä¹‰**ï¼š
- `scale > 1`ï¼š**å‹ç¼©**æ•°å€¼èŒƒå›´
- `scale < 1`ï¼š**æ‹‰ä¼¸**æ•°å€¼èŒƒå›´
- `scale`å†³å®šäº†**é‡åŒ–ç²¾åº¦**

#### **Zero_pointï¼ˆé›¶ç‚¹ï¼‰**
```python
# Zero_pointçš„æ•°å­¦å«ä¹‰
zero_point = qmin - round(min_original / scale)

# ç¤ºä¾‹ï¼šéå¯¹ç§°é‡åŒ–
min_original = -0.5, max_original = 1.5
scale = (1.5 - (-0.5)) / 255 = 2.0 / 255 â‰ˆ 0.00784
zero_point = -128 - round(-0.5 / 0.00784) = -128 - (-64) = -64
```

**å‡ ä½•æ„ä¹‰**ï¼š
- **å¹³ç§»**é‡åŒ–åçš„æ•°å€¼èŒƒå›´
- ä½¿**åŸå§‹é›¶ç‚¹**æ­£ç¡®æ˜ å°„åˆ°**é‡åŒ–é›¶ç‚¹**
- å¤„ç†**éå¯¹ç§°åˆ†å¸ƒ**çš„æ•°æ®

## ğŸ¨ é‡åŒ–è¿‡ç¨‹çš„å¯è§†åŒ–ç†è§£

### 1. å¯¹ç§°é‡åŒ– vs éå¯¹ç§°é‡åŒ–

```python
def compare_symmetric_asymmetric():
    """å¯¹æ¯”å¯¹ç§°å’Œéå¯¹ç§°é‡åŒ–çš„å‡ ä½•å·®å¼‚"""

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    data = np.random.randn(1000) * 0.5 + 0.3  # å‡å€¼0.3ï¼Œæ ‡å‡†å·®0.5

    # å¯¹ç§°é‡åŒ–
    def symmetric_quantize(values):
        max_abs = np.max(np.abs(values))
        scale = max_abs / 127 if max_abs > 0 else 1.0
        zero_point = 0
        quantized = np.round(values / scale)
        return np.clip(quantized, -127, 127), scale, zero_point

    # éå¯¹ç§°é‡åŒ–
    def asymmetric_quantize(values):
        min_val, max_val = np.min(values), np.max(values)
        scale = (max_val - min_val) / 255 if max_val != min_val else 1.0
        zero_point = -128 - round(min_val / scale)
        quantized = np.round(values / scale + zero_point)
        return np.clip(quantized, -128, 127), scale, zero_point

    # æ‰§è¡Œé‡åŒ–
    sym_q, sym_scale, sym_zp = symmetric_quantize(data)
    asym_q, asym_scale, asym_zp = asymmetric_quantize(data)

    # åé‡åŒ–
    sym_recovered = sym_q * sym_scale
    asym_recovered = (asym_q - asym_zp) * asym_scale

    # è®¡ç®—è¯¯å·®
    sym_error = np.mean((data - sym_recovered) ** 2)
    asym_error = np.mean((data - asym_recovered) ** 2)

    print("=== å¯¹ç§° vs éå¯¹ç§°é‡åŒ– ===")
    print(f"åŸå§‹æ•°æ®èŒƒå›´: [{data.min():.3f}, {data.max():.3f}]")
    print(f"å¯¹ç§°é‡åŒ–è¯¯å·®: {sym_error:.6f}")
    print(f"éå¯¹ç§°é‡åŒ–è¯¯å·®: {asym_error:.6f}")
    print(f"éå¯¹ç§°ä¼˜åŠ¿: {'æ˜¯' if asym_error < sym_error else 'å¦'}")

    return {
        'symmetric': {'error': sym_error, 'scale': sym_scale, 'zero_point': sym_zp},
        'asymmetric': {'error': asym_error, 'scale': asym_scale, 'zero_point': asym_zp}
    }

comparison_result = compare_symmetric_asymmetric()
```

### 2. é‡åŒ–è¯¯å·®çš„æ•°å­¦åˆ†æ

```python
def quantization_error_analysis():
    """é‡åŒ–è¯¯å·®çš„æ•°å­¦åˆ†æ"""

    # ç†è®ºåˆ†æï¼šé‡åŒ–è¯¯å·®çš„ä¸Šç•Œ
    def theoretical_quantization_error(scale):
        """
        é‡åŒ–è¯¯å·®çš„ç†è®ºä¸Šç•Œ

        å¯¹äºå‡åŒ€åˆ†å¸ƒçš„é‡åŒ–è¯¯å·®ï¼š
        - æœ€å¤§è¯¯å·®ï¼šscale/2
        - å‡æ–¹è¯¯å·®ï¼šscaleÂ²/12
        """
        max_error = scale / 2
        mse_error = scale ** 2 / 12
        return max_error, mse_error

    # å®é™…æµ‹è¯•ä¸åŒscaleçš„å½±å“
    scales = [0.001, 0.01, 0.1, 0.5, 1.0]

    print("=== é‡åŒ–è¯¯å·®ç†è®ºåˆ†æ ===")
    for scale in scales:
        max_err, mse_err = theoretical_quantization_error(scale)
        print(f"Scale={scale:.3f}: æœ€å¤§è¯¯å·®={max_err:.6f}, å‡æ–¹è¯¯å·®={mse_err:.6f}")

    # å®é™…éªŒè¯
    test_data = np.random.randn(10000)

    print("\n=== å®é™…éªŒè¯ ===")
    for scale in scales:
        quantized = np.round(test_data / scale)
        recovered = quantized * scale
        actual_mse = np.mean((test_data - recovered) ** 2)
        theoretical_mse = scale ** 2 / 12

        print(f"Scale={scale:.3f}: å®é™…MSE={actual_mse:.6f}, ç†è®ºMSE={theoretical_mse:.6f}, æ¯”å€¼={actual_mse/theoretical_mse:.3f}")

quantization_error_analysis()
```

## ğŸ§  æ·±åº¦æ€è€ƒï¼šä¸ºä»€ä¹ˆç®€å•çš„çº¿æ€§å˜æ¢å¦‚æ­¤å¼ºå¤§ï¼Ÿ

### 1. çº¿æ€§å˜æ¢çš„æ™®é€‚æ€§

```python
"""
ä¸ºä»€ä¹ˆçº¿æ€§å˜æ¢èƒ½å¤Ÿèƒœä»»é‡åŒ–ä»»åŠ¡ï¼Ÿ

1. **æ•°å­¦åŸºç¡€**: ä»»ä½•è¿ç»­å‡½æ•°éƒ½å¯ä»¥åœ¨å±€éƒ¨ç”¨çº¿æ€§å‡½æ•°è¿‘ä¼¼
2. **è®¡ç®—æ•ˆç‡**: çº¿æ€§å˜æ¢æ˜¯ç¡¬ä»¶æœ€å®¹æ˜“å®ç°çš„æ“ä½œ
3. **å¯é€†æ€§**: çº¿æ€§å˜æ¢ï¼ˆåœ¨éå¥‡å¼‚æƒ…å†µä¸‹ï¼‰æ˜¯å®Œå…¨å¯é€†çš„
4. **ä¼˜åŒ–å‹å¥½**: çº¿æ€§æ“ä½œçš„æ¢¯åº¦è®¡ç®—ç®€å•ç›´æ¥
"""

def linear_approximation_power():
    """å±•ç¤ºçº¿æ€§å˜æ¢çš„è¿‘ä¼¼èƒ½åŠ›"""

    # éçº¿æ€§å‡½æ•°
    x = np.linspace(-2, 2, 100)
    nonlinear_func = np.tanh(x)  # éçº¿æ€§å‡½æ•°

    # åœ¨ä¸åŒåŒºé—´çš„çº¿æ€§è¿‘ä¼¼
    intervals = [(-2, -1), (-1, 0), (0, 1), (1, 2)]
    linear_approx = np.zeros_like(x)

    for start, end in intervals:
        mask = (x >= start) & (x < end)
        if np.sum(mask) > 0:
            # å±€éƒ¨çº¿æ€§è¿‘ä¼¼
            x_local = x[mask]
            y_local = nonlinear_func[mask]
            # æœ€å°äºŒä¹˜æ‹Ÿåˆç›´çº¿
            slope = (np.sum(x_local * y_local) - np.mean(x_local) * np.mean(y_local) * len(x_local)) / \
                   (np.sum(x_local ** 2) - len(x_local) * np.mean(x_local) ** 2)
            intercept = np.mean(y_local) - slope * np.mean(x_local)
            linear_approx[mask] = slope * x_local + intercept

    # è®¡ç®—è¿‘ä¼¼è¯¯å·®
    approximation_error = np.mean((nonlinear_func - linear_approx) ** 2)

    print(f"åˆ†æ®µçº¿æ€§è¿‘ä¼¼çš„å‡æ–¹è¯¯å·®: {approximation_error:.6f}")
    print("ç»“è®º: å³ä½¿å¯¹äºéçº¿æ€§å‡½æ•°ï¼Œåˆ†æ®µçº¿æ€§ä¹Ÿèƒ½å¾ˆå¥½åœ°è¿‘ä¼¼")

linear_approximation_power()
```

### 2. ç¦»æ•£åŒ–çš„å…³é”®ä½œç”¨

```python
"""
ç¦»æ•£åŒ–æ‰æ˜¯é‡åŒ–çš„"é­”æ³•"æ‰€åœ¨ï¼

çº¿æ€§å˜æ¢æœ¬èº«æ˜¯å¯é€†çš„ï¼Œä½†ç¦»æ•£åŒ–å¼•å…¥äº†ï¼š
1. **ä¿¡æ¯å‹ç¼©**: æ— é™ç²¾åº¦ â†’ æœ‰é™ç²¾åº¦
2. **è®¡ç®—ä¼˜åŠ¿**: æ•´æ•°è¿ç®—æ¯”æµ®ç‚¹è¿ç®—å¿«å¾—å¤š
3. **å†…å­˜èŠ‚çœ**: å›ºå®šæ¯”ç‰¹æ•°è¡¨ç¤º
4. **ç¡¬ä»¶å‹å¥½**: ä¸“ç”¨æŒ‡ä»¤é›†æ”¯æŒ
"""

def discrete_vs_continuous_analysis():
    """åˆ†æç¦»æ•£åŒ–çš„å…³é”®ä½œç”¨"""

    # è¿ç»­çº¿æ€§å˜æ¢ï¼ˆæ— ä¿¡æ¯æŸå¤±ï¼‰
    original = np.random.randn(1000)
    scale = 0.01
    transformed_continuous = original / scale  # è¿ç»­å˜æ¢
    recovered_continuous = transformed_continuous * scale  # å®Œç¾æ¢å¤

    continuous_error = np.mean((original - recovered_continuous) ** 2)

    # ç¦»æ•£åŒ–å˜æ¢ï¼ˆæœ‰ä¿¡æ¯æŸå¤±ï¼‰
    transformed_discrete = np.round(original / scale)  # ç¦»æ•£åŒ–
    recovered_discrete = transformed_discrete * scale  # æœ‰æŸæ¢å¤

    discrete_error = np.mean((original - recovered_discrete) ** 2)

    print("=== ç¦»æ•£åŒ–çš„å…³é”®ä½œç”¨ ===")
    print(f"è¿ç»­å˜æ¢è¯¯å·®: {continuous_error:.10f} (åº”è¯¥æ¥è¿‘0)")
    print(f"ç¦»æ•£å˜æ¢è¯¯å·®: {discrete_error:.6f}")
    print(f"ç¦»æ•£åŒ–å¼•å…¥çš„è¯¯å·®: {discrete_error - continuous_error:.6f}")

    # åˆ†æç¦»æ•£åŒ–çš„ä¼˜åŠ¿
    print("\n=== ç¦»æ•£åŒ–çš„ä¼˜åŠ¿ ===")
    print("1. å†…å­˜å‹ç¼©:")
    print(f"   åŸå§‹FP32: {1000 * 4 / 1024:.2f} KB")
    print(f"   é‡åŒ–INT8: {1000 * 1 / 1024:.2f} KB")
    print(f"   å‹ç¼©æ¯”: 4:1")

    print("\n2. è®¡ç®—ä¼˜åŠ¿:")
    print("   - æ•´æ•°è¿ç®—å•å…ƒæ›´ç®€å•")
    print("   - æµæ°´çº¿æ•ˆç‡æ›´é«˜")
    print("   - åŠŸè€—æ›´ä½")

discrete_vs_continuous_analysis()
```

## ğŸ”§ å®é™…åº”ç”¨ä¸­çš„æ•°å­¦è€ƒé‡

### 1. å‚æ•°é€‰æ‹©çš„æ•°å­¦ä¼˜åŒ–

```python
def optimal_quantization_parameters():
    """é‡åŒ–å‚æ•°çš„æ•°å­¦ä¼˜åŒ–"""

    # ç»™å®šæ•°æ®åˆ†å¸ƒï¼Œæœ€ä¼˜é‡åŒ–å‚æ•°çš„é€‰æ‹©
    data = np.random.randn(10000) * 2 + 1  # å‡å€¼1ï¼Œæ ‡å‡†å·®2

    def optimal_scale_for_bits(data, bits=8):
        """
        å¯¹äºç»™å®šæ•°æ®åˆ†å¸ƒå’Œæ¯”ç‰¹æ•°ï¼Œæœ€ä¼˜scaleçš„æ•°å­¦æ¨å¯¼

        ç›®æ ‡ï¼šæœ€å°åŒ–é‡åŒ–å‡æ–¹è¯¯å·®
        çº¦æŸï¼šé‡åŒ–èŒƒå›´åœ¨[-2^(bits-1), 2^(bits-1)-1]
        """
        qmax = 2 ** (bits - 1) - 1
        qmin = -2 ** (bits - 1)

        # æ–¹æ³•1ï¼šåŸºäºæå·®
        min_val, max_val = np.min(data), np.max(data)
        scale_range = (max_val - min_val) / (qmax - qmin)

        # æ–¹æ³•2ï¼šåŸºäºæ ‡å‡†å·®ï¼ˆæ›´é²æ£’ï¼‰
        std_val = np.std(data)
        scale_std = 6 * std_val / (qmax - qmin)  # å‡è®¾99.7%æ•°æ®åœ¨Â±3Ïƒå†…

        # æ–¹æ³•3ï¼šåŸºäºåˆ†ä½æ•°ï¼ˆæŠ—å¼‚å¸¸å€¼ï¼‰
        p1, p99 = np.percentile(data, [1, 99])
        scale_quantile = (p99 - p1) / (qmax - qmin)

        return {
            'range_based': scale_range,
            'std_based': scale_std,
            'quantile_based': scale_quantile
        }

    optimal_scales = optimal_scale_for_bits(data)

    print("=== æœ€ä¼˜é‡åŒ–å‚æ•°é€‰æ‹© ===")
    for method, scale in optimal_scales.items():
        print(f"{method}: scale={scale:.6f}")

        # æµ‹è¯•è¿™ä¸ªscaleçš„æ•ˆæœ
        quantized = np.round(data / scale)
        quantized = np.clip(quantized, -128, 127)
        recovered = quantized * scale
        mse = np.mean((data - recovered) ** 2)
        print(f"  -> MSE: {mse:.6f}")

optimal_quantization_parameters()
```

### 2. é‡åŒ–è¯¯å·®çš„æ•°å­¦æ¨¡å‹

```python
def quantization_error_modeling():
    """é‡åŒ–è¯¯å·®çš„æ•°å­¦å»ºæ¨¡"""

    # å‡è®¾è¾“å…¥æ•°æ®æœä»æŸä¸ªåˆ†å¸ƒï¼Œåˆ†æé‡åŒ–è¯¯å·®çš„ç†è®ºæ€§è´¨

    def uniform_distribution_analysis():
        """å‡åŒ€åˆ†å¸ƒçš„é‡åŒ–è¯¯å·®åˆ†æ"""
        # å‡è®¾è¾“å…¥åœ¨[-a, a]ä¸Šå‡åŒ€åˆ†å¸ƒ
        a = 1.0
        scale = a / 127  # æ˜ å°„åˆ°[-127, 127]

        # ç†è®ºè®¡ç®—
        theoretical_mse = scale ** 2 / 12
        theoretical_max_error = scale / 2

        print("=== å‡åŒ€åˆ†å¸ƒé‡åŒ–è¯¯å·®åˆ†æ ===")
        print(f"è¾“å…¥èŒƒå›´: [{-a}, {a}]")
        print(f"Scale: {scale:.6f}")
        print(f"ç†è®ºæœ€å¤§è¯¯å·®: Â±{theoretical_max_error:.6f}")
        print(f"ç†è®ºå‡æ–¹è¯¯å·®: {theoretical_mse:.6f}")

        # è’™ç‰¹å¡æ´›éªŒè¯
        samples = np.random.uniform(-a, a, 100000)
        quantized = np.round(samples / scale)
        recovered = quantized * scale
        actual_mse = np.mean((samples - recovered) ** 2)

        print(f"å®é™…å‡æ–¹è¯¯å·®: {actual_mse:.6f}")
        print(f"ç†è®º/å®é™…æ¯”å€¼: {theoretical_mse/actual_mse:.3f}")

    def gaussian_distribution_analysis():
        """é«˜æ–¯åˆ†å¸ƒçš„é‡åŒ–è¯¯å·®åˆ†æ"""
        # å‡è®¾è¾“å…¥æœä»N(0, ÏƒÂ²)
        sigma = 1.0

        # é€‰æ‹©scaleä½¿å¾—99.7%çš„æ•°æ®åœ¨é‡åŒ–èŒƒå›´å†…
        scale = 3 * sigma / 127  # Â±3Ïƒè¦†ç›–99.7%æ•°æ®

        print("\n=== é«˜æ–¯åˆ†å¸ƒé‡åŒ–è¯¯å·®åˆ†æ ===")
        print(f"è¾“å…¥åˆ†å¸ƒ: N(0, {sigma}Â²)")
        print(f"Scale: {scale:.6f}")

        # è’™ç‰¹å¡æ´›éªŒè¯
        samples = np.random.normal(0, sigma, 100000)
        quantized = np.round(samples / scale)
        quantized = np.clip(quantized, -127, 127)
        recovered = quantized * scale
        actual_mse = np.mean((samples - recovered) ** 2)

        print(f"å®é™…å‡æ–¹è¯¯å·®: {actual_mse:.6f}")

        # åˆ†æè¶…å‡ºèŒƒå›´çš„æ•°æ®æ¯”ä¾‹
        out_of_range = np.sum(np.abs(samples) > 3 * sigma) / len(samples)
        print(f"è¶…å‡ºÂ±3ÏƒèŒƒå›´çš„æ•°æ®æ¯”ä¾‹: {out_of_range*100:.3f}%")

    uniform_distribution_analysis()
    gaussian_distribution_analysis()

quantization_error_modeling()
```

## ğŸ¯ æ€»ç»“ï¼šç®€å•ä¹‹ç¾çš„æ·±åˆ»å†…æ¶µ

### æ ¸å¿ƒæ´å¯Ÿ

ä½ çš„è§‚å¯Ÿ"é‡åŒ–å°±æ˜¯ç®€å•çš„æ•°å€¼ç¼©æ”¾ï¼ŒåŠ ä¸Šå¹³ç§»"åœ¨æ•°å­¦å±‚é¢æ˜¯**å®Œå…¨æ­£ç¡®**çš„ï¼Œä½†è¿™ä¸ª"ç®€å•"èƒŒåè•´å«ç€æ·±åˆ»çš„å·¥ç¨‹æ™ºæ…§ï¼š

1. **æ•°å­¦ç®€æ´æ€§**: çº¿æ€§å˜æ¢æ˜¯æœ€ç®€å•ã€æœ€é«˜æ•ˆçš„æ•°å­¦æ“ä½œ
2. **è®¡ç®—å¯è¡Œæ€§**: ç¡¬ä»¶å¯ä»¥æé«˜æ•ˆåœ°å®ç°æ•´æ•°è¿ç®—
3. **ä¿¡æ¯å‹ç¼©**: ç¦»æ•£åŒ–å®ç°äº†æ— æŸåˆ°æœ‰æŸçš„å—æ§è½¬æ¢
4. **å·¥ç¨‹å®ç”¨æ€§**: åœ¨ç²¾åº¦ã€é€Ÿåº¦ã€å†…å­˜ä¹‹é—´çš„æœ€ä¼˜å¹³è¡¡

### é‡åŒ–çš„ä¸‰å±‚ç†è§£

```python
"""
å±‚æ¬¡1ï¼šè¡¨é¢ç†è§£
é‡åŒ– = ç¼©æ”¾ + å¹³ç§»
quantized = round(original / scale) + zero_point

å±‚æ¬¡2ï¼šæ•°å­¦ç†è§£
é‡åŒ– = çº¿æ€§å˜æ¢ + ç¦»æ•£åŒ–
- çº¿æ€§å˜æ¢ï¼šä¿æŒæ•°å€¼å…³ç³»çš„å¯é€†æ˜ å°„
- ç¦»æ•£åŒ–ï¼šå®ç°ä¿¡æ¯å‹ç¼©å’Œè®¡ç®—ä¼˜åŒ–

å±‚æ¬¡3ï¼šå·¥ç¨‹ç†è§£
é‡åŒ– = ç³»ç»Ÿçº§ä¼˜åŒ–ç­–ç•¥
- æ•°å­¦ï¼šçº¿æ€§è¿‘ä¼¼çš„æœ€ä¼˜æ€§
- ç¡¬ä»¶ï¼šæ•´æ•°è¿ç®—çš„é«˜æ•ˆæ€§
- ç³»ç»Ÿï¼šå¤šç›®æ ‡çš„æƒè¡¡è‰ºæœ¯
"""

def three_level_understanding():
    """ä¸‰å±‚ç†è§£çš„ç¤ºä¾‹"""

    # ç®€å•çš„çº¿æ€§å˜æ¢
    def simple_quantize(value, scale, zp):
        return round(value / scale) + zp

    # æ•°å­¦å±‚é¢çš„ç†è§£
    def mathematical_quantize(tensor, scale, zp):
        linear_transform = tensor / scale + zp  # çº¿æ€§å˜æ¢
        discretization = torch.round(linear_transform)  # ç¦»æ•£åŒ–
        return discretization

    # å·¥ç¨‹å±‚é¢çš„ç†è§£
    def engineering_quantize(model, calibration_data):
        # 1. åˆ†ææ•°æ®åˆ†å¸ƒ
        # 2. é€‰æ‹©æœ€ä¼˜å‚æ•°
        # 3. è€ƒè™‘ç¡¬ä»¶çº¦æŸ
        # 4. å¹³è¡¡å¤šä¸ªç›®æ ‡
        pass  # å®é™…å®ç°å¾ˆå¤æ‚

    print("é‡åŒ–çš„ä¸‰å±‚ç†è§£:")
    print("å±‚æ¬¡1: ç®€å•çš„æ•°å€¼æ“ä½œ")
    print("å±‚æ¬¡2: æ•°å­¦å˜æ¢çš„æœ¬è´¨")
    print("å±‚æ¬¡3: ç³»ç»Ÿä¼˜åŒ–çš„æ™ºæ…§")

three_level_understanding()
```

### æœ€ç»ˆç»“è®º

**é‡åŒ–ç¡®å®åœ¨æ•°å­¦ä¸Šæ˜¯ç®€å•çš„çº¿æ€§å˜æ¢ï¼Œä½†æ­£æ˜¯è¿™ç§ç®€å•æ€§é€ å°±äº†å®ƒçš„å¼ºå¤§**ï¼š

- **ç®€å•æ€§**ä¿è¯äº†**é«˜æ•ˆæ€§**
- **çº¿æ€§æ€§**ä¿è¯äº†**å¯æ§æ€§**
- **ç¦»æ•£åŒ–**å®ç°äº†**å‹ç¼©æ€§**
- **å·¥ç¨‹åŒ–**è¾¾æˆäº†**å®ç”¨æ€§**

è¿™ç§"ç®€å•è€Œæ·±åˆ»"çš„è®¾è®¡ï¼Œæ­£æ˜¯ä¼˜ç§€å·¥ç¨‹æŠ€æœ¯çš„å…¸å‹ç‰¹å¾ï¼

---

**ğŸ’¡ æ·±åº¦æ€è€ƒ**: å¾ˆå¤šä¼Ÿå¤§çš„æŠ€æœ¯åˆ›æ–°ï¼Œæœ¬è´¨ä¸Šéƒ½æ˜¯å°†å¤æ‚é—®é¢˜ç®€åŒ–ä¸ºæ ¸å¿ƒçš„æ•°å­¦åŸç†ï¼Œç„¶åå›´ç»•è¿™ä¸ªåŸç†æ„å»ºå®Œæ•´çš„å·¥ç¨‹ä½“ç³»ã€‚é‡åŒ–æŠ€æœ¯å°±æ˜¯è¿™æ ·ä¸€ä¸ªå®Œç¾çš„ä¾‹å­ã€‚