#!/usr/bin/env python3
"""
PyTorch Building Blocks & Resource Accounting - å¯è§†åŒ–å®è·µæ¼”ç¤º
è¿™ä¸ªè„šæœ¬å°†Lecture 02çš„æ ¸å¿ƒæ¦‚å¿µè½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„ã€å¯è§†åŒ–çš„æ¼”ç¤º

ä½œè€…: CS336 Deep Learning Systems
æ—¥æœŸ: 2025-11-07
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import psutil
import time
import functools
from typing import Dict, List, Any, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')

# é…ç½®ä¸­æ–‡å­—ä½“ï¼Œé¿å…ä¸­æ–‡æ˜¾ç¤ºè­¦å‘Š
import matplotlib.font_manager as fm
import os

def setup_chinese_font():
    """è®¾ç½® matplotlib ä¸­æ–‡å­—ä½“ - å¢å¼ºç‰ˆ"""
    # ä¼˜å…ˆå°è¯•ç›´æ¥ä½¿ç”¨å­—ä½“æ–‡ä»¶è·¯å¾„ï¼ˆæœ€å¯é çš„æ–¹æ³•ï¼‰
    system_font_paths = [
        '/System/Library/Fonts/PingFang.ttc',
        '/System/Library/Fonts/STHeiti Light.ttc',
        '/System/Library/Fonts/STHeiti Medium.ttc',
        '/Library/Fonts/Arial Unicode.ttf',
    ]
    
    font_path = None
    chinese_font_name = None
    
    # æŸ¥æ‰¾å¯ç”¨çš„å­—ä½“æ–‡ä»¶
    for path in system_font_paths:
        if os.path.exists(path):
            font_path = path
            try:
                # è·å–å­—ä½“åç§°
                font_prop = fm.FontProperties(fname=path)
                chinese_font_name = font_prop.get_name()
                break
            except:
                continue
    
    # å¦‚æœæ‰¾åˆ°å­—ä½“æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨æ–‡ä»¶è·¯å¾„
    if font_path:
        try:
            # å°†å­—ä½“æ·»åŠ åˆ°matplotlibçš„å­—ä½“åˆ—è¡¨
            try:
                fm.fontManager.addfont(font_path)
            except (AttributeError, ValueError):
                # æ—§ç‰ˆæœ¬æˆ–å­—ä½“å·²å­˜åœ¨
                pass
            
            # ä½¿ç”¨å­—ä½“æ–‡ä»¶è·¯å¾„è®¾ç½®å…¨å±€å­—ä½“
            plt.rcParams['font.sans-serif'] = [chinese_font_name] + ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            print(f"âœ“ å·²è®¾ç½®ä¸­æ–‡å­—ä½“: {chinese_font_name} (æ¥è‡ª {font_path})")
            return font_path  # è¿”å›å­—ä½“è·¯å¾„ï¼Œç”¨äºåç»­æ˜¾å¼æŒ‡å®š
        except Exception as e:
            print(f"âš  è®¾ç½®å­—ä½“æ–‡ä»¶å¤±è´¥: {e}")
    
    # å¤‡ç”¨æ–¹æ¡ˆï¼šé€šè¿‡å­—ä½“åç§°æŸ¥æ‰¾
    mac_fonts = ['PingFang SC', 'STHeiti', 'Arial Unicode MS', 'Heiti TC', 'SimHei', 'Hiragino Sans GB']
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    
    for font_name in mac_fonts:
        if font_name in available_fonts:
            plt.rcParams['font.sans-serif'] = [font_name] + ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            print(f"âœ“ å·²è®¾ç½®ä¸­æ–‡å­—ä½“: {font_name}")
            return font_name
    
    # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'Arial', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    print("âš  æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨å¤‡ç”¨å­—ä½“è®¾ç½®")
    return None

# åˆå§‹åŒ–å­—ä½“è®¾ç½®ï¼ˆåœ¨å¯¼å…¥å…¶ä»–æ¨¡å—ä¹‹å‰ï¼‰
chinese_font_name = setup_chinese_font()

# åˆ›å»ºå­—ä½“å±æ€§å¯¹è±¡ï¼Œç”¨äºæ˜¾å¼æŒ‡å®šå­—ä½“
def get_chinese_font_prop(size=12):
    """è·å–ä¸­æ–‡å­—ä½“å±æ€§å¯¹è±¡"""
    if chinese_font_name:
        # å¦‚æœæ˜¯å­—ä½“æ–‡ä»¶è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
        if isinstance(chinese_font_name, str) and os.path.exists(chinese_font_name):
            return fm.FontProperties(fname=chinese_font_name, size=size)
        else:
            # å¦‚æœæ˜¯å­—ä½“åç§°ï¼Œä½¿ç”¨familyå‚æ•°
            return fm.FontProperties(family=chinese_font_name, size=size)
    else:
        return fm.FontProperties(family='sans-serif', size=size)

# è®¾ç½®ç»˜å›¾é£æ ¼
sns.set_style("whitegrid")
plt.style.use('seaborn-v0_8-darkgrid')

print("ğŸš€ PyTorch Building Blocks & Resource Accounting - å¯è§†åŒ–å®è·µæ¼”ç¤º")
print("=" * 80)

class FLOPCalculator:
    """PyTorchæ¨¡å‹FLOPè®¡ç®—å™¨ - å¢å¼ºç‰ˆ"""

    def __init__(self):
        self.flops = 0
        self.layer_flops = {}
        self.hooks = []

    def _conv_flop(self, input_shape: tuple, output_shape: tuple,
                   kernel_shape: tuple, groups: int = 1) -> int:
        """è®¡ç®—å·ç§¯æ“ä½œFLOP"""
        batch_size = input_shape[0]
        output_dims = output_shape[2:]
        kernel_dims = kernel_shape[2:]
        in_channels = input_shape[1]
        out_channels = output_shape[1]

        filters_per_channel = out_channels // groups
        conv_per_position_flops = functools.reduce(
            lambda a, b: a * b, kernel_dims) * in_channels // groups
        active_elements_count = batch_size * functools.reduce(
            lambda a, b: a * b, output_dims)

        overall_conv_flops = conv_per_position_flops * active_elements_count * filters_per_channel
        bias_flops = out_channels * active_elements_count

        return overall_conv_flops + bias_flops

    def _linear_flop(self, input_shape: tuple, weight_shape: tuple,
                     has_bias: bool = True) -> int:
        """è®¡ç®—çº¿æ€§å±‚FLOP"""
        batch_size = input_shape[0]
        in_features = input_shape[1]
        out_features = weight_shape[0]

        mul_flops = batch_size * in_features * out_features
        add_flops = batch_size * out_features if has_bias else 0

        return mul_flops + add_flops

    def _create_hook(self, layer_name: str, layer_type: str):
        """åˆ›å»ºFLOPè®¡ç®—é’©å­"""
        def hook_fn(module, input, output):
            if layer_type == 'conv2d':
                if hasattr(module, 'weight'):
                    kernel_shape = module.weight.shape
                    input_shape = input[0].shape
                    output_shape = output.shape
                    flops = self._conv_flop(input_shape, output_shape,
                                          kernel_shape, module.groups)
                    self.flops += flops
                    self.layer_flops[layer_name] = flops

            elif layer_type == 'linear':
                if hasattr(module, 'weight'):
                    input_shape = input[0].shape
                    weight_shape = module.weight.shape
                    has_bias = hasattr(module, 'bias') and module.bias is not None
                    flops = self._linear_flop(input_shape, weight_shape, has_bias)
                    self.flops += flops
                    self.layer_flops[layer_name] = flops

        return hook_fn

    def analyze_model(self, model: nn.Module, input_shape: tuple) -> Dict[str, Any]:
        """åˆ†ææ¨¡å‹FLOP"""
        self.flops = 0
        self.layer_flops = {}
        self.hooks = []

        # æ³¨å†Œé’©å­
        for name, module in model.named_modules():
            if name:  # è·³è¿‡æ ¹æ¨¡å—
                if isinstance(module, nn.Conv2d):
                    hook = module.register_forward_hook(
                        self._create_hook(name, 'conv2d'))
                    self.hooks.append(hook)

                elif isinstance(module, nn.Linear):
                    hook = module.register_forward_hook(
                        self._create_hook(name, 'linear'))
                    self.hooks.append(hook)

        # è¿è¡Œå‰å‘ä¼ æ’­
        device = next(model.parameters()).device
        dummy_input = torch.randn(input_shape, device=device)

        with torch.no_grad():
            _ = model(dummy_input)

        # æ¸…ç†é’©å­
        for hook in self.hooks:
            hook.remove()

        return {
            'total_flops': self.flops,
            'layer_flops': self.layer_flops,
            'flops_readable': self._format_flops(self.flops),
            'parameters': sum(p.numel() for p in model.parameters()),
            'parameters_readable': self._format_count(
                sum(p.numel() for p in model.parameters()))
        }

    def _format_flops(self, flops: int) -> str:
        """æ ¼å¼åŒ–FLOPæ˜¾ç¤º"""
        if flops >= 1e15:
            return f"{flops/1e15:.2f} PFLOP"
        elif flops >= 1e12:
            return f"{flops/1e12:.2f} TFLOP"
        elif flops >= 1e9:
            return f"{flops/1e9:.2f} GFLOP"
        elif flops >= 1e6:
            return f"{flops/1e6:.2f} MFLOP"
        elif flops >= 1e3:
            return f"{flops/1e3:.2f} KFLOP"
        else:
            return f"{flops} FLOP"

    def _format_count(self, count: int) -> str:
        """æ ¼å¼åŒ–æ•°é‡æ˜¾ç¤º"""
        if count >= 1e9:
            return f"{count/1e9:.2f} B"
        elif count >= 1e6:
            return f"{count/1e6:.2f} M"
        elif count >= 1e3:
            return f"{count/1e3:.2f} K"
        else:
            return str(count)


class MemoryProfiler:
    """GPU/CPUå†…å­˜ä½¿ç”¨åˆ†æå™¨ - å¢å¼ºç‰ˆ"""

    def __init__(self):
        self.snapshots = []
        self.peak_memory = {'cpu': 0, 'gpu': 0}

    def snapshot(self, label: str = ""):
        """æ‹æ‘„å†…å­˜å¿«ç…§"""
        # CPUå†…å­˜
        cpu_memory = psutil.virtual_memory()
        cpu_used = cpu_memory.used / 1024**3  # GB

        # GPUå†…å­˜
        gpu_memory = {'allocated': 0, 'cached': 0, 'max_allocated': 0}
        if torch.cuda.is_available():
            gpu_memory['allocated'] = torch.cuda.memory_allocated() / 1024**3
            gpu_memory['cached'] = torch.cuda.memory_reserved() / 1024**3
            gpu_memory['max_allocated'] = torch.cuda.max_memory_allocated() / 1024**3

        snapshot = {
            'timestamp': time.time(),
            'label': label,
            'cpu_used_gb': cpu_used,
            'cpu_percent': cpu_memory.percent,
            **gpu_memory
        }

        self.snapshots.append(snapshot)

        # æ›´æ–°å³°å€¼
        self.peak_memory['cpu'] = max(self.peak_memory['cpu'], cpu_used)
        if torch.cuda.is_available():
            self.peak_memory['gpu'] = max(self.peak_memory['gpu'], gpu_memory['allocated'])

        return snapshot

    def reset(self):
        """é‡ç½®åˆ†æå™¨"""
        self.snapshots = []
        self.peak_memory = {'cpu': 0, 'gpu': 0}
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()

    def get_memory_analysis(self, model: nn.Module, param_memory: float,
                          activation_memory: float) -> Dict[str, float]:
        """è·å–å†…å­˜åˆ†æç»“æœ"""
        grad_memory = param_memory  # æ¢¯åº¦å†…å­˜ä¸å‚æ•°ç›¸åŒ
        optimizer_memory = param_memory * 2  # Adamä¼˜åŒ–å™¨çŠ¶æ€

        return {
            'parameter_memory_gb': param_memory,
            'gradient_memory_gb': grad_memory,
            'optimizer_memory_gb': optimizer_memory,
            'activation_memory_gb': activation_memory,
            'total_training_memory_gb': param_memory + grad_memory + optimizer_memory + activation_memory,
            'total_inference_memory_gb': param_memory + activation_memory
        }


def visualize_flop_analysis():
    """å¯è§†åŒ–FLOPåˆ†æ"""
    print("\nğŸ§® FLOPåˆ†æå¯è§†åŒ–æ¼”ç¤º")
    print("=" * 50)

    # åˆ›å»ºä¸åŒè§„æ¨¡çš„æ¨¡å‹
    models = {
        'Small CNN': nn.Sequential(
            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(),
            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),
            nn.Linear(32, 10)
        ),
        'Medium CNN': nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),
            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),
            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),
            nn.Linear(128, 10)
        ),
        'Large CNN': nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(),
            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),
            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),
            nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),
            nn.Linear(512, 10)
        )
    }

    input_shape = (1, 3, 32, 32)
    results = {}

    # åˆ†ææ¯ä¸ªæ¨¡å‹
    for name, model in models.items():
        calculator = FLOPCalculator()
        result = calculator.analyze_model(model, input_shape)
        results[name] = result
        print(f"{name}: {result['flops_readable']}, {result['parameters_readable']} å‚æ•°")

    # åˆ›å»ºå¯è§†åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('FLOPåˆ†æå¯è§†åŒ–', fontsize=16, fontweight='bold', 
                 fontproperties=get_chinese_font_prop(16))

    # 1. FLOPå¯¹æ¯”æŸ±çŠ¶å›¾
    model_names = list(results.keys())
    flop_values = [results[name]['total_flops'] / 1e6 for name in model_names]  # MFLOP

    bars = axes[0, 0].bar(model_names, flop_values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
    axes[0, 0].set_title('æ¨¡å‹FLOPå¯¹æ¯”', fontsize=14, fontweight='bold',
                         fontproperties=get_chinese_font_prop(14))
    axes[0, 0].set_ylabel('FLOP (MFLOP)')
    axes[0, 0].tick_params(axis='x', rotation=45)

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, flop_values):
        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(flop_values)*0.01,
                       f'{value:.1f}', ha='center', va='bottom', fontweight='bold')

    # 2. å‚æ•°æ•°é‡å¯¹æ¯”
    param_values = [results[name]['parameters'] for name in model_names]

    bars = axes[0, 1].bar(model_names, param_values, color=['#95E77E', '#FFD93D', '#FF6BCB'])
    axes[0, 1].set_title('æ¨¡å‹å‚æ•°æ•°é‡å¯¹æ¯”', fontsize=14, fontweight='bold',
                         fontproperties=get_chinese_font_prop(14))
    axes[0, 1].set_ylabel('å‚æ•°æ•°é‡', fontproperties=get_chinese_font_prop())
    axes[0, 1].tick_params(axis='x', rotation=45)

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, param_values):
        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(param_values)*0.01,
                       f'{value:,}', ha='center', va='bottom', fontweight='bold')

    # 3. FLOP vs å‚æ•°æ•£ç‚¹å›¾
    axes[1, 0].scatter(param_values, flop_values,
                       s=200, c=['#FF6B6B', '#4ECDC4', '#45B7D1'],
                       alpha=0.7, edgecolors='black')

    for i, name in enumerate(model_names):
        axes[1, 0].annotate(name, (param_values[i], flop_values[i]),
                           xytext=(5, 5), textcoords='offset points', fontweight='bold')

    axes[1, 0].set_xlabel('å‚æ•°æ•°é‡', fontproperties=get_chinese_font_prop())
    axes[1, 0].set_ylabel('FLOP (MFLOP)')
    axes[1, 0].set_title('FLOP vs å‚æ•°æ•°é‡å…³ç³»', fontsize=14, fontweight='bold',
                         fontproperties=get_chinese_font_prop(14))

    # 4. æ•ˆç‡æŒ‡æ ‡ï¼ˆFLOP/å‚æ•°ï¼‰
    efficiency = [flop/param for flop, param in zip(flop_values, param_values)]

    bars = axes[1, 1].bar(model_names, efficiency, color=['#C9B6E5', '#FFB6B9', '#B6E5D8'])
    axes[1, 1].set_title('è®¡ç®—æ•ˆç‡ (FLOP/å‚æ•°)', fontsize=14, fontweight='bold',
                         fontproperties=get_chinese_font_prop(14))
    axes[1, 1].set_ylabel('FLOP per å‚æ•°', fontproperties=get_chinese_font_prop())
    axes[1, 1].tick_params(axis='x', rotation=45)

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, efficiency):
        axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(efficiency)*0.01,
                       f'{value:.2f}', ha='center', va='bottom', fontweight='bold')

    plt.tight_layout()
    plt.savefig('/Users/peixingxin/code/spring2025-lectures/æ·±åº¦è®¨è®º/FLOPåˆ†æå¯è§†åŒ–.png',
                dpi=300, bbox_inches='tight')
    plt.show()


def visualize_memory_analysis():
    """å¯è§†åŒ–å†…å­˜åˆ†æ"""
    print("\nğŸ’¾ å†…å­˜åˆ†æå¯è§†åŒ–æ¼”ç¤º")
    print("=" * 50)

    # åˆ›å»ºä¸åŒè§„æ¨¡çš„æ¨¡å‹è¿›è¡Œå†…å­˜åˆ†æ
    model_configs = [
        {'name': 'Small', 'layers': [256, 128, 64], 'batch_size': 32},
        {'name': 'Medium', 'layers': [512, 256, 128], 'batch_size': 16},
        {'name': 'Large', 'layers': [1024, 512, 256], 'batch_size': 8}
    ]

    results = {}

    for config in model_configs:
        # åˆ›å»ºæ¨¡å‹
        layers = []
        input_dim = config['layers'][0]
        for hidden_dim in config['layers'][1:]:
            layers.extend([nn.Linear(input_dim, hidden_dim), nn.ReLU()])
            input_dim = hidden_dim
        layers.append(nn.Linear(input_dim, 10))

        model = nn.Sequential(*layers)

        # è®¡ç®—å‚æ•°å†…å­˜
        param_memory = sum(p.numel() * p.element_size() for p in model.parameters()) / 1024**3

        # ä¼°ç®—æ¿€æ´»å†…å­˜ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
        activation_memory = param_memory * 0.5  # ç²—ç•¥ä¼°ç®—

        # è·å–å†…å­˜åˆ†æ
        profiler = MemoryProfiler()
        analysis = profiler.get_memory_analysis(model, param_memory, activation_memory)

        results[config['name']] = {
            'param_memory': param_memory,
            'activation_memory': activation_memory,
            'training_memory': analysis['total_training_memory_gb'],
            'inference_memory': analysis['total_inference_memory_gb']
        }

        print(f"{config['name']} æ¨¡å‹:")
        print(f"  å‚æ•°å†…å­˜: {param_memory:.3f} GB")
        print(f"  æ¿€æ´»å†…å­˜: {activation_memory:.3f} GB")
        print(f"  è®­ç»ƒå†…å­˜: {analysis['total_training_memory_gb']:.3f} GB")
        print(f"  æ¨ç†å†…å­˜: {analysis['total_inference_memory_gb']:.3f} GB")

    # åˆ›å»ºå¯è§†åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('å†…å­˜åˆ†æå¯è§†åŒ–', fontsize=16, fontweight='bold',
                 fontproperties=get_chinese_font_prop(16))

    model_names = list(results.keys())

    # 1. å†…å­˜ç»„æˆå †å å›¾ï¼ˆè®­ç»ƒï¼‰
    param_mem = [results[name]['param_memory'] for name in model_names]
    act_mem = [results[name]['activation_memory'] for name in model_names]
    grad_mem = param_mem.copy()  # æ¢¯åº¦å†…å­˜=å‚æ•°å†…å­˜
    opt_mem = [p * 2 for p in param_mem]  # ä¼˜åŒ–å™¨å†…å­˜=2*å‚æ•°å†…å­˜

    x = np.arange(len(model_names))
    width = 0.6
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFD93D']
    labels = ['å‚æ•°', 'æ¿€æ´»', 'æ¢¯åº¦', 'ä¼˜åŒ–å™¨']
    
    # åˆ›å»ºå †å æŸ±çŠ¶å›¾
    axes[0, 0].bar(x, param_mem, width, label=labels[0], color=colors[0], alpha=0.8)
    bottom = param_mem
    axes[0, 0].bar(x, act_mem, width, bottom=bottom, label=labels[1], color=colors[1], alpha=0.8)
    bottom = [b + a for b, a in zip(bottom, act_mem)]
    axes[0, 0].bar(x, grad_mem, width, bottom=bottom, label=labels[2], color=colors[2], alpha=0.8)
    bottom = [b + g for b, g in zip(bottom, grad_mem)]
    axes[0, 0].bar(x, opt_mem, width, bottom=bottom, label=labels[3], color=colors[3], alpha=0.8)
    
    axes[0, 0].set_title('è®­ç»ƒå†…å­˜ç»„æˆ', fontsize=14, fontweight='bold',
                         fontproperties=get_chinese_font_prop(14))
    axes[0, 0].set_ylabel('å†…å­˜ (GB)', fontproperties=get_chinese_font_prop())
    axes[0, 0].set_xticks(x)
    axes[0, 0].set_xticklabels(model_names)
    axes[0, 0].legend(prop=get_chinese_font_prop(10))

    # 2. è®­ç»ƒ vs æ¨ç†å†…å­˜å¯¹æ¯”
    training_mem = [results[name]['training_memory'] for name in model_names]
    inference_mem = [results[name]['inference_memory'] for name in model_names]

    x = np.arange(len(model_names))
    width = 0.35

    axes[0, 1].bar(x - width/2, training_mem, width, label='è®­ç»ƒ', color='#FF6B6B', alpha=0.8)
    axes[0, 1].bar(x + width/2, inference_mem, width, label='æ¨ç†', color='#4ECDC4', alpha=0.8)

    axes[0, 1].set_title('è®­ç»ƒ vs æ¨ç†å†…å­˜å¯¹æ¯”', fontsize=14, fontweight='bold',
                        fontproperties=get_chinese_font_prop(14))
    axes[0, 1].set_ylabel('å†…å­˜ (GB)', fontproperties=get_chinese_font_prop())
    axes[0, 1].set_xticks(x)
    axes[0, 1].set_xticklabels(model_names)
    axes[0, 1].legend(prop=get_chinese_font_prop(10))

    # 3. å†…å­˜å¢é•¿è¶‹åŠ¿
    model_sizes = [sum(p.numel() for p in nn.Sequential(
        *[nn.Linear(config['layers'][i] if i == 0 else config['layers'][i-1],
                    config['layers'][i]) for i in range(len(config['layers']))]
    ).parameters()) for config in model_configs]

    axes[1, 0].plot(model_sizes, training_mem, 'o-', linewidth=3, markersize=8,
                    color='#FF6B6B', label='è®­ç»ƒå†…å­˜')
    axes[1, 0].plot(model_sizes, inference_mem, 's-', linewidth=3, markersize=8,
                    color='#4ECDC4', label='æ¨ç†å†…å­˜')

    axes[1, 0].set_xlabel('æ¨¡å‹å‚æ•°æ•°é‡', fontproperties=get_chinese_font_prop())
    axes[1, 0].set_ylabel('å†…å­˜ (GB)', fontproperties=get_chinese_font_prop())
    axes[1, 0].set_title('å†…å­˜å¢é•¿è¶‹åŠ¿', fontsize=14, fontweight='bold',
                         fontproperties=get_chinese_font_prop(14))
    axes[1, 0].legend(prop=get_chinese_font_prop(10))
    axes[1, 0].grid(True, alpha=0.3)

    # 4. å†…å­˜æ•ˆç‡ï¼ˆæ¨ç†å†…å­˜/å‚æ•°ï¼‰
    memory_efficiency = [results[name]['inference_memory'] / results[name]['param_memory']
                        for name in model_names]

    bars = axes[1, 1].bar(model_names, memory_efficiency,
                          color=['#95E77E', '#FFD93D', '#FF6BCB'])
    axes[1, 1].set_title('å†…å­˜æ•ˆç‡ (æ¨ç†å†…å­˜/å‚æ•°å†…å­˜)', fontsize=14, fontweight='bold',
                         fontproperties=get_chinese_font_prop(14))
    axes[1, 1].set_ylabel('æ•ˆç‡æ¯”ç‡', fontproperties=get_chinese_font_prop())
    axes[1, 1].tick_params(axis='x', rotation=45)

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, memory_efficiency):
        axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(memory_efficiency)*0.01,
                       f'{value:.2f}', ha='center', va='bottom', fontweight='bold')

    plt.tight_layout()
    plt.savefig('/Users/peixingxin/code/spring2025-lectures/æ·±åº¦è®¨è®º/å†…å­˜åˆ†æå¯è§†åŒ–.png',
                dpi=300, bbox_inches='tight')
    plt.show()


def visualize_training_performance():
    """å¯è§†åŒ–è®­ç»ƒæ€§èƒ½"""
    print("\nğŸ”„ è®­ç»ƒæ€§èƒ½å¯è§†åŒ–æ¼”ç¤º")
    print("=" * 50)

    # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
    epochs = 50

    # æ¨¡æ‹Ÿä¸åŒçš„è®­ç»ƒåœºæ™¯
    scenarios = {
        'åŸºçº¿è®­ç»ƒ': {
            'train_loss': np.linspace(2.0, 0.5, epochs) + np.random.normal(0, 0.05, epochs),
            'val_loss': np.linspace(2.1, 0.6, epochs) + np.random.normal(0, 0.08, epochs),
            'throughput': np.random.uniform(50, 60, epochs),
            'color': '#FF6B6B'
        },
        'æ··åˆç²¾åº¦è®­ç»ƒ': {
            'train_loss': np.linspace(2.0, 0.45, epochs) + np.random.normal(0, 0.04, epochs),
            'val_loss': np.linspace(2.1, 0.55, epochs) + np.random.normal(0, 0.06, epochs),
            'throughput': np.random.uniform(80, 95, epochs),
            'color': '#4ECDC4'
        },
        'æ¢¯åº¦ç´¯ç§¯': {
            'train_loss': np.linspace(2.0, 0.48, epochs) + np.random.normal(0, 0.045, epochs),
            'val_loss': np.linspace(2.1, 0.58, epochs) + np.random.normal(0, 0.07, epochs),
            'throughput': np.random.uniform(45, 55, epochs),
            'color': '#45B7D1'
        }
    }

    # åˆ›å»ºå¯è§†åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('è®­ç»ƒæ€§èƒ½å¯è§†åŒ–å¯¹æ¯”', fontsize=16, fontweight='bold')

    epoch_range = range(1, epochs + 1)

    # 1. è®­ç»ƒæŸå¤±æ›²çº¿
    for name, data in scenarios.items():
        axes[0, 0].plot(epoch_range, data['train_loss'], linewidth=2, label=name,
                       color=data['color'], alpha=0.8)

    axes[0, 0].set_title('è®­ç»ƒæŸå¤±å˜åŒ–', fontsize=14, fontweight='bold')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('æŸå¤±')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # 2. éªŒè¯æŸå¤±æ›²çº¿
    for name, data in scenarios.items():
        axes[0, 1].plot(epoch_range, data['val_loss'], linewidth=2, label=name,
                       color=data['color'], alpha=0.8)

    axes[0, 1].set_title('éªŒè¯æŸå¤±å˜åŒ–', fontsize=14, fontweight='bold')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('æŸå¤±')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)

    # 3. ååé‡å¯¹æ¯”
    for name, data in scenarios.items():
        axes[1, 0].plot(epoch_range, data['throughput'], linewidth=2, label=name,
                       color=data['color'], alpha=0.8)

    axes[1, 0].set_title('è®­ç»ƒååé‡', fontsize=14, fontweight='bold')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('æ ·æœ¬/ç§’')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)

    # 4. æœ€ç»ˆæ€§èƒ½å¯¹æ¯”é›·è¾¾å›¾
    categories = ['æœ€ç»ˆç²¾åº¦', 'è®­ç»ƒé€Ÿåº¦', 'å†…å­˜æ•ˆç‡', 'ç¨³å®šæ€§', 'æ”¶æ•›é€Ÿåº¦']

    # è®¡ç®—å„é¡¹æŒ‡æ ‡ï¼ˆå½’ä¸€åŒ–åˆ°0-1ï¼‰
    final_scores = {}
    for name, data in scenarios.items():
        final_loss = data['val_loss'][-1]
        avg_throughput = np.mean(data['throughput'])
        loss_std = np.std(data['val_loss'][-10:])  # æœ€å10ä¸ªepochçš„æ ‡å‡†å·®

        scores = [
            1.0 - (final_loss / 2.1),  # ç²¾åº¦ (æŸå¤±è¶Šä½è¶Šå¥½)
            avg_throughput / 100,       # é€Ÿåº¦ (å½’ä¸€åŒ–åˆ°100)
            0.8 if name == 'æ··åˆç²¾åº¦è®­ç»ƒ' else 0.6,  # å†…å­˜æ•ˆç‡
            1.0 - (loss_std / 0.1),    # ç¨³å®šæ€§
            1.0 - (data['val_loss'][10] / data['val_loss'][0])  # æ”¶æ•›é€Ÿåº¦
        ]
        final_scores[name] = scores

    # ç»˜åˆ¶é›·è¾¾å›¾
    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
    angles += angles[:1]  # é—­åˆå›¾å½¢

    ax_radar = plt.subplot(2, 2, 4, projection='polar')

    for name, scores in final_scores.items():
        scores += scores[:1]  # é—­åˆå›¾å½¢
        ax_radar.plot(angles, scores, 'o-', linewidth=2, label=name,
                     color=scenarios[name]['color'], markersize=6)
        ax_radar.fill(angles, scores, alpha=0.25, color=scenarios[name]['color'])

    ax_radar.set_xticks(angles[:-1])
    ax_radar.set_xticklabels(categories)
    ax_radar.set_ylim(0, 1)
    ax_radar.set_title('ç»¼åˆæ€§èƒ½å¯¹æ¯”', fontsize=14, fontweight='bold', pad=20)
    ax_radar.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))

    plt.tight_layout()
    plt.savefig('/Users/peixingxin/code/spring2025-lectures/æ·±åº¦è®¨è®º/è®­ç»ƒæ€§èƒ½å¯è§†åŒ–.png',
                dpi=300, bbox_inches='tight')
    plt.show()


def demonstrate_mixed_precision_benefits():
    """æ¼”ç¤ºæ··åˆç²¾åº¦è®­ç»ƒçš„å®é™…å¥½å¤„"""
    print("\nâš¡ æ··åˆç²¾åº¦è®­ç»ƒæ•ˆç›Šæ¼”ç¤º")
    print("=" * 50)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    model = nn.Sequential(
        nn.Linear(2048, 4096),
        nn.ReLU(),
        nn.Linear(4096, 2048),
        nn.ReLU(),
        nn.Linear(2048, 1000)
    ).to(device)

    batch_size = 64
    input_data = torch.randn(batch_size, 2048, device=device)
    target = torch.randint(0, 1000, (batch_size,), device=device)

    results = {}

    # FP32 åŸºå‡†æµ‹è¯•
    print("\nğŸ”¢ FP32 åŸºå‡†æµ‹è¯•:")
    model_fp32 = model.float()
    optimizer_fp32 = optim.Adam(model_fp32.parameters())

    # é¢„çƒ­
    for _ in range(5):
        optimizer_fp32.zero_grad()
        output = model_fp32(input_data)
        loss = nn.CrossEntropyLoss()(output, target)
        loss.backward()
        optimizer_fp32.step()

    # æ­£å¼æµ‹è¯•
    if torch.cuda.is_available():
        torch.cuda.synchronize()

    start_time = time.time()
    total_loss = 0

    for _ in range(50):
        optimizer_fp32.zero_grad()
        output = model_fp32(input_data)
        loss = nn.CrossEntropyLoss()(output, target)
        loss.backward()
        optimizer_fp32.step()
        total_loss += loss.item()

    if torch.cuda.is_available():
        torch.cuda.synchronize()

    fp32_time = time.time() - start_time
    fp32_memory = torch.cuda.max_memory_allocated() / 1024**3 if torch.cuda.is_available() else 0

    results['FP32'] = {
        'time': fp32_time,
        'memory': fp32_memory,
        'loss': total_loss / 50
    }

    print(f"  å¹³å‡æ—¶é—´: {fp32_time/50*1000:.2f}ms/iter")
    print(f"  å³°å€¼å†…å­˜: {fp32_memory:.2f}GB")
    print(f"  å¹³å‡æŸå¤±: {results['FP32']['loss']:.4f}")

    if torch.cuda.is_available():
        # æ··åˆç²¾åº¦æµ‹è¯•
        print("\nğŸ”€ æ··åˆç²¾åº¦è®­ç»ƒæµ‹è¯•:")
        model_amp = model
        optimizer_amp = optim.Adam(model_amp.parameters())
        scaler = torch.cuda.amp.GradScaler()

        torch.cuda.reset_peak_memory_stats()

        # é¢„çƒ­
        for _ in range(5):
            optimizer_amp.zero_grad()
            with torch.cuda.amp.autocast():
                output = model_amp(input_data)
                loss = nn.CrossEntropyLoss()(output, target)
            scaler.scale(loss).backward()
            scaler.step(optimizer_amp)
            scaler.update()

        torch.cuda.synchronize()
        start_time = time.time()
        total_loss = 0

        for _ in range(50):
            optimizer_amp.zero_grad()
            with torch.cuda.amp.autocast():
                output = model_amp(input_data)
                loss = nn.CrossEntropyLoss()(output, target)
            scaler.scale(loss).backward()
            scaler.step(optimizer_amp)
            scaler.update()
            total_loss += loss.item()

        torch.cuda.synchronize()
        amp_time = time.time() - start_time
        amp_memory = torch.cuda.max_memory_allocated() / 1024**3

        results['AMP'] = {
            'time': amp_time,
            'memory': amp_memory,
            'loss': total_loss / 50
        }

        print(f"  å¹³å‡æ—¶é—´: {amp_time/50*1000:.2f}ms/iter")
        print(f"  å³°å€¼å†…å­˜: {amp_memory:.2f}GB")
        print(f"  å¹³å‡æŸå¤±: {results['AMP']['loss']:.4f}")

        # æ€§èƒ½å¯¹æ¯”
        speedup = fp32_time / amp_time
        memory_saved = (fp32_memory - amp_memory) / fp32_memory * 100

        print(f"\nğŸ“Š æ€§èƒ½æå‡:")
        print(f"  åŠ é€Ÿæ¯”: {speedup:.2f}x")
        print(f"  å†…å­˜èŠ‚çœ: {memory_saved:.1f}%")

        # å¯è§†åŒ–å¯¹æ¯”
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        fig.suptitle('æ··åˆç²¾åº¦è®­ç»ƒæ•ˆç›Šå¯¹æ¯”', fontsize=16, fontweight='bold')

        # æ—¶é—´å¯¹æ¯”
        times = [fp32_time, amp_time]
        labels = ['FP32', 'æ··åˆç²¾åº¦']
        colors = ['#FF6B6B', '#4ECDC4']

        bars = axes[0].bar(labels, times, color=colors, alpha=0.8)
        axes[0].set_title('è®­ç»ƒæ—¶é—´å¯¹æ¯” (50 iterations)', fontweight='bold')
        axes[0].set_ylabel('æ—¶é—´ (ç§’)')

        for bar, value in zip(bars, times):
            axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(times)*0.01,
                        f'{value:.3f}s', ha='center', va='bottom', fontweight='bold')

        # å†…å­˜å¯¹æ¯”
        memories = [fp32_memory, amp_memory]

        bars = axes[1].bar(labels, memories, color=colors, alpha=0.8)
        axes[1].set_title('å†…å­˜ä½¿ç”¨å¯¹æ¯”', fontweight='bold')
        axes[1].set_ylabel('å†…å­˜ (GB)')

        for bar, value in zip(bars, memories):
            axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(memories)*0.01,
                        f'{value:.2f}GB', ha='center', va='bottom', fontweight='bold')

        # æ•ˆç›ŠæŒ‡æ ‡
        metrics = [f'{speedup:.2f}x', f'{memory_saved:.1f}%']
        metric_labels = ['åŠ é€Ÿæ¯”', 'å†…å­˜èŠ‚çœ']

        bars = axes[2].bar(metric_labels, [float(m.rstrip('x%')) for m in metrics],
                          color=['#45B7D1', '#FFD93D'], alpha=0.8)
        axes[2].set_title('æ€§èƒ½æå‡æŒ‡æ ‡', fontweight='bold')
        axes[2].set_ylabel('æå‡å¹…åº¦')

        for bar, value, label in zip(bars, [float(m.rstrip('x%')) for m in metrics], metrics):
            axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() +
                        float(metrics[0].rstrip('x%'))*0.01, label,
                        ha='center', va='bottom', fontweight='bold')

        plt.tight_layout()
        plt.savefig('/Users/peixingxin/code/spring2025-lectures/æ·±åº¦è®¨è®º/æ··åˆç²¾åº¦æ•ˆç›Šå¯¹æ¯”.png',
                    dpi=300, bbox_inches='tight')
        plt.show()


def create_comprehensive_dashboard():
    """åˆ›å»ºç»¼åˆæ€§èƒ½ä»ªè¡¨æ¿"""
    print("\nğŸ“Š ç»¼åˆæ€§èƒ½ä»ªè¡¨æ¿")
    print("=" * 50)

    # æ¨¡æ‹Ÿä¸åŒè§„æ¨¡æ¨¡å‹çš„ç»¼åˆæ•°æ®
    model_configs = [
        {'name': 'ResNet-18', 'params': '11.7M', 'flops': '1.8 GFLOP', 'memory': '0.8GB', 'accuracy': '69.8%'},
        {'name': 'ResNet-34', 'params': '21.8M', 'flops': '3.7 GFLOP', 'memory': '1.2GB', 'accuracy': '73.3%'},
        {'name': 'ResNet-50', 'params': '25.6M', 'flops': '4.1 GFLOP', 'memory': '1.5GB', 'accuracy': '76.2%'},
        {'name': 'MobileNet-V2', 'params': '3.5M', 'flops': '0.3 GFLOP', 'memory': '0.3GB', 'accuracy': '71.8%'},
        {'name': 'EfficientNet-B0', 'params': '5.3M', 'flops': '0.4 GFLOP', 'memory': '0.4GB', 'accuracy': '77.1%'}
    ]

    # è§£ææ•°æ®
    names = [config['name'] for config in model_configs]
    params = [float(config['params'].rstrip('M')) for config in model_configs]
    flops = [float(config['flops'].split()[0]) for config in model_configs]
    memory = [float(config['memory'].rstrip('GB')) for config in model_configs]
    accuracy = [float(config['accuracy'].rstrip('%')) for config in model_configs]

    # åˆ›å»ºç»¼åˆä»ªè¡¨æ¿
    fig = plt.figure(figsize=(20, 12))
    gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)

    fig.suptitle('æ·±åº¦å­¦ä¹ æ¨¡å‹ç»¼åˆæ€§èƒ½ä»ªè¡¨æ¿', fontsize=20, fontweight='bold')

    # 1. å‚æ•° vs FLOP æ•£ç‚¹å›¾
    ax1 = fig.add_subplot(gs[0, :2])
    scatter = ax1.scatter(params, flops, s=[m * 500 for m in memory], c=accuracy,
                         cmap='viridis', alpha=0.7, edgecolors='black')

    for i, name in enumerate(names):
        ax1.annotate(name, (params[i], flops[i]), xytext=(5, 5),
                    textcoords='offset points', fontweight='bold')

    ax1.set_xlabel('å‚æ•°æ•°é‡ (M)')
    ax1.set_ylabel('FLOP (GFLOP)')
    ax1.set_title('æ¨¡å‹å¤æ‚åº¦åˆ†æ (æ°”æ³¡å¤§å°=å†…å­˜, é¢œè‰²=ç²¾åº¦)', fontweight='bold')
    plt.colorbar(scatter, ax=ax1, label='ç²¾åº¦ (%)')

    # 2. æ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾
    ax2 = fig.add_subplot(gs[0, 2:], projection='polar')

    # å½’ä¸€åŒ–æŒ‡æ ‡
    max_params, max_flops, max_memory, max_acc = max(params), max(flops), max(memory), max(accuracy)

    categories = ['å‚æ•°æ•ˆç‡', 'è®¡ç®—æ•ˆç‡', 'å†…å­˜æ•ˆç‡', 'ç²¾åº¦']

    for i, config in enumerate(model_configs):
        scores = [
            1 - (params[i] / max_params),      # å‚æ•°æ•ˆç‡ (è¶Šå°è¶Šå¥½)
            1 - (flops[i] / max_flops),        # è®¡ç®—æ•ˆç‡ (è¶Šå°è¶Šå¥½)
            1 - (memory[i] / max_memory),      # å†…å­˜æ•ˆç‡ (è¶Šå°è¶Šå¥½)
            accuracy[i] / max_acc              # ç²¾åº¦ (è¶Šå¤§è¶Šå¥½)
        ]

        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        scores += scores[:1]
        angles += angles[:1]

        ax2.plot(angles, scores, 'o-', linewidth=2, label=config['name'], markersize=6)
        ax2.fill(angles, scores, alpha=0.15)

    ax2.set_xticks(angles[:-1])
    ax2.set_xticklabels(categories)
    ax2.set_ylim(0, 1)
    ax2.set_title('ç»¼åˆæ€§èƒ½é›·è¾¾å›¾', fontweight='bold', pad=20)
    ax2.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))

    # 3. ç²¾åº¦ vs å‚æ•°å…³ç³»
    ax3 = fig.add_subplot(gs[1, 0])
    ax3.plot(params, accuracy, 'o-', linewidth=3, markersize=8, color='#FF6B6B')
    ax3.set_xlabel('å‚æ•°æ•°é‡ (M)')
    ax3.set_ylabel('ç²¾åº¦ (%)')
    ax3.set_title('ç²¾åº¦ vs å‚æ•°æ•°é‡', fontweight='bold')
    ax3.grid(True, alpha=0.3)

    # 4. ç²¾åº¦ vs FLOPå…³ç³»
    ax4 = fig.add_subplot(gs[1, 1])
    ax4.plot(flops, accuracy, 'o-', linewidth=3, markersize=8, color='#4ECDC4')
    ax4.set_xlabel('FLOP (GFLOP)')
    ax4.set_ylabel('ç²¾åº¦ (%)')
    ax4.set_title('ç²¾åº¦ vs è®¡ç®—é‡', fontweight='bold')
    ax4.grid(True, alpha=0.3)

    # 5. å†…å­˜ä½¿ç”¨å¯¹æ¯”
    ax5 = fig.add_subplot(gs[1, 2])
    bars = ax5.bar(names, memory, color=['#95E77E', '#FFD93D', '#FF6BCB', '#C9B6E5', '#FFB6B9'])
    ax5.set_title('å†…å­˜ä½¿ç”¨å¯¹æ¯”', fontweight='bold')
    ax5.set_ylabel('å†…å­˜ (GB)')
    ax5.tick_params(axis='x', rotation=45)

    for bar, value in zip(bars, memory):
        ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(memory)*0.01,
                f'{value}GB', ha='center', va='bottom', fontweight='bold')

    # 6. æ•ˆç‡æ’è¡Œæ¦œ
    ax6 = fig.add_subplot(gs[1, 3])

    # è®¡ç®—ç»¼åˆæ•ˆç‡åˆ†æ•°
    efficiency_scores = []
    for i in range(len(names)):
        score = (accuracy[i] / max_acc) / ((params[i] / max_params + flops[i] / max_flops + memory[i] / max_memory) / 3)
        efficiency_scores.append(score)

    sorted_indices = np.argsort(efficiency_scores)[::-1]
    sorted_names = [names[i] for i in sorted_indices]
    sorted_scores = [efficiency_scores[i] for i in sorted_indices]

    bars = ax6.barh(sorted_names, sorted_scores, color='#45B7D1', alpha=0.8)
    ax6.set_title('ç»¼åˆæ•ˆç‡æ’è¡Œæ¦œ', fontweight='bold')
    ax6.set_xlabel('æ•ˆç‡åˆ†æ•°')

    # 7. æ¨èçŸ©é˜µ (ç²¾åº¦ vs æ•ˆç‡)
    ax7 = fig.add_subplot(gs[2, :2])

    for i, name in enumerate(names):
        color = 'red' if efficiency_scores[i] > np.mean(efficiency_scores) else 'blue'
        size = 100 + accuracy[i] * 10
        ax7.scatter(efficiency_scores[i], accuracy[i], s=size, alpha=0.7,
                   color=color, edgecolors='black')
        ax7.annotate(name, (efficiency_scores[i], accuracy[i]), xytext=(5, 5),
                    textcoords='offset points', fontweight='bold')

    # æ·»åŠ æ¨èåŒºåŸŸ
    ax7.axhline(y=np.mean(accuracy), color='gray', linestyle='--', alpha=0.5, label='å¹³å‡ç²¾åº¦')
    ax7.axvline(x=np.mean(efficiency_scores), color='gray', linestyle='--', alpha=0.5, label='å¹³å‡æ•ˆç‡')

    ax7.set_xlabel('æ•ˆç‡åˆ†æ•°')
    ax7.set_ylabel('ç²¾åº¦ (%)')
    ax7.set_title('æ¨¡å‹é€‰æ‹©æ¨èçŸ©é˜µ', fontweight='bold')
    ax7.legend()
    ax7.grid(True, alpha=0.3)

    # 8. å…³é”®æ´å¯Ÿæ–‡æœ¬
    ax8 = fig.add_subplot(gs[2, 2:])
    ax8.axis('off')

    best_accuracy_idx = np.argmax(accuracy)
    best_efficiency_idx = np.argmax(efficiency_scores)
    lowest_memory_idx = np.argmin(memory)

    insights = f"""
    ğŸ¯ å…³é”®æ´å¯Ÿ:

    ğŸ† æœ€é«˜ç²¾åº¦: {names[best_accuracy_idx]}
       ç²¾åº¦: {accuracy[best_accuracy_idx]}%
       å‚æ•°: {params[best_accuracy_idx]}M

    âš¡ æœ€é«˜æ•ˆç‡: {names[best_efficiency_idx]}
       æ•ˆç‡åˆ†æ•°: {efficiency_scores[best_efficiency_idx]:.3f}
       å†…å­˜: {memory[best_efficiency_idx]}GB

    ğŸ’¾ æœ€ä½å†…å­˜: {names[lowest_memory_idx]}
       å†…å­˜: {memory[lowest_memory_idx]}GB
       ç²¾åº¦: {accuracy[lowest_memory_idx]}%

    ğŸ“Š æ¨èé€‰æ‹©:
    â€¢ è¿½æ±‚ç²¾åº¦: {names[best_accuracy_idx]}
    â€¢ å¹³è¡¡æ€§èƒ½: {names[best_efficiency_idx]}
    â€¢ ç§»åŠ¨éƒ¨ç½²: {names[lowest_memory_idx]}
    """

    ax8.text(0.1, 0.5, insights, fontsize=12, verticalalignment='center',
            fontfamily='monospace', bbox=dict(boxstyle="round,pad=0.5",
            facecolor="lightgray", alpha=0.8))

    plt.savefig('/Users/peixingxin/code/spring2025-lectures/æ·±åº¦è®¨è®º/ç»¼åˆæ€§èƒ½ä»ªè¡¨æ¿.png',
                dpi=300, bbox_inches='tight')
    plt.show()


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ å¼€å§‹PyTorchå¯è§†åŒ–å®è·µæ¼”ç¤º")
    print("=" * 80)

    try:
        # 1. FLOPåˆ†æå¯è§†åŒ–
        visualize_flop_analysis()

        # 2. å†…å­˜åˆ†æå¯è§†åŒ–
        visualize_memory_analysis()

        # 3. è®­ç»ƒæ€§èƒ½å¯è§†åŒ–
        visualize_training_performance()

        # 4. æ··åˆç²¾åº¦æ•ˆç›Šæ¼”ç¤º
        demonstrate_mixed_precision_benefits()

        # 5. ç»¼åˆæ€§èƒ½ä»ªè¡¨æ¿
        create_comprehensive_dashboard()

        print("\nâœ… æ‰€æœ‰å¯è§†åŒ–æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ“ ç”Ÿæˆçš„å›¾ç‰‡æ–‡ä»¶:")
        print("  ğŸ“Š FLOPåˆ†æå¯è§†åŒ–.png")
        print("  ğŸ’¾ å†…å­˜åˆ†æå¯è§†åŒ–.png")
        print("  ğŸ”„ è®­ç»ƒæ€§èƒ½å¯è§†åŒ–.png")
        print("  âš¡ æ··åˆç²¾åº¦æ•ˆç›Šå¯¹æ¯”.png")
        print("  ğŸ“ˆ ç»¼åˆæ€§èƒ½ä»ªè¡¨æ¿.png")

        print("\nğŸ’¡ å…³é”®å­¦ä¹ æ”¶è·:")
        print("  1. ğŸ” å­¦ä¼šäº†ç²¾ç¡®åˆ†æå’Œå¯è§†åŒ–æ¨¡å‹çš„è®¡ç®—å¤æ‚åº¦")
        print("  2. ğŸ’¾ ç†è§£äº†å†…å­˜ä½¿ç”¨çš„ç»„æˆå’Œä¼˜åŒ–ç­–ç•¥")
        print("  3.  ğŸš€ æŒæ¡äº†è®­ç»ƒæ€§èƒ½çš„ç›‘æ§å’Œå¯¹æ¯”æ–¹æ³•")
        print("  4.  âš¡ ä½“éªŒäº†æ··åˆç²¾åº¦è®­ç»ƒçš„å®é™…æ•ˆç›Š")
        print("  5.  ğŸ“Š å»ºç«‹äº†æ¨¡å‹æ€§èƒ½è¯„ä¼°çš„ç»¼åˆè§†è§’")

        print("\nğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®:")
        print("  â€¢ å°è¯•å°†è¿™äº›å·¥å…·åº”ç”¨åˆ°è‡ªå·±çš„æ¨¡å‹ä¸Š")
        print("  â€¢ å®éªŒä¸åŒçš„ä¼˜åŒ–ç­–ç•¥ç»„åˆ")
        print("  â€¢ æ·±å…¥å­¦ä¹ Transformeræ¶æ„çš„FLOPåˆ†æ")
        print("  â€¢ æ¢ç´¢åˆ†å¸ƒå¼è®­ç»ƒçš„æ€§èƒ½åˆ†æ")

    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥ä¾èµ–åŒ…æ˜¯å¦æ­£ç¡®å®‰è£…:")
        print("   pip install torch matplotlib seaborn numpy psutil")


if __name__ == "__main__":
    main()