#!/usr/bin/env python3
"""
Tokenizationå®è·µç»ƒä¹ 
åŸºäºLecture 01çš„å†…å®¹ï¼Œå®ç°å’Œæµ‹è¯•ä¸åŒçš„tokenizer
"""

import re
from abc import ABC, abstractmethod
from collections import defaultdict
from typing import List, Dict, Tuple


class Tokenizer(ABC):
    """TokenizeræŠ½è±¡åŸºç±»"""
    @abstractmethod
    def encode(self, string: str) -> List[int]:
        """å°†å­—ç¬¦ä¸²ç¼–ç ä¸ºtokenåºåˆ—"""
        pass

    @abstractmethod
    def decode(self, indices: List[int]) -> str:
        """å°†tokenåºåˆ—è§£ç ä¸ºå­—ç¬¦ä¸²"""
        pass


class CharacterTokenizer(Tokenizer):
    """å­—ç¬¦çº§tokenizer"""
    def encode(self, string: str) -> List[int]:
        return list(map(ord, string))

    def decode(self, indices: List[int]) -> str:
        return "".join(map(chr, indices))


class ByteTokenizer(Tokenizer):
    """å­—èŠ‚çº§tokenizer"""
    def encode(self, string: str) -> List[int]:
        string_bytes = string.encode("utf-8")
        return list(map(int, string_bytes))

    def decode(self, indices: List[int]) -> str:
        string_bytes = bytes(indices)
        return string_bytes.decode("utf-8")


class BPETokenizerParams:
    """BPE tokenizerå‚æ•°"""
    def __init__(self, vocab: Dict[int, bytes], merges: Dict[Tuple[int, int], int]):
        self.vocab = vocab          # index -> bytes
        self.merges = merges        # (index1, index2) -> new_index


def merge(indices: List[int], pair: Tuple[int, int], new_index: int) -> List[int]:
    """å°†indicesä¸­çš„pairæ›¿æ¢ä¸ºnew_index"""
    new_indices = []
    i = 0
    while i < len(indices):
        if (i + 1 < len(indices) and
            indices[i] == pair[0] and
            indices[i + 1] == pair[1]):
            new_indices.append(new_index)
            i += 2
        else:
            new_indices.append(indices[i])
            i += 1
    return new_indices


def train_bpe(string: str, num_merges: int) -> BPETokenizerParams:
    """è®­ç»ƒBPE tokenizer"""
    print(f"è®­ç»ƒBPE tokenizerï¼Œæ–‡æœ¬é•¿åº¦: {len(string)} å­—ç¬¦")

    # åˆå§‹åŒ–ï¼šå°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå­—èŠ‚åºåˆ—
    indices = list(map(int, string.encode("utf-8")))
    print(f"åˆå§‹å­—èŠ‚åºåˆ—é•¿åº¦: {len(indices)}")

    # åˆå§‹åŒ–è¯æ±‡è¡¨å’Œåˆå¹¶è§„åˆ™
    vocab = {x: bytes([x]) for x in range(256)}
    merges = {}

    for i in range(num_merges):
        # ç»Ÿè®¡ç›¸é‚»tokenå¯¹é¢‘ç‡
        counts = defaultdict(int)
        for index1, index2 in zip(indices, indices[1:]):
            counts[(index1, index2)] += 1

        if not counts:
            break

        # æ‰¾åˆ°æœ€é¢‘ç¹çš„tokenå¯¹
        pair = max(counts, key=counts.get)
        new_index = 256 + i

        print(f"ç¬¬{i+1}æ¬¡åˆå¹¶: {pair} -> {new_index} (é¢‘ç‡: {counts[pair]})")

        # è®°å½•åˆå¹¶è§„åˆ™
        merges[pair] = new_index
        vocab[new_index] = vocab[pair[0]] + vocab[pair[1]]

        # åº”ç”¨åˆå¹¶
        indices = merge(indices, pair, new_index)
        print(f"åˆå¹¶ååºåˆ—é•¿åº¦: {len(indices)}")

    print(f"è®­ç»ƒå®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {len(vocab)}")
    return BPETokenizerParams(vocab=vocab, merges=merges)


class BPETokenizer(Tokenizer):
    """BPE tokenizerå®ç°"""
    def __init__(self, params: BPETokenizerParams):
        self.params = params

    def encode(self, string: str) -> List[int]:
        # è½¬æ¢ä¸ºå­—èŠ‚åºåˆ—
        indices = list(map(int, string.encode("utf-8")))

        # åº”ç”¨æ‰€æœ‰åˆå¹¶è§„åˆ™ï¼ˆç®€å•ä½†ä½æ•ˆçš„å®ç°ï¼‰
        for pair, new_index in self.params.merges.items():
            indices = merge(indices, pair, new_index)

        return indices

    def decode(self, indices: List[int]) -> str:
        # å°†tokenç´¢å¼•è½¬æ¢ä¸ºå­—èŠ‚
        bytes_list = [self.params.vocab[idx] for idx in indices]
        # è§£ç ä¸ºå­—ç¬¦ä¸²
        return b"".join(bytes_list).decode("utf-8")


def get_compression_ratio(string: str, indices: List[int]) -> float:
    """è®¡ç®—å‹ç¼©æ¯”"""
    num_bytes = len(bytes(string, encoding="utf-8"))
    num_tokens = len(indices)
    return num_bytes / num_tokens


def test_tokenizer(tokenizer: Tokenizer, name: str, test_strings: List[str]):
    """æµ‹è¯•tokenizeræ€§èƒ½"""
    print(f"\n=== æµ‹è¯• {name} ===")

    for string in test_strings:
        try:
            # ç¼–ç 
            indices = tokenizer.encode(string)
            # è§£ç 
            reconstructed = tokenizer.decode(indices)
            # éªŒè¯
            success = string == reconstructed
            # å‹ç¼©æ¯”
            ratio = get_compression_ratio(string, indices)

            print(f"åŸæ–‡: {string[:30]}{'...' if len(string) > 30 else ''}")
            print(f"Tokens: {indices[:10]}{'...' if len(indices) > 10 else ''}")
            print(f"Tokenæ•°é‡: {len(indices)}, å‹ç¼©æ¯”: {ratio:.2f}")
            print(f"å¾€è¿”æµ‹è¯•: {'âœ… é€šè¿‡' if success else 'âŒ å¤±è´¥'}")
            print()

        except Exception as e:
            print(f"å¤„ç† '{string}' æ—¶å‡ºé”™: {e}")
            print()


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºä¸åŒtokenizerçš„æ•ˆæœ"""
    print("ğŸš€ Tokenizationå®è·µæ¼”ç¤º")
    print("=" * 50)

    # æµ‹è¯•å­—ç¬¦ä¸²
    test_strings = [
        "Hello, world!",
        "Hello, ğŸŒ! ä½ å¥½!",
        "the cat in the hat",
        "supercalifragilisticexpialidocious",
        "I'll say this is amazing!"
    ]

    # 1. æµ‹è¯•å­—ç¬¦çº§tokenizer
    char_tokenizer = CharacterTokenizer()
    test_tokenizer(char_tokenizer, "Character-based Tokenizer", test_strings)

    # 2. æµ‹è¯•å­—èŠ‚çº§tokenizer
    byte_tokenizer = ByteTokenizer()
    test_tokenizer(byte_tokenizer, "Byte-based Tokenizer", test_strings)

    # 3. è®­ç»ƒå’Œæµ‹è¯•BPE tokenizer
    print("\n" + "=" * 50)
    print("ğŸ”§ è®­ç»ƒBPE Tokenizer")
    print("=" * 50)

    # ä½¿ç”¨ç®€å•çš„è®­ç»ƒæ–‡æœ¬
    training_text = "the cat in the hat the cat sat on the mat the quick brown fox"
    bpe_params = train_bpe(training_text, num_merges=10)

    bpe_tokenizer = BPETokenizer(bpe_params)
    test_tokenizer(bpe_tokenizer, "BPE Tokenizer", test_strings)

    # 4. æ¼”ç¤ºBPEè®­ç»ƒè¿‡ç¨‹
    print("\n" + "=" * 50)
    print("ğŸ“Š BPEè®­ç»ƒè¿‡ç¨‹è¯¦ç»†æ¼”ç¤º")
    print("=" * 50)

    simple_text = "aaabdaaabac"
    print(f"è®­ç»ƒæ–‡æœ¬: {simple_text}")

    # æ‰‹åŠ¨æ¼”ç¤ºå‰å‡ æ­¥åˆå¹¶
    indices = list(map(int, simple_text.encode("utf-8")))
    print(f"åˆå§‹å­—èŠ‚: {indices}")
    print(f"å¯¹åº”å­—ç¬¦: {[chr(b) for b in indices]}")

    # ç»Ÿè®¡ç›¸é‚»å¯¹
    counts = defaultdict(int)
    for i in range(len(indices)-1):
        counts[(indices[i], indices[i+1])] += 1

    print(f"ç›¸é‚»å¯¹é¢‘ç‡: {dict(counts)}")
    if counts:
        most_common = max(counts, key=counts.get)
        print(f"æœ€é¢‘ç¹çš„å¯¹: {most_common} (å‡ºç° {counts[most_common]} æ¬¡)")


if __name__ == "__main__":
    main()