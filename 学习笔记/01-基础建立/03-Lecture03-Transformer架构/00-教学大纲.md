# Lecture 03: Transformer Architecture - å®Œæ•´æ•™å­¦å¤§çº²

## ğŸ“š è¯¾ç¨‹æ¦‚è¿°

**åŸºäº**: Stanford CS336 Spring 2025 Lecture 3 - Architecture
**å­¦ä¹ æ—¶é•¿**: 3-4å¤©æ·±åº¦å­¦ä¹ 
**æ ¸å¿ƒç›®æ ‡**: ä»ç¬¬ä¸€æ€§åŸç†ç†è§£Transformeræ¶æ„ï¼ŒæŒæ¡ç°ä»£LLMçš„æ ¸å¿ƒç»„ä»¶

---

## ğŸ¯ å­¦ä¹ ç›®æ ‡

### æŠ€æœ¯ç›®æ ‡
1. **æ·±å…¥ç†è§£Self-Attentionæœºåˆ¶**: æ•°å­¦åŸç†ã€è®¡ç®—æµç¨‹ã€å¤æ‚åº¦åˆ†æ
2. **æŒæ¡Transformeræ¶æ„**: å®Œæ•´çš„encoder-decoderç»“æ„å’Œdecoder-onlyå˜ä½“
3. **ç†è§£Position Encoding**: å„ç§ä½ç½®ç¼–ç æ–¹æ³•åŠå…¶æƒè¡¡
4. **ç†Ÿæ‚‰ç°ä»£å˜ä½“**: GPTã€BERTã€LLaMAç­‰æ¶æ„çš„è®¾è®¡é€‰æ‹©

### ç³»ç»Ÿæ€ç»´ç›®æ ‡
1. **èµ„æºæ„è¯†**: ç†è§£attentionçš„å†…å­˜å’Œè®¡ç®—ç“¶é¢ˆ
2. **ä¼˜åŒ–æ€ç»´**: FlashAttentionã€ç¨€ç–attentionç­‰ä¼˜åŒ–æ–¹æ³•
3. **å·¥ç¨‹æƒè¡¡**: ä¸åŒæ¶æ„é€‰æ‹©çš„trade-offs

---

## ğŸ“– è¯¾ç¨‹å†…å®¹ç»“æ„

### Part 1: ä»RNNåˆ°Transformerçš„æ¼”è¿› (30åˆ†é’Ÿ)

#### 1.1 åºåˆ—å»ºæ¨¡çš„å†å²
- **RNNçš„å±€é™æ€§**:
  - Sequential dependencyæ— æ³•å¹¶è¡ŒåŒ–
  - æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸é—®é¢˜
  - é•¿è·ç¦»ä¾èµ–å»ºæ¨¡å›°éš¾

- **LSTM/GRUçš„æ”¹è¿›**:
  - é—¨æ§æœºåˆ¶ç¼“è§£æ¢¯åº¦é—®é¢˜
  - ä½†ä»ç„¶æ˜¯sequentialçš„

- **ä¸ºä»€ä¹ˆéœ€è¦æ–°æ¶æ„ï¼Ÿ**
  - å¹¶è¡ŒåŒ–è®­ç»ƒçš„éœ€æ±‚
  - æ›´å¥½çš„é•¿è·ç¦»ä¾èµ–å»ºæ¨¡
  - è®¡ç®—æ•ˆç‡çš„è€ƒè™‘

#### å…³é”®é—®é¢˜
- Q: RNNä¸ºä»€ä¹ˆæ— æ³•å¹¶è¡ŒåŒ–è®­ç»ƒï¼Ÿ
- Q: LSTMçš„"è®°å¿†å•å…ƒ"èƒ½è®°ä½å¤šè¿œçš„ä¿¡æ¯ï¼Ÿ
- Q: å¦‚æœè¦è®¾è®¡æ–°æ¶æ„ï¼Œæ ¸å¿ƒéœ€æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ

---

### Part 2: Attentionæœºåˆ¶çš„è¯ç”Ÿ (45åˆ†é’Ÿ)

#### 2.1 Attentionçš„èµ·æº
- **Seq2Seq with Attention (Bahdanau 2014)**:
  - Encoder-decoderæ¶æ„
  - è§£å†³"fixed-size bottleneck"é—®é¢˜
  - Decoderåœ¨ç”Ÿæˆæ¯ä¸ªè¯æ—¶"å…³æ³¨"encoderçš„ä¸åŒä½ç½®

#### 2.2 ä»Attentionåˆ°Self-Attention
- **æ ¸å¿ƒè½¬å˜**: åºåˆ—å†…éƒ¨çš„å…³æ³¨ (intra-sequence)
- **Selfçš„å«ä¹‰**: Queryã€Keyã€Valueéƒ½æ¥è‡ªåŒä¸€ä¸ªåºåˆ—

#### 2.3 Attentionæ•°å­¦æœºåˆ¶
```python
# å®Œæ•´çš„attentionè®¡ç®—æµç¨‹
def attention(Q, K, V):
    """
    Q: QueryçŸ©é˜µ [seq_len, d_k]
    K: KeyçŸ©é˜µ [seq_len, d_k]
    V: ValueçŸ©é˜µ [seq_len, d_v]
    """
    # 1. è®¡ç®—attention scores
    scores = Q @ K.T  # [seq_len, seq_len]

    # 2. Scaled dot-product (ä¸ºä»€ä¹ˆè¦scale?)
    scores = scores / sqrt(d_k)

    # 3. Softmaxå½’ä¸€åŒ–
    attention_weights = softmax(scores, dim=-1)

    # 4. åŠ æƒæ±‚å’ŒValues
    output = attention_weights @ V  # [seq_len, d_v]

    return output, attention_weights
```

#### å…³é”®æ´å¯Ÿ
- **ä¸ºä»€ä¹ˆæ˜¯ç‚¹ç§¯ï¼Ÿ**: è¡¡é‡Queryå’ŒKeyçš„ç›¸ä¼¼åº¦
- **Softmaxçš„ä½œç”¨**: å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œ"è½¯"é€‰æ‹©
- **å¯å¾®åˆ†æ€§**: ç«¯åˆ°ç«¯è®­ç»ƒçš„å…³é”®

#### æ·±åº¦é—®é¢˜
- Q: ä¸ºä»€ä¹ˆç”¨ç‚¹ç§¯è€Œä¸æ˜¯å…¶ä»–ç›¸ä¼¼åº¦åº¦é‡ï¼ˆå¦‚ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰ï¼Ÿ
- Q: Attentionæƒé‡çŸ©é˜µçš„æ¯ä¸€è¡Œã€æ¯ä¸€åˆ—ä»£è¡¨ä»€ä¹ˆï¼Ÿ
- Q: Self-attentionå¦‚ä½•æ•è·é•¿è·ç¦»ä¾èµ–ï¼Ÿ

---

### Part 3: Multi-Head Attentionè¯¦è§£ (60åˆ†é’Ÿ)

#### 3.1 ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´ï¼Ÿ
- **å•å¤´çš„å±€é™**: åªèƒ½å­¦ä¹ ä¸€ç§å…³æ³¨æ¨¡å¼
- **å¤šå¤´çš„ä¼˜åŠ¿**:
  - æ•è·ä¸åŒç±»å‹çš„å…³ç³»ï¼ˆè¯­æ³•ã€è¯­ä¹‰ã€ä½ç½®ç­‰ï¼‰
  - å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›
  - ç±»æ¯”ï¼šCNNä¸­çš„å¤šä¸ªfilters

#### 3.2 Multi-Head Attentionæ¶æ„
```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # æŠ•å½±çŸ©é˜µ
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)

    def forward(self, x):
        batch_size, seq_len, d_model = x.shape

        # 1. çº¿æ€§æŠ•å½±
        Q = self.W_Q(x)  # [batch, seq, d_model]
        K = self.W_K(x)
        V = self.W_V(x)

        # 2. åˆ†å‰²æˆå¤šä¸ªå¤´
        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k)
        Q = Q.transpose(1, 2)  # [batch, heads, seq, d_k]
        # ç±»ä¼¼åœ°å¤„ç†K, V

        # 3. å¹¶è¡Œè®¡ç®—attention
        attention_output = scaled_dot_product_attention(Q, K, V)

        # 4. æ‹¼æ¥heads
        attention_output = attention_output.transpose(1, 2).contiguous()
        attention_output = attention_output.view(batch_size, seq_len, d_model)

        # 5. æœ€ç»ˆæŠ•å½±
        output = self.W_O(attention_output)

        return output
```

#### 3.3 å¤šå¤´çš„å‚æ•°é‡åˆ†æ
**é—®é¢˜**: hä¸ªå¤´çš„å‚æ•°é‡æ˜¯å•å¤´çš„å‡ å€ï¼Ÿ

**åˆ†æ**:
- æŠ•å½±çŸ©é˜µ: W_Q, W_K, W_V, W_O
- æ¯ä¸ªéƒ½æ˜¯ [d_model, d_model]
- æ€»å‚æ•°é‡: 4 * d_modelÂ²
- **å…³é”®æ´å¯Ÿ**: ä¸å¤´æ•°æ— å…³ï¼å› ä¸ºæ¯ä¸ªå¤´çš„ç»´åº¦ç¼©å°äº†

#### æ·±åº¦é—®é¢˜
- Q: å¦‚ä½•å¯è§†åŒ–ä¸åŒheadså­¦åˆ°çš„æ¨¡å¼ï¼Ÿ
- Q: æ˜¯å¦æ‰€æœ‰headséƒ½åŒç­‰é‡è¦ï¼Ÿï¼ˆæç¤ºï¼šHead Pruningï¼‰
- Q: Multi-Headèƒ½å¦å¹¶è¡Œè®¡ç®—ï¼Ÿå¦‚ä½•ä¼˜åŒ–ï¼Ÿ

---

### Part 4: Transformerå®Œæ•´æ¶æ„ (90åˆ†é’Ÿ)

#### 4.1 Transformer Blockç»“æ„
```
Input
  â†“
[Add & Norm] â† Multi-Head Attention
  â†“
[Add & Norm] â† Feed-Forward Network
  â†“
Output
```

#### 4.2 å…³é”®ç»„ä»¶è¯¦è§£

**4.2.1 æ®‹å·®è¿æ¥ (Residual Connection)**
```python
# ä¸ºä»€ä¹ˆè¿™ä¹ˆé‡è¦ï¼Ÿ
x = x + MultiHeadAttention(x)
x = x + FeedForward(x)
```
- **æ¢¯åº¦æµ**: ç›´æ¥é€šè·¯ç¼“è§£æ¢¯åº¦æ¶ˆå¤±
- **ä¿¡æ¯æµ**: ä¿ç•™åŸå§‹ä¿¡æ¯
- **æ·±å±‚ç½‘ç»œçš„å…³é”®**: å…è®¸å †å æ•°ç™¾å±‚

**4.2.2 Layer Normalization**
```python
# Pre-LN vs Post-LN
# Post-LN (åŸå§‹è®ºæ–‡):
x = LayerNorm(x + Attention(x))

# Pre-LN (ç°ä»£å®è·µ):
x = x + Attention(LayerNorm(x))
```
- **ä¸ºä»€ä¹ˆç°ä»£æ¨¡å‹ç”¨Pre-LNï¼Ÿ**: è®­ç»ƒæ›´ç¨³å®š
- **Normalizationçš„ä½œç”¨**: ç¨³å®šæ¿€æ´»å€¼åˆ†å¸ƒ

**4.2.3 Feed-Forward Network**
```python
class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)  # é€šå¸¸d_ff = 4*d_model
        self.linear2 = nn.Linear(d_ff, d_model)
        self.activation = nn.GELU()  # æˆ–ReLU

    def forward(self, x):
        return self.linear2(self.activation(self.linear1(x)))
```
- **ä¸ºä»€ä¹ˆéœ€è¦FFNï¼Ÿ**: Attentionæ··åˆä¿¡æ¯ï¼ŒFFNåšéçº¿æ€§å˜æ¢
- **4xæ‰©å±•çš„æ„ä¹‰**: å¢åŠ æ¨¡å‹å®¹é‡
- **Position-wise**: æ¯ä¸ªä½ç½®ç‹¬ç«‹å¤„ç†

#### 4.3 Position Encoding

**é—®é¢˜**: Attentionå¯¹é¡ºåºä¸æ•æ„Ÿï¼Œå¦‚ä½•æ³¨å…¥ä½ç½®ä¿¡æ¯ï¼Ÿ

**4.3.1 Sinusoidal Position Encoding (åŸå§‹è®ºæ–‡)**
```python
def sinusoidal_position_encoding(seq_len, d_model):
    pe = torch.zeros(seq_len, d_model)
    position = torch.arange(0, seq_len).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, d_model, 2) *
                        -(math.log(10000.0) / d_model))

    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)

    return pe
```
- **ä¼˜ç‚¹**: å¯ä»¥æ³›åŒ–åˆ°æ›´é•¿åºåˆ—
- **ç¼ºç‚¹**: å›ºå®šçš„ï¼Œä¸å¯å­¦ä¹ 

**4.3.2 Learned Position Embeddings**
```python
self.position_embeddings = nn.Embedding(max_seq_len, d_model)
```
- **ä¼˜ç‚¹**: å¯å­¦ä¹ ï¼Œå¯èƒ½æ›´å¥½
- **ç¼ºç‚¹**: æ— æ³•æ³›åŒ–åˆ°è®­ç»ƒæ—¶æœªè§è¿‡çš„é•¿åº¦

**4.3.3 ç°ä»£æ–¹æ³•**
- **Rotary Position Embedding (RoPE)**: LLaMAä½¿ç”¨
- **ALiBi**: åœ¨attention scoresä¸ŠåŠ bias
- **ç›¸å¯¹ä½ç½®ç¼–ç **: T5, DeBERTa

#### 4.4 Decoderä¸­çš„Masked Attention
```python
def create_causal_mask(seq_len):
    # ä¸Šä¸‰è§’çŸ©é˜µï¼Œé˜²æ­¢"çœ‹åˆ°æœªæ¥"
    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)
    mask = mask.masked_fill(mask == 1, float('-inf'))
    return mask

# åœ¨attentionè®¡ç®—ä¸­åº”ç”¨
scores = scores + mask  # è¢«maskçš„ä½ç½®å˜ä¸º-inf
attention_weights = softmax(scores)  # -infå˜ä¸º0
```

#### æ·±åº¦é—®é¢˜
- Q: Pre-LN vs Post-LNå¯¹è®­ç»ƒåŠ¨æ€æœ‰ä½•å½±å“ï¼Ÿ
- Q: ä¸ºä»€ä¹ˆFFNçš„ä¸­é—´ç»´åº¦æ˜¯4å€ï¼Ÿèƒ½å¦è°ƒæ•´ï¼Ÿ
- Q: Causal maskå¦‚ä½•å½±å“è®­ç»ƒå’Œæ¨ç†ï¼Ÿ

---

### Part 5: ç°ä»£Transformerå˜ä½“ (60åˆ†é’Ÿ)

#### 5.1 Decoder-Onlyæ¶æ„ (GPTç³»åˆ—)
```
ç‰¹ç‚¹:
- åªæœ‰decoder stack
- Causal (auto-regressive) attention
- é¢„è®­ç»ƒ: ä¸‹ä¸€ä¸ªtokené¢„æµ‹
```

**ä¸ºä»€ä¹ˆDecoder-Onlyå¦‚æ­¤æˆåŠŸï¼Ÿ**
- ç®€åŒ–æ¶æ„
- æ‰©å±•æ€§å¥½
- é€šç”¨æ€§å¼ºï¼ˆç”Ÿæˆä»»åŠ¡ä¸ºä¸»ï¼‰

#### 5.2 Encoder-Onlyæ¶æ„ (BERTç³»åˆ—)
```
ç‰¹ç‚¹:
- åªæœ‰encoder stack
- Bidirectional attention
- é¢„è®­ç»ƒ: Masked Language Modeling
```

**é€‚ç”¨åœºæ™¯**:
- ç†è§£ä»»åŠ¡ï¼ˆåˆ†ç±»ã€NERç­‰ï¼‰
- ä¸é€‚åˆç”Ÿæˆ

#### 5.3 æ¶æ„å¯¹æ¯”

| ç‰¹æ€§ | GPT (Decoder-Only) | BERT (Encoder-Only) | T5 (Encoder-Decoder) |
|------|-------------------|---------------------|----------------------|
| Attention | Causal | Bidirectional | Both |
| é¢„è®­ç»ƒä»»åŠ¡ | Next Token | MLM | Span Corruption |
| ä¸»è¦ç”¨é€” | ç”Ÿæˆ | ç†è§£ | é€šç”¨ |
| å‚æ•°æ•ˆç‡ | é«˜ | ä¸­ | ä½ |

#### 5.4 LLaMAæ¶æ„åˆ›æ–°
1. **Pre-normalization** (Pre-LN)
2. **RMSNorm** æ›¿ä»£ LayerNorm
3. **SwiGLU** æ›¿ä»£æ ‡å‡†FFNæ¿€æ´»
4. **Rotary Positional Embeddings (RoPE)**
5. **Grouped-Query Attention (GQA)** - æ¨ç†ä¼˜åŒ–

#### æ·±åº¦é—®é¢˜
- Q: ä¸ºä»€ä¹ˆGPT-3åªç”¨decoderè€Œä¸æ˜¯full transformerï¼Ÿ
- Q: BERTçš„åŒå‘æ€§ä¸ºä½•ä¸é€‚åˆç”Ÿæˆä»»åŠ¡ï¼Ÿ
- Q: æœªæ¥LLMæ¶æ„ä¼šå¦‚ä½•æ¼”è¿›ï¼Ÿ

---

### Part 6: æ•ˆç‡ä¸ä¼˜åŒ– (45åˆ†é’Ÿ)

#### 6.1 Attentionçš„ç“¶é¢ˆåˆ†æ

**è®¡ç®—å¤æ‚åº¦**:
- AttentionçŸ©é˜µ: O(nÂ²Â·d)
- åºåˆ—é•¿åº¦næ˜¯ä¸»è¦ç“¶é¢ˆ

**å†…å­˜å¤æ‚åº¦**:
- å­˜å‚¨attentionçŸ©é˜µ: O(nÂ²)
- åå‘ä¼ æ’­æ—¶éœ€è¦ä¿å­˜ä¸­é—´ç»“æœ

#### 6.2 FlashAttention
```python
æ ¸å¿ƒæ€æƒ³:
1. Tiling: åˆ†å—è®¡ç®—ï¼Œå‡å°‘HBMè®¿é—®
2. Recomputation: åå‘æ—¶é‡ç®—ï¼ŒèŠ‚çœå†…å­˜
3. Kernel Fusion: ç®—å­èåˆä¼˜åŒ–

æ•ˆæœ:
- 2-4xåŠ é€Ÿ
- å†…å­˜å‡å°‘åˆ°O(n)
```

#### 6.3 å…¶ä»–ä¼˜åŒ–æ–¹æ³•

**ç¨€ç–Attention**:
- Local attention: åªå…³æ³¨é™„è¿‘tokens
- Strided attention: è·³è·ƒå¼å…³æ³¨
- Random attention: éšæœºé‡‡æ ·

**çº¿æ€§Attention**:
- Linformer: ä½ç§©è¿‘ä¼¼
- Performer: æ ¸æ–¹æ³•è¿‘ä¼¼
- Trade-off: ç‰ºç‰²ç²¾åº¦æ¢é€Ÿåº¦

**KVç¼“å­˜ (æ¨ç†ä¼˜åŒ–)**:
```python
# è‡ªå›å½’ç”Ÿæˆæ—¶ï¼Œé¿å…é‡å¤è®¡ç®—
class KVCache:
    def __init__(self):
        self.cache_k = []
        self.cache_v = []

    def update(self, new_k, new_v):
        self.cache_k.append(new_k)
        self.cache_v.append(new_v)
        return torch.cat(self.cache_k), torch.cat(self.cache_v)
```

#### æ·±åº¦é—®é¢˜
- Q: FlashAttentionå¦‚ä½•æ”¹å˜å†…å­˜è®¿é—®æ¨¡å¼ï¼Ÿ
- Q: ç¨€ç–attentionçš„è´¨é‡æŸå¤±æœ‰å¤šå¤§ï¼Ÿ
- Q: KVç¼“å­˜çš„å†…å­˜å ç”¨å¦‚ä½•è®¡ç®—ï¼Ÿï¼ˆå›é¡¾Lecture 02ï¼ï¼‰

---

## ğŸ¯ å®è·µé¡¹ç›®

### é¡¹ç›®1: ä»é›¶å®ç°Transformer (å¿…åš)
**ç›®æ ‡**: å®ç°å®Œæ•´çš„Transformer encoder
**è¦æ±‚**:
- [ ] å®ç°Scaled Dot-Product Attention
- [ ] å®ç°Multi-Head Attention
- [ ] å®ç°Position Encoding (sinusoidalå’Œlearned)
- [ ] å®ç°å®Œæ•´çš„Transformer Block
- [ ] åœ¨ç®€å•ä»»åŠ¡ä¸Šè®­ç»ƒéªŒè¯

### é¡¹ç›®2: Attentionå¯è§†åŒ– (æ¨è)
**ç›®æ ‡**: å¯è§†åŒ–ä¸åŒlayerså’Œheadsçš„attentionæ¨¡å¼
**è¦æ±‚**:
- [ ] æå–attentionæƒé‡
- [ ] ç»˜åˆ¶attention heatmap
- [ ] åˆ†æä¸åŒheadsçš„å…³æ³¨æ¨¡å¼
- [ ] è§£é‡Šè§‚å¯Ÿåˆ°çš„ç°è±¡

### é¡¹ç›®3: æ•ˆç‡ä¼˜åŒ–å®éªŒ (è¿›é˜¶)
**ç›®æ ‡**: å¯¹æ¯”ä¸åŒä¼˜åŒ–æ–¹æ³•çš„æ•ˆæœ
**è¦æ±‚**:
- [ ] æµ‹é‡æ ‡å‡†attentionçš„æ—¶é—´å’Œå†…å­˜
- [ ] å®ç°å¹¶æµ‹é‡FlashAttention (æˆ–ä½¿ç”¨åº“)
- [ ] å®ç°KVç¼“å­˜ä¼˜åŒ–æ¨ç†
- [ ] æ€§èƒ½å¯¹æ¯”åˆ†æ

---

## ğŸ“Š å­¦ä¹ æ£€æŸ¥æ¸…å•

### æ¦‚å¿µç†è§£
- [ ] ç†è§£Self-Attentionçš„å®Œæ•´è®¡ç®—æµç¨‹
- [ ] èƒ½è§£é‡ŠQã€Kã€Vçš„å«ä¹‰å’Œä½œç”¨
- [ ] ç†è§£scaled dot-productçš„scalingåŸå› 
- [ ] æŒæ¡Multi-Head Attentionçš„æ¶æ„å’Œä¼˜åŠ¿
- [ ] ç†è§£Position Encodingçš„å¿…è¦æ€§å’Œå„ç§æ–¹æ³•
- [ ] æŒæ¡Residual Connectionå’ŒLayerNormçš„ä½œç”¨
- [ ] ç†è§£Pre-LN vs Post-LNçš„åŒºåˆ«
- [ ] æŒæ¡Causal Maskingçš„å®ç°å’Œä½œç”¨

### å¤æ‚åº¦åˆ†æ
- [ ] èƒ½åˆ†æattentionçš„æ—¶é—´å¤æ‚åº¦O(nÂ²Â·d)
- [ ] èƒ½åˆ†æattentionçš„ç©ºé—´å¤æ‚åº¦O(nÂ²)
- [ ] ç†è§£ä¸ºä»€ä¹ˆnÂ²æ˜¯ç“¶é¢ˆ
- [ ] äº†è§£é™ä½å¤æ‚åº¦çš„å„ç§æ–¹æ³•

### æ¶æ„å¯¹æ¯”
- [ ] ç†è§£Decoder-Only vs Encoder-Only vs Encoder-Decoder
- [ ] äº†è§£GPTã€BERTã€T5ç­‰æ¶æ„çš„è®¾è®¡é€‰æ‹©
- [ ] ç†è§£ç°ä»£LLM (LLaMAç­‰) çš„æ¶æ„åˆ›æ–°

### ä¼˜åŒ–æ–¹æ³•
- [ ] ç†è§£FlashAttentionçš„æ ¸å¿ƒæ€æƒ³
- [ ] äº†è§£ç¨€ç–attentionçš„å„ç§å˜ä½“
- [ ] æŒæ¡KVç¼“å­˜åœ¨æ¨ç†ä¸­çš„ä½œç”¨

---

## ğŸ“š è¡¥å……é˜…è¯»

### å¿…è¯»è®ºæ–‡
1. **Attention Is All You Need** (2017) - åŸå§‹Transformerè®ºæ–‡
2. **BERT**: Pre-training of Deep Bidirectional Transformers (2018)
3. **GPT-3**: Language Models are Few-Shot Learners (2020)
4. **FlashAttention**: Fast and Memory-Efficient Exact Attention (2022)
5. **LLaMA**: Open and Efficient Foundation Language Models (2023)

### æ¨èåšå®¢
- The Illustrated Transformer (Jay Alammar)
- The Annotated Transformer (Harvard NLP)
- Attention? Attention! (Lilian Weng)

---

## ğŸ“ å­¦ä¹ æ—¶é—´å®‰æ’

**Day 1** (4-5å°æ—¶):
- Part 1-2: RNNåˆ°Attentionçš„æ¼”è¿›
- å®ŒæˆQ1-Q8æ·±åº¦é—®ç­”
- å®ç°åŸºç¡€attention

**Day 2** (4-5å°æ—¶):
- Part 3-4: Multi-Head Attentionå’Œå®Œæ•´æ¶æ„
- å®ŒæˆQ9-Q16æ·±åº¦é—®ç­”
- å®ç°Multi-Head Attentionå’ŒTransformer Block

**Day 3** (4-5å°æ—¶):
- Part 5-6: ç°ä»£å˜ä½“å’Œä¼˜åŒ–
- å®ŒæˆQ17-Q24æ·±åº¦é—®ç­”
- å¯è§†åŒ–é¡¹ç›®æˆ–ä¼˜åŒ–å®éªŒ

**Day 4** (å¯é€‰ï¼Œ2-3å°æ—¶):
- æ·±åº¦è®¨è®ºè®°å½•
- é¡¹ç›®å®Œå–„å’Œæ€»ç»“

---

**åˆ›å»ºæ—¥æœŸ**: 2025-11-10
**åŸºäº**: Stanford CS336 Spring 2025 Lecture 3
**çŠ¶æ€**: å®Œæ•´æ•™å­¦å¤§çº²
