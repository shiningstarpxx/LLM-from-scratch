# Lecture 03: Transformer Architecture 苏格拉底式深度问答

## 🎯 学习目标

通过24个精心设计的引导性问题，深入理解Transformer架构的核心机制、设计哲学和工程实现。

## 📚 问题分类

### 第一部分：Self-Attention机制 (Q1-Q6)
### 第二部分：Multi-Head Attention (Q7-Q12)
### 第三部分：Position Encoding (Q13-Q16)
### 第四部分：架构设计 (Q17-Q20)
### 第五部分：效率与优化 (Q21-Q24)

---

## 🧠 第一部分：Self-Attention机制 (Q1-Q6)

### Q1: 为什么叫"Self-Attention"？它在"关注"什么？

**引导思考**:
- Attention机制最初用于什么场景（提示：seq2seq）？
- "Self"体现在哪里？与传统attention有什么本质区别？
- 一个token如何"关注"其他tokens？

**你的思考**:
```
在这里写下你的理解...
```

---

### Q2: Query、Key、Value的命名来自哪里？为什么不叫A、B、C？

**引导思考**:
- 这些命名与数据库查询有什么关系？
- 在信息检索中，Query和Key分别代表什么？
- Value为什么要和Key分开？

**你的思考**:
```
在这里写下你的理解...
```

---

### Q3: Attention Score的计算公式 `score = Q @ K.T / sqrt(d_k)` 中，为什么要除以 sqrt(d_k)？

**引导思考**:
- 如果不除会发生什么？
- 这与softmax的梯度有什么关系？
- d_k（维度）如何影响点积的方差？

**你的思考**:
```
在这里写下你的理解...
```

---

### Q4: Self-Attention的计算复杂度是多少？为什么说它是瓶颈？

**引导思考**:
- 对于序列长度n，计算attention矩阵需要多少操作？
- 当n=1000 vs n=10000时，复杂度增长多少倍？
- 这个复杂度能否降低？有哪些方法？

**你的思考**:
```
在这里写下你的理解...
```

---

### Q5: Attention的输出是什么？它如何改变了输入的表示？

**引导思考**:
- 输出是Values的什么运算？
- Attention权重矩阵的每一行代表什么？
- 输出如何"融合"了上下文信息？

**你的思考**:
```
在这里写下你的理解...
```

---

### Q6: 为什么Attention被称为"可微分的字典查询"？

**引导思考**:
- 传统字典查询（如HashMap）是如何工作的？
- Attention如何实现"软"查询（soft lookup）？
- 可微分性为什么重要？

**你的思考**:
```
在这里写下你的理解...
```

---

## 🧠 第二部分：Multi-Head Attention (Q7-Q12)

### Q7: 为什么需要Multi-Head Attention？单个head不够吗？

**引导思考**:
- 不同的heads可能学到什么不同的模式？
- 这与CNN中多个filters的思想有何相似？
- 多个heads如何提高模型的表达能力？

**你的思考**:
```
在这里写下你的理解...
```

---

### Q8: 多头注意力的参数量是单头的几倍？

**引导思考**:
- 假设h个heads，每个head的维度是d_k = d_model/h
- Q、K、V的投影矩阵参数量如何计算？
- 输出投影矩阵W_O的参数量是多少？

**你的思考**:
```
在这里写下你的理解...
```

---

### Q9: 不同的heads会学到什么不同的关注模式？

**引导思考**:
- 有的head可能关注什么语言学信息（语法、语义）？
- 距离信息（nearby vs distant tokens）如何影响不同heads？
- 如何可视化和解释不同heads的行为？

**你的思考**:
```
在这里写下你的理解...
```

---

### Q10: Multi-Head Attention的并行度如何？能否充分利用GPU？

**引导思考**:
- 不同heads之间有依赖关系吗？
- 如何高效实现多头attention（hint：reshape操作）？
- 这个操作是compute-bound还是memory-bound？

**你的思考**:
```
在这里写下你的理解...
```

---

### Q11: 为什么每个head的维度是 d_model/h 而不是 d_model？

**引导思考**:
- 如果每个head都是d_model维，总参数量是多少？
- 缩小每个head的维度有什么好处？
- 这种设计的trade-off是什么？

**你的思考**:
```
在这里写下你的理解...
```

---

### Q12: 如果heads=1且d_k=d_model，Multi-Head Attention退化成什么？

**引导思考**:
- 这是否等同于单头Self-Attention？
- 输出投影W_O还起作用吗？
- 这个极端情况说明了什么？

**你的思考**:
```
在这里写下你的理解...
```

---

## 🧠 第三部分：Position Encoding (Q13-Q16)

### Q13: 为什么Transformer需要Position Encoding？

**引导思考**:
- Self-Attention本身对位置敏感吗？
- 如果打乱输入顺序，attention输出会变吗？
- RNN/CNN如何隐式编码位置信息？

**你的思考**:
```
在这里写下你的理解...
```

---

### Q14: 为什么使用正弦/余弦函数而不是简单的learnable embeddings？

**引导思考**:
- Sinusoidal position encoding的公式是什么？
- 它如何处理训练时未见过的序列长度？
- Learnable embeddings有什么局限？

**你的思考**:
```
在这里写下你的理解...
```

---

### Q15: Position Encoding是加到input embeddings上，还是有其他方式？为什么选择"加法"？

**引导思考**:
- 加法 vs 拼接（concatenation）的区别？
- 加法如何保持维度不变？
- 这种设计对模型容量有何影响？

**你的思考**:
```
在这里写下你的理解...
```

---

### Q16: 现代LLM（如GPT-3）还使用正弦position encoding吗？有什么新方法？

**引导思考**:
- Learned position embeddings的优缺点？
- Rotary Position Embedding (RoPE)的核心思想？
- ALiBi（Attention with Linear Biases）如何工作？

**你的思考**:
```
在这里写下你的理解...
```

---

## 🧠 第四部分：架构设计 (Q17-Q20)

### Q17: Transformer Block中，为什么要使用残差连接（Residual Connection）？

**引导思考**:
- 深层网络训练的梯度消失问题如何解决？
- 残差连接如何帮助梯度反向传播？
- 从信息流的角度，残差连接提供了什么？

**你的思考**:
```
在这里写下你的理解...
```

---

### Q18: Layer Normalization放在残差连接的什么位置？Pre-LN vs Post-LN有何区别？

**引导思考**:
- 原始Transformer论文使用Post-LN，为什么现代模型多用Pre-LN？
- 两种方式对训练稳定性有何影响？
- Pre-LN如何缓解深层网络的训练困难？

**你的思考**:
```
在这里写下你的理解...
```

---

### Q19: Feed-Forward Network（FFN）的作用是什么？为什么需要它？

**引导思考**:
- Attention层已经能混合信息了，为什么还需要FFN？
- FFN的"位置独立性"（position-wise）意味着什么？
- 中间维度扩大（通常4x）的作用是什么？

**你的思考**:
```
在这里写下你的理解...
```

---

### Q20: Decoder中的Masked Self-Attention如何实现？为什么需要mask？

**引导思考**:
- "Causal masking"是什么意思？
- 如何通过mask矩阵防止"看到未来"？
- 训练和推理时的masking有何不同？

**你的思考**:
```
在这里写下你的理解...
```

---

## 🧠 第五部分：效率与优化 (Q21-Q24)

### Q21: Transformer的内存瓶颈在哪里？如何优化？

**引导思考**:
- Attention矩阵（n×n）的内存占用如何？
- FlashAttention的核心思想是什么？
- KV缓存在推理时起什么作用？

**你的思考**:
```
在这里写下你的理解...
```

---

### Q22: 如何降低Self-Attention的O(n²)复杂度？有哪些方法？

**引导思考**:
- Sparse Attention的思想？
- Linformer/Performer如何近似attention？
- 这些方法的trade-off是什么？

**你的思考**:
```
在这里写下你的理解...
```

---

### Q23: 为什么Transformer能够并行化训练，而RNN不行？

**引导思考**:
- RNN的sequential dependency在哪里？
- Transformer如何在训练时并行处理整个序列？
- 这对训练速度有多大影响？

**你的思考**:
```
在这里写下你的理解...
```

---

### Q24: 如果算力无限，Transformer架构还需要改进吗？未来方向是什么？

**引导思考**:
- 当前Transformer的根本性限制是什么？
- 混合架构（Transformer + CNN/RNN）的可能性？
- State Space Models（如Mamba）能否替代Transformer？

**你的思考**:
```
在这里写下你的理解...
```

---

---

## 🔥 进阶挑战问题 (Optional)

### 挑战1: 从FLOP角度分析Transformer
基于Lecture 02的FLOP计算知识：
- 计算一个Transformer Block的总FLOP数
- Attention vs FFN的FLOP占比？
- 如何从FLOP预估训练时间？

### 挑战2: 内存精确计算
应用Lecture 02的内存分析：
- 前向传播需要存储哪些中间结果？
- 反向传播的内存峰值在哪里？
- Activation checkpointing如何节省内存？

### 挑战3: 系统性能分析
结合Lecture 02的系统思维：
- Attention是compute-bound还是memory-bound？
- 如何通过算子融合优化Transformer？
- GPU利用率在训练时的变化模式？

### 挑战4: 架构演进的商业逻辑
回顾Lecture 02的经济学视角：
- 为什么GPT选择decoder-only而不是full transformer？
- 从成本角度，简化架构的ROI是多少？
- 未来架构创新的商业驱动力是什么？

---

## 📚 与前序课程的连接

### 连接Lecture 01: Tokenization
- **问题**: Transformer的input embedding来自哪里？
- **思考**: Token sequence → Embedding sequence的过程
- **延伸**: Subword tokenization如何影响序列长度（因此影响attention复杂度）？

### 连接Lecture 02: Resource Accounting
- **问题**: 如何计算Transformer训练的内存需求？
- **思考**:
  ```python
  memory = parameters + gradients + optimizer_states + activations
  # Attention的activation占比是多少？
  ```
- **延伸**: 混合精度训练对Transformer特别有效的原因？

---

## 🎯 实践检查清单

### 编程实现 (必做)
- [ ] 实现Scaled Dot-Product Attention函数
- [ ] 实现Multi-Head Attention类
- [ ] 实现Position Encoding (sinusoidal)
- [ ] 实现完整的Transformer Block
- [ ] 在toy数据集上测试

### 数学推导 (推荐)
- [ ] 推导attention的梯度公式
- [ ] 分析softmax饱和对梯度的影响
- [ ] 计算Multi-Head Attention的参数量
- [ ] 分析attention的计算和内存复杂度

### 可视化分析 (推荐)
- [ ] 可视化attention权重矩阵
- [ ] 观察不同layers的attention模式变化
- [ ] 对比不同heads的关注模式
- [ ] 分析position encoding的周期性

### 效率优化 (进阶)
- [ ] Profile标准attention的性能
- [ ] 实现KV缓存优化推理
- [ ] 对比FlashAttention的加速效果
- [ ] 测量不同序列长度的性能曲线

---

## 📖 学习建议

### 学习策略
1. **先理论后实践**: 先深入思考24个问题，再动手编程
2. **对比学习**: 与RNN/CNN对比，理解Transformer的创新
3. **可视化辅助**: 画图帮助理解attention的工作机制
4. **系统思维**: 始终考虑效率、内存、可扩展性

### 时间安排
- **Day 1** (4-5小时): Q1-Q12 (Self-Attention + Multi-Head)
- **Day 2** (4-5小时): Q13-Q20 (Position + 架构设计)
- **Day 3** (4-5小时): Q21-Q24 (效率优化) + 编程实践
- **Day 4** (可选): 深度讨论记录 + 项目完善

### 常见陷阱
⚠️ **不要**只看代码实现而不理解数学原理
⚠️ **不要**忽略Position Encoding的重要性
⚠️ **不要**认为Attention是万能的（要理解其局限）
⚠️ **不要**忘记连接Lecture 02的资源分析思维

---

## 🔗 参考资料

### 必读论文
1. **Attention Is All You Need** (Vaswani et al., 2017) - 原始论文
2. **BERT** (Devlin et al., 2018) - Encoder-only架构
3. **GPT-2/3** (Radford et al., 2019/2020) - Decoder-only架构
4. **FlashAttention** (Dao et al., 2022) - 效率优化

### 推荐博客
- The Illustrated Transformer (Jay Alammar)
- The Annotated Transformer (Harvard NLP)
- Transformer Family (Lilian Weng)

### 课程资源
- 官方课件: `nonexecutable/2025 Lecture 3 - architecture.pdf`
- 代码参考: lecture_10.py, lecture_13.py (包含Transformer实现)

---

**创建日期**: 2025-11-10
**更新日期**: 2025-11-10 (基于官方课件完善)
**状态**: ✅ 完整教学框架 → 等待学员思考
**课程来源**: Stanford CS336 Spring 2025
