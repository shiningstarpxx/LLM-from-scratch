# Lecture 02: PyTorch Building Blocks & Resource Accounting - å®è·µä»£ç 

## ğŸš€ å®è·µæ¦‚è¿°

æœ¬èŠ‚åŒ…å«Lecture 02æ ¸å¿ƒæ¦‚å¿µçš„å®è·µå®ç°ï¼Œé‡ç‚¹å…³æ³¨ï¼š
- **FLOPè®¡ç®—å·¥å…·**çš„å®ç°
- **å†…å­˜åˆ†æå·¥å…·**çš„å¼€å‘
- **è®­ç»ƒå¾ªç¯**çš„å®Œæ•´æ„å»º
- **æ€§èƒ½ä¼˜åŒ–**æŠ€å·§çš„æ¼”ç¤º

---

## ğŸ“Š FLOPè®¡ç®—å·¥å…·

### ğŸ§® åŸºç¡€FLOPè®¡ç®—å™¨

```python
import torch
import torch.nn as nn
from typing import Dict, Any
import functools

class FLOPCalculator:
    """PyTorchæ¨¡å‹FLOPè®¡ç®—å™¨"""

    def __init__(self):
        self.flops = 0
        self.hooks = []

    def _conv_flop(self, input_shape: tuple, output_shape: tuple,
                   kernel_shape: tuple, groups: int = 1) -> int:
        """è®¡ç®—å·ç§¯æ“ä½œFLOP"""
        batch_size = input_shape[0]
        output_dims = output_shape[2:]
        kernel_dims = kernel_shape[2:]
        in_channels = input_shape[1]
        out_channels = output_shape[1]

        filters_per_channel = out_channels // groups
        conv_per_position_flops = functools.reduce(
            lambda a, b: a * b, kernel_dims) * in_channels // groups
        active_elements_count = batch_size * functools.reduce(
            lambda a, b: a * b, output_dims)

        overall_conv_flops = conv_per_position_flops * active_elements_count * filters_per_channel
        bias_flops = out_channels * active_elements_count

        return overall_conv_flops + bias_flops

    def _linear_flop(self, input_shape: tuple, weight_shape: tuple,
                     has_bias: bool = True) -> int:
        """è®¡ç®—çº¿æ€§å±‚FLOP"""
        batch_size = input_shape[0]
        in_features = input_shape[1]
        out_features = weight_shape[0]

        mul_flops = batch_size * in_features * out_features
        add_flops = batch_size * out_features if has_bias else 0

        return mul_flops + add_flops

    def _attention_flop(self, seq_len: int, hidden_dim: int,
                       num_heads: int = 8) -> int:
        """è®¡ç®—è‡ªæ³¨æ„åŠ›æœºåˆ¶FLOP"""
        head_dim = hidden_dim // num_heads

        # Q, K, VæŠ•å½±: 3 Ã— (seq_len Ã— hidden_dim Ã— hidden_dim)
        qkv_flops = 3 * seq_len * hidden_dim * hidden_dim

        # æ³¨æ„åŠ›åˆ†æ•°è®¡ç®—: seq_lenÂ² Ã— hidden_dim
        score_flops = seq_len * seq_len * hidden_dim

        # Softmax: 3 Ã— seq_lenÂ² (exp, sum, div)
        softmax_flops = 3 * seq_len * seq_len

        # æ³¨æ„åŠ›åŠ æƒ: seq_lenÂ² Ã— head_dim
        attn_flops = seq_len * seq_len * head_dim

        # è¾“å‡ºæŠ•å½±: seq_len Ã— hidden_dim Ã— hidden_dim
        output_flops = seq_len * hidden_dim * hidden_dim

        return qkv_flops + score_flops + softmax_flops + attn_flops + output_flops

    def _create_hook(self, layer_type: str):
        """åˆ›å»ºFLOPè®¡ç®—é’©å­"""
        def hook_fn(module, input, output):
            if layer_type == 'conv2d':
                if hasattr(module, 'weight'):
                    kernel_shape = module.weight.shape
                    input_shape = input[0].shape
                    output_shape = output.shape
                    flops = self._conv_flop(input_shape, output_shape,
                                          kernel_shape, module.groups)
                    self.flops += flops

            elif layer_type == 'linear':
                if hasattr(module, 'weight'):
                    input_shape = input[0].shape
                    weight_shape = module.weight.shape
                    has_bias = hasattr(module, 'bias') and module.bias is not None
                    flops = self._linear_flop(input_shape, weight_shape, has_bias)
                    self.flops += flops

            elif layer_type == 'matmul':
                # çŸ©é˜µä¹˜æ³•: 2 Ã— m Ã— k Ã— n
                input_shape = input[0].shape
                weight_shape = input[1].shape
                if len(input_shape) >= 2 and len(weight_shape) >= 2:
                    m = functools.reduce(lambda a, b: a * b, input_shape[:-1])
                    k = input_shape[-1]
                    n = weight_shape[-1]
                    flops = 2 * m * k * n
                    self.flops += flops

        return hook_fn

    def analyze_model(self, model: nn.Module, input_shape: tuple) -> Dict[str, Any]:
        """åˆ†ææ¨¡å‹FLOP"""
        self.flops = 0
        self.hooks = []

        # æ³¨å†Œé’©å­
        for name, module in model.named_modules():
            if isinstance(module, nn.Conv2d):
                hook = module.register_forward_hook(
                    self._create_hook('conv2d'))
                self.hooks.append(hook)

            elif isinstance(module, nn.Linear):
                hook = module.register_forward_hook(
                    self._create_hook('linear'))
                self.hooks.append(hook)

        # è¿è¡Œå‰å‘ä¼ æ’­
        device = next(model.parameters()).device
        dummy_input = torch.randn(input_shape, device=device)

        with torch.no_grad():
            _ = model(dummy_input)

        # æ¸…ç†é’©å­
        for hook in self.hooks:
            hook.remove()

        return {
            'total_flops': self.flops,
            'flops_readable': self._format_flops(self.flops),
            'parameters': sum(p.numel() for p in model.parameters()),
            'parameters_readable': self._format_count(
                sum(p.numel() for p in model.parameters()))
        }

    def _format_flops(self, flops: int) -> str:
        """æ ¼å¼åŒ–FLOPæ˜¾ç¤º"""
        if flops >= 1e15:
            return f"{flops/1e15:.2f} PFLOP"
        elif flops >= 1e12:
            return f"{flops/1e12:.2f} TFLOP"
        elif flops >= 1e9:
            return f"{flops/1e9:.2f} GFLOP"
        elif flops >= 1e6:
            return f"{flops/1e6:.2f} MFLOP"
        elif flops >= 1e3:
            return f"{flops/1e3:.2f} KFLOP"
        else:
            return f"{flops} FLOP"

    def _format_count(self, count: int) -> str:
        """æ ¼å¼åŒ–æ•°é‡æ˜¾ç¤º"""
        if count >= 1e9:
            return f"{count/1e9:.2f} B"
        elif count >= 1e6:
            return f"{count/1e6:.2f} M"
        elif count >= 1e3:
            return f"{count/1e3:.2f} K"
        else:
            return str(count)


# ä½¿ç”¨ç¤ºä¾‹
def demo_flop_calculator():
    """FLOPè®¡ç®—å™¨æ¼”ç¤º"""
    print("ğŸ§® FLOPè®¡ç®—å™¨æ¼”ç¤º")
    print("=" * 50)

    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    model = nn.Sequential(
        nn.Conv2d(3, 64, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.Conv2d(64, 128, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.AdaptiveAvgPool2d((1, 1)),
        nn.Flatten(),
        nn.Linear(128, 10)
    )

    calculator = FLOPCalculator()
    result = calculator.analyze_model(model, (1, 3, 32, 32))

    print(f"æ€»FLOP: {result['flops_readable']}")
    print(f"å‚æ•°æ•°é‡: {result['parameters_readable']}")
    print(f"æ¯å‚æ•°FLOP: {result['total_flops'] / result['parameters']:.2f}")
```

### ğŸ“ˆ æ³¨æ„åŠ›FLOPåˆ†æå™¨

```python
class AttentionFLOPAnalyzer:
    """æ³¨æ„åŠ›æœºåˆ¶FLOPåˆ†æå™¨"""

    @staticmethod
    def analyze_transformer_layer(seq_len: int, hidden_dim: int,
                                 num_heads: int, ffn_dim: int = None) -> Dict[str, int]:
        """åˆ†æTransformerå±‚çš„FLOP"""
        if ffn_dim is None:
            ffn_dim = hidden_dim * 4

        head_dim = hidden_dim // num_heads

        # æ³¨æ„åŠ›éƒ¨åˆ†
        qkv_flops = 3 * seq_len * hidden_dim * hidden_dim  # Q,K,VæŠ•å½±
        attn_score_flops = seq_len * seq_len * hidden_dim  # æ³¨æ„åŠ›åˆ†æ•°
        softmax_flops = 3 * seq_len * seq_len  # Softmax
        attn_output_flops = seq_len * hidden_dim * hidden_dim  # è¾“å‡ºæŠ•å½±

        attention_flops = (qkv_flops + attn_score_flops +
                          softmax_flops + attn_output_flops)

        # å‰é¦ˆç½‘ç»œéƒ¨åˆ†
        ffn_up_flops = seq_len * hidden_dim * ffn_dim  # ä¸ŠæŠ•å½±
        ffn_down_flops = seq_len * ffn_dim * hidden_dim  # ä¸‹æŠ•å½±

        ffn_flops = ffn_up_flops + ffn_down_flops

        # å±‚å½’ä¸€åŒ–å’Œæ®‹å·®è¿æ¥
        layernorm_flops = 2 * seq_len * hidden_dim  # ä¸¤ä¸ªå±‚å½’ä¸€åŒ–
        residual_flops = 2 * seq_len * hidden_dim  # ä¸¤ä¸ªæ®‹å·®è¿æ¥

        total_flops = attention_flops + ffn_flops + layernorm_flops + residual_flops

        return {
            'attention': attention_flops,
            'ffn': ffn_flops,
            'layernorm': layernorm_flops,
            'residual': residual_flops,
            'total': total_flops,
            'attention_percent': attention_flops / total_flops * 100,
            'ffn_percent': ffn_flops / total_flops * 100
        }

    @staticmethod
    def analyze_transformer_model(seq_len: int, hidden_dim: int,
                                 num_heads: int, num_layers: int,
                                 vocab_size: int) -> Dict[str, Any]:
        """åˆ†æå®Œæ•´Transformeræ¨¡å‹FLOP"""
        # å•å±‚FLOP
        layer_flops = AttentionFLOPAnalyzer.analyze_transformer_layer(
            seq_len, hidden_dim, num_heads)

        # æ‰€æœ‰å±‚
        total_layer_flops = layer_flops['total'] * num_layers

        # è¯åµŒå…¥å±‚
        embedding_flops = seq_len * vocab_size * hidden_dim

        # è¾“å‡ºå±‚
        output_flops = seq_len * hidden_dim * vocab_size

        total_flops = total_layer_flops + embedding_flops + output_flops

        return {
            'per_layer': layer_flops,
            'all_layers': total_layer_flops,
            'embedding': embedding_flops,
            'output': output_flops,
            'total': total_flops,
            'flops_per_token': total_flops / seq_len,
            'params_estimate': (  # ç²—ç•¥å‚æ•°ä¼°è®¡
                hidden_dim * vocab_size +  # è¯åµŒå…¥
                num_layers * (  # æ¯å±‚å‚æ•°
                    4 * hidden_dim * hidden_dim +  # æ³¨æ„åŠ› (Q,K,V,O)
                    2 * hidden_dim * hidden_dim * 4 +  # FFN
                    4 * hidden_dim  # å±‚å½’ä¸€åŒ–
                ) +
                hidden_dim * vocab_size  # è¾“å‡ºå±‚
            )
        }


def demo_attention_flop_analysis():
    """æ³¨æ„åŠ›FLOPåˆ†ææ¼”ç¤º"""
    print("\nğŸ“Š æ³¨æ„åŠ›FLOPåˆ†ææ¼”ç¤º")
    print("=" * 50)

    # GPT-2 Small é…ç½®
    configs = [
        {'name': 'GPT-2 Small', 'hidden_dim': 768, 'num_heads': 12, 'num_layers': 12, 'vocab_size': 50257},
        {'name': 'GPT-2 Medium', 'hidden_dim': 1024, 'num_heads': 16, 'num_layers': 24, 'vocab_size': 50257},
        {'name': 'GPT-2 Large', 'hidden_dim': 1280, 'num_heads': 20, 'num_layers': 36, 'vocab_size': 50257},
    ]

    seq_len = 1024

    for config in configs:
        result = AttentionFLOPAnalyzer.analyze_transformer_model(
            seq_len, config['hidden_dim'], config['num_heads'],
            config['num_layers'], config['vocab_size']
        )

        print(f"\nğŸ¤– {config['name']}:")
        print(f"  æ€»FLOP (seq_len={seq_len}): {result['total']/1e12:.2f} TFLOP")
        print(f"  æ¯token FLOP: {result['flops_per_token']/1e6:.2f} MFLOP")
        print(f"  ä¼°è®¡å‚æ•°: {result['params_estimate']/1e6:.1f}M")
        print(f"  æ³¨æ„åŠ›å æ¯”: {result['per_layer']['attention_percent']:.1f}%")
        print(f"  FFNå æ¯”: {result['per_layer']['ffn_percent']:.1f}%")
```

---

## ğŸ’¾ å†…å­˜åˆ†æå·¥å…·

### ğŸ“Š å†…å­˜ä½¿ç”¨è¿½è¸ªå™¨

```python
import torch
import psutil
import gc
from typing import Dict, List, Any
import time

class MemoryProfiler:
    """GPU/CPUå†…å­˜ä½¿ç”¨åˆ†æå™¨"""

    def __init__(self):
        self.snapshots = []
        self.peak_memory = {'cpu': 0, 'gpu': 0}

    def snapshot(self, label: str = ""):
        """æ‹æ‘„å†…å­˜å¿«ç…§"""
        # CPUå†…å­˜
        cpu_memory = psutil.virtual_memory()
        cpu_used = cpu_memory.used / 1024**3  # GB

        # GPUå†…å­˜
        gpu_memory = {'allocated': 0, 'cached': 0, 'max_allocated': 0}
        if torch.cuda.is_available():
            gpu_memory['allocated'] = torch.cuda.memory_allocated() / 1024**3
            gpu_memory['cached'] = torch.cuda.memory_reserved() / 1024**3
            gpu_memory['max_allocated'] = torch.cuda.max_memory_allocated() / 1024**3

        snapshot = {
            'timestamp': time.time(),
            'label': label,
            'cpu_used_gb': cpu_used,
            'cpu_percent': cpu_memory.percent,
            **gpu_memory
        }

        self.snapshots.append(snapshot)

        # æ›´æ–°å³°å€¼
        self.peak_memory['cpu'] = max(self.peak_memory['cpu'], cpu_used)
        if torch.cuda.is_available():
            self.peak_memory['gpu'] = max(self.peak_memory['gpu'], gpu_memory['allocated'])

        return snapshot

    def reset(self):
        """é‡ç½®åˆ†æå™¨"""
        self.snapshots = []
        self.peak_memory = {'cpu': 0, 'gpu': 0}
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()

    def analyze_model_memory(self, model: nn.Module, input_shape: tuple,
                           device: str = 'cuda') -> Dict[str, Any]:
        """åˆ†ææ¨¡å‹å†…å­˜ä½¿ç”¨"""
        self.reset()

        # æ¨¡å‹å‚æ•°å†…å­˜
        param_memory = sum(p.numel() * p.element_size()
                          for p in model.parameters()) / 1024**3

        # æ¨¡å‹æ¢¯åº¦å†…å­˜ (è®­ç»ƒæ—¶)
        grad_memory = param_memory  # ä¸å‚æ•°ç›¸åŒå¤§å°

        # ä¼˜åŒ–å™¨çŠ¶æ€å†…å­˜ (Adam)
        optimizer_memory = param_memory * 2  # åŠ¨é‡ + æ–¹å·®

        # åŸºçº¿å†…å­˜
        self.snapshot("baseline")

        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
        if device == 'cuda':
            model = model.cuda()
        self.snapshot("model_loaded")

        # å‰å‘ä¼ æ’­
        dummy_input = torch.randn(input_shape, device=device)
        with torch.no_grad():
            _ = model(dummy_input)
        self.snapshot("forward_pass")

        # åå‘ä¼ æ’­
        dummy_output = model(dummy_input)
        dummy_loss = dummy_output.mean()
        dummy_loss.backward()
        self.snapshot("backward_pass")

        # è®¡ç®—æ¿€æ´»å†…å­˜
        activation_memory = (self.snapshots[-1]['gpu_allocated'] if device == 'cuda'
                           else self.snapshots[-1]['cpu_used_gb']) - param_memory

        return {
            'parameter_memory_gb': param_memory,
            'gradient_memory_gb': grad_memory,
            'optimizer_memory_gb': optimizer_memory,
            'activation_memory_gb': activation_memory,
            'total_training_memory_gb': param_memory + grad_memory + optimizer_memory + activation_memory,
            'total_inference_memory_gb': param_memory + activation_memory,
            'snapshots': self.snapshots
        }

    def print_memory_analysis(self, analysis: Dict[str, Any]):
        """æ‰“å°å†…å­˜åˆ†æç»“æœ"""
        print("\nğŸ’¾ å†…å­˜ä½¿ç”¨åˆ†æ")
        print("=" * 50)
        print(f"å‚æ•°å†…å­˜: {analysis['parameter_memory_gb']:.2f} GB")
        print(f"æ¢¯åº¦å†…å­˜: {analysis['gradient_memory_gb']:.2f} GB")
        print(f"ä¼˜åŒ–å™¨å†…å­˜: {analysis['optimizer_memory_gb']:.2f} GB")
        print(f"æ¿€æ´»å†…å­˜: {analysis['activation_memory_gb']:.2f} GB")
        print("-" * 50)
        print(f"è®­ç»ƒæ€»å†…å­˜: {analysis['total_training_memory_gb']:.2f} GB")
        print(f"æ¨ç†æ€»å†…å­˜: {analysis['total_inference_memory_gb']:.2f} GB")


def demo_memory_profiler():
    """å†…å­˜åˆ†æå™¨æ¼”ç¤º"""
    print("ğŸ’¾ å†…å­˜åˆ†æå™¨æ¼”ç¤º")
    print("=" * 50)

    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    model = nn.Sequential(
        nn.Linear(1024, 4096),
        nn.ReLU(),
        nn.Linear(4096, 4096),
        nn.ReLU(),
        nn.Linear(4096, 1024)
    )

    profiler = MemoryProfiler()

    if torch.cuda.is_available():
        analysis = profiler.analyze_model_memory(model, (32, 1024), 'cuda')
    else:
        analysis = profiler.analyze_model_memory(model, (32, 1024), 'cpu')

    profiler.print_memory_analysis(analysis)
```

---

## ğŸ”„ è®­ç»ƒå¾ªç¯å®ç°

### ğŸ—ï¸ å®Œæ•´è®­ç»ƒå¾ªç¯

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import time
from typing import Dict, Any, Optional
import logging

class AdvancedTrainingLoop:
    """é«˜çº§è®­ç»ƒå¾ªç¯å®ç°"""

    def __init__(self,
                 model: nn.Module,
                 train_loader: DataLoader,
                 val_loader: Optional[DataLoader] = None,
                 optimizer: Optional[optim.Optimizer] = None,
                 scheduler: Optional[optim.lr_scheduler._LRScheduler] = None,
                 device: str = 'cuda',
                 mixed_precision: bool = True,
                 gradient_accumulation_steps: int = 1,
                 max_grad_norm: float = 1.0,
                 use_wandb: bool = False):

        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.device = device
        self.mixed_precision = mixed_precision
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.max_grad_norm = max_grad_norm
        self.use_wandb = use_wandb

        # è®¾ç½®è®¾å¤‡
        self.model = model.to(device)

        # æ··åˆç²¾åº¦
        if mixed_precision:
            self.scaler = torch.cuda.amp.GradScaler()

        # æ—¥å¿—
        self.logger = logging.getLogger(__name__)
        self.metrics_history = {
            'train_loss': [],
            'val_loss': [],
            'learning_rate': [],
            'throughput': [],
            'gpu_memory': []
        }

    def train_epoch(self, epoch: int) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        num_batches = len(self.train_loader)

        # æ€§èƒ½æµ‹é‡
        start_time = time.time()
        num_samples = 0

        # æ¢¯åº¦ç´¯ç§¯
        self.optimizer.zero_grad()

        for batch_idx, batch in enumerate(self.train_loader):
            # æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
            if isinstance(batch, (list, tuple)):
                inputs = batch[0].to(self.device)
                targets = batch[1].to(self.device)
            else:
                inputs = batch.to(self.device)
                targets = None

            batch_size = inputs.size(0)
            num_samples += batch_size

            # å‰å‘ä¼ æ’­
            with torch.cuda.amp.autocast(enabled=self.mixed_precision):
                outputs = self.model(inputs)

                if targets is not None:
                    loss = nn.CrossEntropyLoss()(outputs, targets)
                else:
                    # è‡ªç›‘ç£å­¦ä¹ çš„æŸå¤±è®¡ç®—
                    loss = outputs.mean()  # ç¤ºä¾‹

                # æ¢¯åº¦ç´¯ç§¯ç¼©æ”¾
                loss = loss / self.gradient_accumulation_steps

            # åå‘ä¼ æ’­
            if self.mixed_precision:
                self.scaler.scale(loss).backward()
            else:
                loss.backward()

            # å‚æ•°æ›´æ–°
            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                if self.mixed_precision:
                    # æ¢¯åº¦è£å‰ª
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)

                    # ä¼˜åŒ–å™¨æ­¥éª¤
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)
                    self.optimizer.step()

                self.optimizer.zero_grad()

                # å­¦ä¹ ç‡è°ƒåº¦
                if self.scheduler is not None:
                    self.scheduler.step()

            total_loss += loss.item() * self.gradient_accumulation_steps

            # æ—¥å¿—è®°å½•
            if batch_idx % 100 == 0:
                current_lr = self.optimizer.param_groups[0]['lr']
                gpu_memory = torch.cuda.memory_allocated() / 1024**3 if torch.cuda.is_available() else 0

                self.logger.info(
                    f'Epoch {epoch}, Batch {batch_idx}/{num_batches}, '
                    f'Loss: {loss.item():.4f}, LR: {current_lr:.6f}, '
                    f'GPU Mem: {gpu_memory:.2f}GB'
                )

                if self.use_wandb:
                    wandb.log({
                        'batch_loss': loss.item(),
                        'learning_rate': current_lr,
                        'gpu_memory': gpu_memory
                    })

        # è®¡ç®—å¹³å‡æŸå¤±å’Œååé‡
        avg_loss = total_loss / num_batches
        epoch_time = time.time() - start_time
        throughput = num_samples / epoch_time

        # è®°å½•æŒ‡æ ‡
        self.metrics_history['train_loss'].append(avg_loss)
        self.metrics_history['throughput'].append(throughput)
        if self.optimizer:
            self.metrics_history['learning_rate'].append(self.optimizer.param_groups[0]['lr'])

        return {
            'train_loss': avg_loss,
            'throughput': throughput,
            'epoch_time': epoch_time
        }

    def validate(self, epoch: int) -> Dict[str, float]:
        """éªŒè¯æ¨¡å‹"""
        if self.val_loader is None:
            return {}

        self.model.eval()
        total_loss = 0.0
        num_batches = len(self.val_loader)
        correct = 0
        total = 0

        with torch.no_grad():
            for batch in self.val_loader:
                if isinstance(batch, (list, tuple)):
                    inputs = batch[0].to(self.device)
                    targets = batch[1].to(self.device)
                else:
                    inputs = batch.to(self.device)
                    targets = None

                # å‰å‘ä¼ æ’­
                with torch.cuda.amp.autocast(enabled=self.mixed_precision):
                    outputs = self.model(inputs)

                    if targets is not None:
                        loss = nn.CrossEntropyLoss()(outputs, targets)
                        total_loss += loss.item()

                        # è®¡ç®—å‡†ç¡®ç‡
                        _, predicted = torch.max(outputs.data, 1)
                        total += targets.size(0)
                        correct += (predicted == targets).sum().item()

        avg_loss = total_loss / num_batches
        accuracy = correct / total if total > 0 else 0.0

        self.metrics_history['val_loss'].append(avg_loss)

        return {
            'val_loss': avg_loss,
            'val_accuracy': accuracy
        }

    def train(self, num_epochs: int, save_path: Optional[str] = None) -> Dict[str, Any]:
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        self.logger.info(f'å¼€å§‹è®­ç»ƒï¼Œå…± {num_epochs} ä¸ªepoch')

        best_val_loss = float('inf')

        for epoch in range(num_epochs):
            # è®­ç»ƒ
            train_metrics = self.train_epoch(epoch)

            # éªŒè¯
            val_metrics = self.validate(epoch)

            # åˆå¹¶æŒ‡æ ‡
            epoch_metrics = {**train_metrics, **val_metrics}

            # æ—¥å¿—
            log_str = f'Epoch {epoch + 1}/{num_epochs} - '
            log_str += f'Train Loss: {train_metrics["train_loss"]:.4f}, '
            log_str += f'Throughput: {train_metrics["throughput"]:.2f} samples/s'

            if val_metrics:
                log_str += f', Val Loss: {val_metrics["val_loss"]:.4f}'
                if 'val_accuracy' in val_metrics:
                    log_str += f', Val Acc: {val_metrics["val_accuracy"]:.4f}'

            self.logger.info(log_str)

            # WandBæ—¥å¿—
            if self.use_wandb:
                wandb.log(epoch_metrics)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if save_path and val_metrics.get('val_loss', float('inf')) < best_val_loss:
                best_val_loss = val_metrics['val_loss']
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
                    'metrics_history': self.metrics_history,
                    'best_val_loss': best_val_loss
                }, save_path)
                self.logger.info(f'ä¿å­˜æœ€ä½³æ¨¡å‹åˆ° {save_path}')

        return {
            'metrics_history': self.metrics_history,
            'best_val_loss': best_val_loss
        }


def demo_training_loop():
    """è®­ç»ƒå¾ªç¯æ¼”ç¤º"""
    print("ğŸ”„ è®­ç»ƒå¾ªç¯æ¼”ç¤º")
    print("=" * 50)

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    class DummyDataset(torch.utils.data.Dataset):
        def __init__(self, size=1000):
            self.size = size

        def __len__(self):
            return self.size

        def __getitem__(self, idx):
            return torch.randn(512), torch.randint(0, 10, (1,)).squeeze()

    train_dataset = DummyDataset(1000)
    val_dataset = DummyDataset(200)

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32)

    # åˆ›å»ºæ¨¡å‹
    model = nn.Sequential(
        nn.Linear(512, 1024),
        nn.ReLU(),
        nn.Linear(1024, 512),
        nn.ReLU(),
        nn.Linear(512, 10)
    )

    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)

    # åˆ›å»ºè®­ç»ƒå¾ªç¯
    trainer = AdvancedTrainingLoop(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        optimizer=optimizer,
        scheduler=scheduler,
        device='cuda' if torch.cuda.is_available() else 'cpu',
        mixed_precision=torch.cuda.is_available(),
        gradient_accumulation_steps=1
    )

    # è®­ç»ƒå‡ ä¸ªepochä½œä¸ºæ¼”ç¤º
    print("å¼€å§‹è®­ç»ƒæ¼”ç¤º...")
    metrics = trainer.train(num_epochs=3)

    print(f"\nè®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯æŸå¤±: {metrics['best_val_loss']:.4f}")
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–æŠ€å·§æ¼”ç¤º

### ğŸš€ æ··åˆç²¾åº¦è®­ç»ƒ

```python
def demo_mixed_precision():
    """æ··åˆç²¾åº¦è®­ç»ƒæ¼”ç¤º"""
    print("âš¡ æ··åˆç²¾åº¦è®­ç»ƒæ¼”ç¤º")
    print("=" * 50)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    # åˆ›å»ºå¤§ä¸€ç‚¹æ¨¡å‹ä»¥æ˜¾ç¤ºå·®å¼‚
    model = nn.Sequential(
        nn.Linear(4096, 8192),
        nn.ReLU(),
        nn.Linear(8192, 4096),
        nn.ReLU(),
        nn.Linear(4096, 1000)
    ).to(device)

    # æµ‹è¯•æ•°æ®
    batch_size = 64
    input_data = torch.randn(batch_size, 4096, device=device)
    target = torch.randint(0, 1000, (batch_size,), device=device)

    # FP32è®­ç»ƒ
    print("ğŸ”¢ FP32è®­ç»ƒ:")
    model_fp32 = model.float()
    optimizer_fp32 = optim.Adam(model_fp32.parameters())

    torch.cuda.synchronize()
    start_time = time.time()

    for _ in range(10):
        optimizer_fp32.zero_grad()
        output = model_fp32(input_data)
        loss = nn.CrossEntropyLoss()(output, target)
        loss.backward()
        optimizer_fp32.step()

    torch.cuda.synchronize()
    fp32_time = time.time() - start_time

    fp32_memory = torch.cuda.max_memory_allocated() / 1024**3
    torch.cuda.reset_peak_memory_stats()

    print(f"  æ—¶é—´: {fp32_time:.3f}s")
    print(f"  å³°å€¼å†…å­˜: {fp32_memory:.2f}GB")

    if torch.cuda.is_available():
        # FP16æ··åˆç²¾åº¦è®­ç»ƒ
        print("\nğŸ”€ FP16æ··åˆç²¾åº¦è®­ç»ƒ:")
        model_fp16 = model
        optimizer_fp16 = optim.Adam(model_fp16.parameters())
        scaler = torch.cuda.amp.GradScaler()

        torch.cuda.synchronize()
        start_time = time.time()

        for _ in range(10):
            optimizer_fp16.zero_grad()

            with torch.cuda.amp.autocast():
                output = model_fp16(input_data)
                loss = nn.CrossEntropyLoss()(output, target)

            scaler.scale(loss).backward()
            scaler.step(optimizer_fp16)
            scaler.update()

        torch.cuda.synchronize()
        fp16_time = time.time() - start_time

        fp16_memory = torch.cuda.max_memory_allocated() / 1024**3

        print(f"  æ—¶é—´: {fp16_time:.3f}s")
        print(f"  å³°å€¼å†…å­˜: {fp16_memory:.2f}GB")
        print(f"\nğŸ“Š æ€§èƒ½æå‡:")
        print(f"  åŠ é€Ÿæ¯”: {fp32_time/fp16_time:.2f}x")
        print(f"  å†…å­˜èŠ‚çœ: {(fp32_memory-fp16_memory)/fp32_memory*100:.1f}%")
```

### ğŸ§  æ¢¯åº¦ç´¯ç§¯æ¼”ç¤º

```python
def demo_gradient_accumulation():
    """æ¢¯åº¦ç´¯ç§¯æ¼”ç¤º"""
    print("\nğŸ§  æ¢¯åº¦ç´¯ç§¯æ¼”ç¤º")
    print("=" * 50)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    # å°æ¨¡å‹ï¼Œå¤§æ‰¹æ¬¡æ•ˆæœæ˜æ˜¾
    model = nn.Linear(1000, 1000).to(device)
    optimizer = optim.SGD(model.parameters(), lr=1e-3)

    # æ¨¡æ‹Ÿå¤§æ‰¹æ¬¡æ•°æ®
    big_batch_size = 128
    small_batch_size = 32
    accumulation_steps = big_batch_size // small_batch_size

    # æ­£å¸¸å¤§æ‰¹æ¬¡è®­ç»ƒ
    print(f"ğŸ“¦ æ­£å¸¸æ‰¹æ¬¡ (batch_size={big_batch_size}):")
    big_data = torch.randn(big_batch_size, 1000, device=device)
    big_target = torch.randn(big_batch_size, 1000, device=device)

    optimizer.zero_grad()
    torch.cuda.synchronize() if torch.cuda.is_available() else None
    start_time = time.time()

    output = model(big_data)
    loss = nn.MSELoss()(output, big_target)
    loss.backward()
    optimizer.step()

    torch.cuda.synchronize() if torch.cuda.is_available() else None
    normal_time = time.time() - start_time
    normal_memory = torch.cuda.max_memory_allocated() / 1024**3 if torch.cuda.is_available() else 0

    print(f"  æ—¶é—´: {normal_time:.4f}s")
    print(f"  å†…å­˜: {normal_memory:.2f}GB")

    # æ¢¯åº¦ç´¯ç§¯è®­ç»ƒ
    print(f"\nğŸ”„ æ¢¯åº¦ç´¯ç§¯ (batch_size={small_batch_size}, steps={accumulation_steps}):")
    torch.cuda.reset_peak_memory_stats() if torch.cuda.is_available() else None

    optimizer.zero_grad()
    torch.cuda.synchronize() if torch.cuda.is_available() else None
    start_time = time.time()

    for step in range(accumulation_steps):
        small_data = torch.randn(small_batch_size, 1000, device=device)
        small_target = torch.randn(small_batch_size, 1000, device=device)

        output = model(small_data)
        loss = nn.MSELoss()(output, small_target) / accumulation_steps
        loss.backward()

    optimizer.step()

    torch.cuda.synchronize() if torch.cuda.is_available() else None
    accum_time = time.time() - start_time
    accum_memory = torch.cuda.max_memory_allocated() / 1024**3 if torch.cuda.is_available() else 0

    print(f"  æ—¶é—´: {accum_time:.4f}s")
    print(f"  å†…å­˜: {accum_memory:.2f}GB")
    print(f"\nğŸ“Š æ•ˆæœå¯¹æ¯”:")
    print(f"  å†…å­˜èŠ‚çœ: {(normal_memory-accum_memory)/normal_memory*100:.1f}%")
    print(f"  æ—¶é—´å¼€é”€: {accum_time/normal_time:.2f}x")
```

---

## ğŸ¯ ç»¼åˆæ€§èƒ½æµ‹è¯•

### ğŸ“Š å®Œæ•´åŸºå‡†æµ‹è¯•

```python
def comprehensive_benchmark():
    """ç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\nğŸ¯ ç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 60)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"è®¾å¤‡: {device}")

    # æµ‹è¯•é…ç½®
    configs = [
        {'name': 'Small', 'hidden': 256, 'layers': 2, 'batch': 64},
        {'name': 'Medium', 'hidden': 512, 'layers': 4, 'batch': 32},
        {'name': 'Large', 'hidden': 1024, 'layers': 8, 'batch': 16},
    ]

    results = []

    for config in configs:
        print(f"\nğŸ¤– {config['name']} æ¨¡å‹:")
        print("-" * 40)

        # åˆ›å»ºæ¨¡å‹
        layers = []
        for _ in range(config['layers']):
            layers.extend([
                nn.Linear(config['hidden'], config['hidden']),
                nn.ReLU()
            ])
        layers.append(nn.Linear(config['hidden'], 10))
        model = nn.Sequential(*layers).to(device)

        # FLOPåˆ†æ
        flop_calc = FLOPCalculator()
        flop_result = flop_calc.analyze_model(model, (config['batch'], config['hidden']))

        # å†…å­˜åˆ†æ
        mem_profiler = MemoryProfiler()
        mem_analysis = mem_profiler.analyze_model_memory(
            model, (config['batch'], config['hidden']), device)

        # æ€§èƒ½æµ‹è¯•
        input_data = torch.randn(config['batch'], config['hidden'], device=device)

        # å‰å‘ä¼ æ’­æ€§èƒ½
        model.eval()
        with torch.no_grad():
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            start_time = time.time()

            for _ in range(100):
                _ = model(input_data)

            torch.cuda.synchronize() if torch.cuda.is_available() else None
            forward_time = (time.time() - start_time) / 100

        # è®¡ç®—ååé‡
        throughput = config['batch'] / forward_time
        flops_utilization = flop_result['total_flops'] / (forward_time * 1e12)  # TFLOP

        result = {
            'config': config['name'],
            'flops': flop_result['flops_readable'],
            'parameters': flop_result['parameters_readable'],
            'training_memory': f"{mem_analysis['total_training_memory_gb']:.2f}GB",
            'inference_memory': f"{mem_analysis['total_inference_memory_gb']:.2f}GB",
            'throughput': f"{throughput:.1f} samples/s",
            'forward_time': f"{forward_time*1000:.2f}ms",
            'flops_utilization': f"{flops_utilization:.2f} TFLOP/s"
        }

        results.append(result)

        print(f"  FLOP: {result['flops']}")
        print(f"  å‚æ•°: {result['parameters']}")
        print(f"  è®­ç»ƒå†…å­˜: {result['training_memory']}")
        print(f"  æ¨ç†å†…å­˜: {result['inference_memory']}")
        print(f"  ååé‡: {result['throughput']}")
        print(f"  å‰å‘æ—¶é—´: {result['forward_time']}")

    # ç»“æœæ±‡æ€»è¡¨
    print(f"\nğŸ“Š æ€§èƒ½æ±‡æ€»è¡¨:")
    print("-" * 80)
    print(f"{'æ¨¡å‹':<8} {'FLOP':<12} {'å‚æ•°':<10} {'è®­ç»ƒå†…å­˜':<10} {'ååé‡':<15}")
    print("-" * 80)

    for result in results:
        print(f"{result['config']:<8} {result['flops']:<12} {result['parameters']:<10} "
              f"{result['training_memory']:<10} {result['throughput']:<15}")

    return results


if __name__ == "__main__":
    print("ğŸš€ PyTorch Building Blocks & Resource Accounting å®è·µæ¼”ç¤º")
    print("=" * 80)

    # è¿è¡Œæ‰€æœ‰æ¼”ç¤º
    demo_flop_calculator()
    demo_attention_flop_analysis()
    demo_memory_profiler()
    demo_training_loop()

    if torch.cuda.is_available():
        demo_mixed_precision()
        demo_gradient_accumulation()

    comprehensive_benchmark()

    print("\nâœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ’¡ å…³é”®æ”¶è·:")
    print("1. å­¦ä¼šäº†ç²¾ç¡®è®¡ç®—æ¨¡å‹çš„FLOPå’Œå†…å­˜éœ€æ±‚")
    print("2. æŒæ¡äº†å®Œæ•´çš„è®­ç»ƒå¾ªç¯å®ç°")
    print("3. ç†è§£äº†æ··åˆç²¾åº¦å’Œæ¢¯åº¦ç´¯ç§¯ç­‰ä¼˜åŒ–æŠ€æœ¯")
    print("4. å»ºç«‹äº†æ€§èƒ½åˆ†æå’Œä¼˜åŒ–çš„æ€ç»´æ¨¡å¼")
```

---

## ğŸ“ ä½¿ç”¨è¯´æ˜

### ğŸ¯ è¿è¡Œç¯å¢ƒè¦æ±‚

```bash
# åŸºç¡€ä¾èµ–
pip install torch torchvision
pip install numpy psutil
pip install wandb  # å¯é€‰ï¼Œç”¨äºå®éªŒè·Ÿè¸ª

# GPUæ”¯æŒ (CUDA)
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

### ğŸš€ å¿«é€Ÿå¼€å§‹

```python
# è¿è¡Œå®Œæ•´æ¼”ç¤º
python practice_code.py

# å•ç‹¬æµ‹è¯•æŸä¸ªç»„ä»¶
from practice_code import FLOPCalculator, MemoryProfiler, AdvancedTrainingLoop

# FLOPè®¡ç®—
calc = FLOPCalculator()
result = calc.analyze_model(model, input_shape)

# å†…å­˜åˆ†æ
profiler = MemoryProfiler()
analysis = profiler.analyze_model_memory(model, input_shape)
```

### ğŸ“Š è‡ªå®šä¹‰ä½¿ç”¨

æ‰€æœ‰å·¥å…·éƒ½è®¾è®¡ä¸ºå¯å¤ç”¨çš„ç»„ä»¶ï¼Œå¯ä»¥è½»æ¾é›†æˆåˆ°è‡ªå·±çš„é¡¹ç›®ä¸­ï¼š

- **FLOPCalculator**: ç”¨äºåˆ†ææ¨¡å‹è®¡ç®—å¤æ‚åº¦
- **MemoryProfiler**: ç”¨äºç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ
- **AdvancedTrainingLoop**: ç”¨äºæ„å»ºé«˜æ•ˆè®­ç»ƒæµç¨‹
- **æ€§èƒ½ä¼˜åŒ–å‡½æ•°**: ç”¨äºæå‡è®­ç»ƒæ•ˆç‡

---

**ğŸ“ å¤‡æ³¨**: è¿™äº›ä»£ç éƒ½æ˜¯æ•™å­¦æ€§è´¨çš„å®ç°ï¼Œä¸“æ³¨äºæ¦‚å¿µç†è§£å’Œå®è·µæ¼”ç¤ºã€‚åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå»ºè®®ä½¿ç”¨æˆç†Ÿçš„å·¥å…·åº“å¦‚fvcoreã€torchinfoç­‰ã€‚