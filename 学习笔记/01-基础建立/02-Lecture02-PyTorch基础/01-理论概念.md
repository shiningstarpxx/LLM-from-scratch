# Lecture 02: PyTorch Building Blocks & Resource Accounting - ç†è®ºæ¦‚å¿µ

## ğŸ“š è¯¾ç¨‹æ¦‚è¿°

### ğŸ¯ å­¦ä¹ ç›®æ ‡

æœ¬è®²åº§èšç„¦äºæ·±åº¦å­¦ä¹ ç³»ç»Ÿçš„åŸºç¡€æ„å»ºå—ï¼Œç‰¹åˆ«æ˜¯ï¼š
- **ç†è§£PyTorchçš„æ ¸å¿ƒç»„ä»¶**
- **æŒæ¡èµ„æºè®¡ç®—å’Œæ€§èƒ½åˆ†æ**
- **å­¦ä¼šæ„å»ºé«˜æ•ˆçš„è®­ç»ƒå¾ªç¯**
- **åŸ¹å…»ç³»ç»Ÿä¼˜åŒ–æ€ç»´**

### ğŸ” æ ¸å¿ƒä¸»é¢˜

1. **å†…å­˜å’Œè®¡ç®—åŸºç¡€** - ç†è§£æ·±åº¦å­¦ä¹ ä¸­çš„èµ„æºéœ€æ±‚
2. **Tensoræ“ä½œå’ŒFLOPè®¡ç®—** - ç²¾ç¡®è®¡ç®—è®¡ç®—å¤æ‚åº¦
3. **èµ„æºè´¦åŠ¡å®è·µ** - å®é™…é¡¹ç›®çš„èµ„æºç®¡ç†
4. **æ„å»ºå®Œæ•´è®­ç»ƒå¾ªç¯** - ä»é›¶å¼€å§‹å®ç°è®­ç»ƒæµç¨‹
5. **æ€§èƒ½ä¼˜åŒ–æŠ€å·§** - æå‡è®­ç»ƒå’Œæ¨ç†æ•ˆç‡

---

## ğŸ§  å†…å­˜å’Œè®¡ç®—åŸºç¡€

### ğŸ’¾ å†…å­˜å±‚æ¬¡ç»“æ„

#### è®¡ç®—æœºå†…å­˜é‡‘å­—å¡”
```
        Registers (CPU)
           â†“ 1-5 cycles
        L1 Cache
           â†“ 3-10 cycles
        L2 Cache
           â†“ 10-20 cycles
        L3 Cache
           â†“ 40-75 cycles
        Main Memory (RAM)
           â†“ 200-300 cycles
        SSD Storage
           â†“ 50,000-100,000 cycles
        HDD Storage
```

#### æ·±åº¦å­¦ä¹ ä¸­çš„å†…å­˜ä½¿ç”¨

**1. æ¨¡å‹å‚æ•°å­˜å‚¨**
- **æƒé‡**: `å‚æ•°æ•°é‡ Ã— æ•°æ®ç±»å‹å¤§å°`
- **æ¢¯åº¦**: ä¸æƒé‡ç›¸åŒå¤§å°
- **ä¼˜åŒ–å™¨çŠ¶æ€**: Adaméœ€è¦2Ã—æƒé‡å¤§å°ï¼ˆåŠ¨é‡+æ–¹å·®ï¼‰

**2. æ¿€æ´»å€¼å­˜å‚¨**
- **å‰å‘ä¼ æ’­**: ä¸­é—´å±‚çš„è¾“å‡º
- **åå‘ä¼ æ’­**: éœ€è¦ä¿å­˜ç”¨äºæ¢¯åº¦è®¡ç®—
- **å†…å­˜å ç”¨**: `batch_size Ã— sequence_length Ã— hidden_size Ã— dtype_size`

**3. ç¼“å†²åŒºå’Œä¸´æ—¶å­˜å‚¨**
- **æ³¨æ„åŠ›æƒé‡**: `batch_size Ã— num_heads Ã— seq_lenÂ²`
- **ä¸´æ—¶è®¡ç®—ç»“æœ**: çŸ©é˜µä¹˜æ³•çš„ä¸­é—´ç»“æœ

### ğŸ”„ è®¡ç®—æ¨¡å¼

#### **çŸ©é˜µä¹˜æ³•ä¸ºæ ¸å¿ƒ**
```python
# çº¿æ€§å±‚: y = xW^T + b
# è®¡ç®—å¤æ‚åº¦: O(batch_size Ã— input_dim Ã— output_dim)

# æ³¨æ„åŠ›æœºåˆ¶: QK^T
# è®¡ç®—å¤æ‚åº¦: O(batch_size Ã— num_heads Ã— seq_lenÂ² Ã— head_dim)
```

#### **å†…å­˜è®¿é—®æ¨¡å¼**
- **é¡ºåºè®¿é—®**: è¿ç»­å†…å­˜è¯»å–ï¼Œæ•ˆç‡é«˜
- **éšæœºè®¿é—®**: è·³è·ƒå¼å†…å­˜è¯»å–ï¼Œæ•ˆç‡ä½
- **ç¼“å­˜å‹å¥½**: æ•°æ®å±€éƒ¨æ€§å¯¹æ€§èƒ½çš„å½±å“

---

## ğŸ“Š Tensoræ“ä½œå’ŒFLOPè®¡ç®—

### ğŸ§® FLOP (Floating Point Operations) å®šä¹‰

**FLOP** = æµ®ç‚¹è¿ç®—æ¬¡æ•°
**FLOPS** = æ¯ç§’æµ®ç‚¹è¿ç®—æ¬¡æ•° (æ€§èƒ½æŒ‡æ ‡)

#### **åŸºæœ¬æ“ä½œçš„FLOPè®¡ç®—**

**1. çŸ©é˜µä¹˜æ³•**
```python
# A: (m, k), B: (k, n) â†’ C: (m, n)
# FLOP = 2 Ã— m Ã— k Ã— n
# (ä¹˜æ³•å’ŒåŠ æ³•å„ç®—ä¸€æ¬¡FLOP)
# æœ€ç»ˆçš„çŸ©é˜µå¤§å°æ˜¯ m * n, æ¯ä¸ªå…ƒç´ æ˜¯ k æ¬¡ä¹˜æ³•ï¼Œk-1 æ¬¡åŠ æ³•å¾—åˆ°ï¼Œæ‰€ä»¥æ˜¯ 2 * k * m * n

# ç¤ºä¾‹: (1024, 1024) Ã— (1024, 1024)
FLOP = 2 Ã— 1024 Ã— 1024 Ã— 1024 = 2,147,483,648 â‰ˆ 2.1 GFLOP
```

**2. å·ç§¯æ“ä½œ**
```python
# è¾“å…¥: (N, C_in, H, W)
# å·ç§¯æ ¸: (C_out, C_in, K_h, K_w)
# è¾“å‡º: (N, C_out, H_out, W_out)

# FLOP = 2 Ã— N Ã— C_out Ã— H_out Ã— W_out Ã— C_in Ã— K_h Ã— K_w
```

**3. æ³¨æ„åŠ›æœºåˆ¶**
```python
# Q, K, V: (batch, seq_len, hidden_dim)
# FLOP__QK^T = 2 Ã— batch Ã— num_heads Ã— seq_lenÂ² Ã— head_dim
# FLOP_softmax = 3 Ã— batch Ã— num_heads Ã— seq_lenÂ²
# FLOP_Attention = 2 Ã— batch Ã— num_heads Ã— seq_lenÂ² Ã— head_dim

# æ€»FLOP â‰ˆ 4 Ã— batch Ã— seq_lenÂ² Ã— hidden_dim
```

### ğŸ“ˆ å®é™…æ¨¡å‹FLOPåˆ†æ

#### **Transformeræ¨¡å‹ç¤ºä¾‹**
```python
# GPT-2 Small (117Må‚æ•°)
# å‰å‘ä¼ æ’­FLOP per token:
# - æ³¨æ„åŠ›å±‚: 2 Ã— d_model Ã— d_model = 2 Ã— 768 Ã— 768
# - å‰é¦ˆç½‘ç»œ: 2 Ã— d_model Ã— 4Ã—d_model = 2 Ã— 768 Ã— 3072
# æ€»FLOP per token â‰ˆ 6M FLOP

# æ¨ç†ååé‡è®¡ç®—:
# å¦‚æœGPUæ€§èƒ½ä¸º 10 TFLOP = 10^13 FLOP/s
# ç†è®ºååé‡ = 10^13 / 6Ã—10^6 â‰ˆ 1.67M tokens/s
```

### ğŸ” FLOPè®¡ç®—å·¥å…·

#### **PyTorchå†…ç½®å·¥å…·**
```python
import torch
import torch.profiler

# ä½¿ç”¨profilerè®¡ç®—FLOP
with torch.profiler.profile(
    activities=[torch.profiler.ProfilerActivity.CPU],
    with_flops=True
) as prof:
    output = model(input)

print(prof.key_averages().table(sort_by="flops"))
```

#### **ç¬¬ä¸‰æ–¹åº“**
- **fvcore**: Facebook Researchçš„FLOPè®¡ç®—åº“
- **ptflops**: ä¸“é—¨ç”¨äºPyTorchæ¨¡å‹
- **thop**: è½»é‡çº§FLOPè®¡ç®—

---

## ğŸ’° èµ„æºè´¦åŠ¡å®è·µ

### ğŸ“Š èµ„æºéœ€æ±‚åˆ†æ

#### **è®­ç»ƒèµ„æºè®¡ç®—**

**1. å•GPUè®­ç»ƒ**
```python
# ç¤ºä¾‹: 7Bå‚æ•°æ¨¡å‹
å‚æ•°å†…å­˜ = 7B Ã— 2 bytes (FP16) = 14GB
æ¢¯åº¦å†…å­˜ = 7B Ã— 2 bytes = 14GB
ä¼˜åŒ–å™¨å†…å­˜ = 7B Ã— 2 Ã— 2 bytes = 28GB (Adam)
æ¿€æ´»å†…å­˜ = batch_size Ã— seq_len Ã— layers Ã— hidden_size Ã— 4

æ€»å†…å­˜éœ€æ±‚ â‰ˆ 56GB + æ¿€æ´»å†…å­˜
```

**2. å¤šGPUè®­ç»ƒ**
```python
# æ•°æ®å¹¶è¡Œ
æ˜¾å­˜éœ€æ±‚ = å•GPUéœ€æ±‚ / GPUæ•°é‡
é€šä¿¡å¼€é”€ = æ¢¯åº¦åŒæ­¥å¸¦å®½

# æ¨¡å‹å¹¶è¡Œ
æ˜¾å­˜éœ€æ±‚ = å•GPUéœ€æ±‚ / GPUæ•°é‡
é€šä¿¡å¼€é”€ = æ¿€æ´»åŒæ­¥å¸¦å®½
```

#### **æ¨ç†èµ„æºè®¡ç®—**

```python
# KVç¼“å­˜å†…å­˜
KV_cache = batch_size Ã— seq_len Ã— layers Ã— hidden_size Ã— 2 bytes

# ç¤ºä¾‹: 7Bæ¨¡å‹ï¼Œ2048ä¸Šä¸‹æ–‡ï¼Œbatch=1
KV_cache = 1 Ã— 2048 Ã— 32 Ã— 4096 Ã— 2 = 536MB (æ¯å±‚)
æ€»KV_cache = 536MB Ã— 32 = 17GB
```

### ğŸ“ˆ æ€§èƒ½åŸºå‡†æµ‹è¯•

#### **å…³é”®æŒ‡æ ‡**
1. **ååé‡** (Throughput)
   - è®­ç»ƒ: tokens/second æˆ– samples/second
   - æ¨ç†: tokens/second æˆ– requests/second

2. **å»¶è¿Ÿ** (Latency)
   - é¦–tokenå»¶è¿Ÿ (Time to First Token)
   - ç”Ÿæˆå»¶è¿Ÿ (Time per Output Token)

3. **èµ„æºåˆ©ç”¨ç‡**
   - GPUåˆ©ç”¨ç‡: `å®é™…FLOP / ç†è®ºFLOP`
   - å†…å­˜åˆ©ç”¨ç‡: `ä½¿ç”¨å†…å­˜ / æ€»å†…å­˜`

4. **æ•ˆç‡æŒ‡æ ‡**
   - FLOPåˆ©ç”¨ç‡ (FLOP Utilization)
   - å†…å­˜å¸¦å®½åˆ©ç”¨ç‡
   - èƒ½æ•ˆ (Tokens per Watt)

---

## ğŸ”„ æ„å»ºå®Œæ•´è®­ç»ƒå¾ªç¯

### ğŸ—ï¸ è®­ç»ƒå¾ªç¯æ¶æ„

#### **æ ¸å¿ƒç»„ä»¶**
```python
class TrainingLoop:
    def __init__(self, model, optimizer, scheduler, dataloader):
        self.model = model
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.dataloader = dataloader

    def train_epoch(self):
        for batch in self.dataloader:
            # å‰å‘ä¼ æ’­
            loss = self.forward_pass(batch)

            # åå‘ä¼ æ’­
            loss.backward()

            # å‚æ•°æ›´æ–°
            self.optimizer.step()
            self.optimizer.zero_grad()

            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()
```

#### **é«˜çº§ç‰¹æ€§**

**1. æ¢¯åº¦ç´¯ç§¯**
```python
accumulation_steps = 4
for i, batch in enumerate(dataloader):
    loss = model(batch) / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

**2. æ··åˆç²¾åº¦è®­ç»ƒ**
```python
scaler = torch.cuda.amp.GradScaler()

with torch.cuda.amp.autocast():
    loss = model(batch)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

**3. æ¢¯åº¦è£å‰ª**
```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

### ğŸ“Š ç›‘æ§å’Œæ—¥å¿—

#### **è®­ç»ƒæŒ‡æ ‡è¿½è¸ª**
```python
# å…³é”®æŒ‡æ ‡
metrics = {
    'train_loss': [],
    'val_loss': [],
    'learning_rate': [],
    'gpu_memory': [],
    'throughput': [],
    'flops_utilization': []
}

# å®æ—¶ç›‘æ§
import wandb
wandb.init(project="model-training")
wandb.watch(model, log="all")
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### ğŸš€ è®¡ç®—ä¼˜åŒ–

#### **1. ç®—å­èåˆ**
```python
# æ‰‹åŠ¨èåˆ
def fused_layer_norm(x, weight, bias, eps=1e-5):
    mean = x.mean(-1, keepdim=True)
    var = x.var(-1, keepdim=True, unbiased=False)
    x_norm = (x - mean) / torch.sqrt(var + eps)
    return x_norm * weight + bias

# è‡ªåŠ¨ç¼–è¯‘
@torch.jit.script
def optimized_function(x):
    return torch.nn.functional.relu(x)
```

#### **2. å†…å­˜ä¼˜åŒ–**
```python
# æ¢¯åº¦æ£€æŸ¥ç‚¹
from torch.utils.checkpoint import checkpoint

# èŠ‚çœæ¿€æ´»å†…å­˜ï¼Œå¢åŠ è®¡ç®—é‡
output = checkpoint(expensive_function, input)

# in-placeæ“ä½œ
x.add_(y)  # è€Œä¸æ˜¯ x = x + y
```

#### **3. æ•°æ®åŠ è½½ä¼˜åŒ–**
```python
# å¤šè¿›ç¨‹æ•°æ®åŠ è½½
dataloader = DataLoader(
    dataset,
    batch_size=32,
    num_workers=8,
    pin_memory=True,
    persistent_workers=True
)

# é¢„å–å’Œç¼“å­˜
prefetch_factor = 2
```

### ğŸ§  æ¨¡å‹ä¼˜åŒ–

#### **1. é‡åŒ–**
```python
# åŠ¨æ€é‡åŒ–
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

# é™æ€é‡åŒ–
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
torch.quantization.prepare(model, inplace=True)
torch.quantization.convert(model, inplace=True)
```

#### **2. å‰ªæ**
```python
# ç»“æ„åŒ–å‰ªæ
import torch.nn.utils.prune as prune

prune.l1_unstructured(module, name='weight', amount=0.2)
prune.remove(module, 'weight')
```

#### **3. çŸ¥è¯†è’¸é¦**
```python
def distillation_loss(student_out, teacher_out, labels, temperature=4.0, alpha=0.7):
    # è½¯æ ‡ç­¾æŸå¤±
    soft_loss = F.kl_div(
        F.log_softmax(student_out/temperature, dim=1),
        F.softmax(teacher_out/temperature, dim=1),
        reduction='batchmean'
    )

    # ç¡¬æ ‡ç­¾æŸå¤±
    hard_loss = F.cross_entropy(student_out, labels)

    return alpha * soft_loss + (1-alpha) * hard_loss
```

### ğŸ“ˆ ç³»ç»Ÿçº§ä¼˜åŒ–

#### **1. åˆ†å¸ƒå¼è®­ç»ƒ**
```python
# DDP (DistributedDataParallel)
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

dist.init_process_group(backend='nccl')
model = DDP(model, device_ids=[local_rank])
```

#### **2. å†…å­˜æ˜ å°„**
```python
# å¤§æ•°æ®é›†å†…å­˜æ˜ å°„
import mmap
import numpy as np

class MemoryMappedDataset:
    def __init__(self, filename):
        self.data = np.memmap(filename, dtype='float32', mode='r')
```

#### **3. å¼‚æ­¥æ‰§è¡Œ**
```python
# CUDAæµ
stream = torch.cuda.Stream()
with torch.cuda.stream(stream):
    async_operation()

# åŒæ­¥ç­‰å¾…
torch.cuda.synchronize()
```

---

## ğŸ’¡ æ ¸å¿ƒæ´å¯Ÿä¸æ€è€ƒ

### ğŸ¯ èµ„æºæ•ˆç‡çš„æœ¬è´¨

**1. è®¡ç®—ä¸å†…å­˜çš„æƒè¡¡**
- æ—¶é—´-ç©ºé—´æƒè¡¡ï¼šæ›´å¤šå†…å­˜æ¢å–æ›´å°‘è®¡ç®—
- ç®—æ³•å¤æ‚åº¦ vs å®é™…æ€§èƒ½ï¼šç†è®ºåˆ†æ vs ç¡¬ä»¶ç‰¹æ€§

**2. æ‰¹å¤„ç†çš„è‰ºæœ¯**
- å¤§æ‰¹é‡ï¼šé«˜ååé‡ï¼Œé«˜å†…å­˜éœ€æ±‚
- å°æ‰¹é‡ï¼šä½å»¶è¿Ÿï¼Œä½å†…å­˜æ•ˆç‡
- åŠ¨æ€æ‰¹å¤„ç†ï¼šæ ¹æ®è´Ÿè½½è‡ªé€‚åº”

**3. å¹¶è¡ŒåŒ–çš„ç»´åº¦**
- æ•°æ®å¹¶è¡Œï¼šç®€å•æœ‰æ•ˆï¼Œå—é™äºæ¨¡å‹å¤§å°
- æ¨¡å‹å¹¶è¡Œï¼šå¤æ‚ä½†å¯æ‰©å±•ï¼Œé€šä¿¡å¼€é”€å¤§
- æµæ°´çº¿å¹¶è¡Œï¼šå¹³è¡¡è´Ÿè½½ï¼Œå¤„ç†æ°”æ³¡é—®é¢˜

### ğŸ¤” è®¾è®¡å“²å­¦

**"Premature optimization is the root of all evil"**
- å…ˆæ­£ç¡®å®ç°ï¼Œå†ä¼˜åŒ–æ€§èƒ½
- åŸºäºprofilingæ•°æ®åšä¼˜åŒ–å†³ç­–
- æƒè¡¡å¼€å‘æˆæœ¬ vs æ€§èƒ½æ”¶ç›Š

**"Measure, don't guess"**
- ä½¿ç”¨æ€§èƒ½åˆ†æå·¥å…·
- å»ºç«‹åŸºå‡†æµ‹è¯•æ¡†æ¶
- è¿½è¸ªå…³é”®æŒ‡æ ‡å˜åŒ–

### ğŸ”® æœªæ¥è¶‹åŠ¿

**1. ç¡¬ä»¶ååŒè®¾è®¡**
- ä¸“ç”¨AIèŠ¯ç‰‡çš„å‘å±•
- å†…å­˜è®¡ç®—æŠ€æœ¯çš„å…´èµ·
- å…‰è®¡ç®—å’Œé‡å­è®¡ç®—çš„æ¢ç´¢

**2. è½¯ä»¶æ ˆä¼˜åŒ–**
- ç¼–è¯‘å™¨æŠ€æœ¯çš„è¿›æ­¥
- è‡ªåŠ¨æ€§èƒ½è°ƒä¼˜
- ç«¯åˆ°ç«¯ä¼˜åŒ–æ¡†æ¶

**3. ç®—æ³•åˆ›æ–°**
- æ›´é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶
- ç¨€ç–åŒ–æŠ€æœ¯
- åŠ¨æ€è®¡ç®—å›¾

---

## ğŸ“ å­¦ä¹ è¦ç‚¹æ€»ç»“

### âœ… æŒæ¡çš„æ ¸å¿ƒæ¦‚å¿µ

1. **å†…å­˜å±‚æ¬¡ç»“æ„**å’Œå…¶å¯¹æ·±åº¦å­¦ä¹ æ€§èƒ½çš„å½±å“
2. **FLOPè®¡ç®—**æ–¹æ³•å’Œå®é™…æ¨¡å‹åˆ†æ
3. **èµ„æºè´¦åŠ¡**åœ¨é¡¹ç›®è§„åˆ’ä¸­çš„åº”ç”¨
4. **è®­ç»ƒå¾ªç¯**çš„å®Œæ•´å®ç°å’Œä¼˜åŒ–
5. **æ€§èƒ½ä¼˜åŒ–**çš„å„ç§æŠ€å·§å’Œæƒè¡¡

### ğŸ¯ å®è·µæŠ€èƒ½

1. èƒ½å¤Ÿ**ç²¾ç¡®è®¡ç®—**æ¨¡å‹çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚
2. æŒæ¡**æ€§èƒ½åˆ†æ**å·¥å…·çš„ä½¿ç”¨
3. å­¦ä¼š**æ„å»ºé«˜æ•ˆ**çš„è®­ç»ƒå¾ªç¯
4. ç†è§£**ä¼˜åŒ–æŠ€å·§**çš„é€‚ç”¨åœºæ™¯

### ğŸ”¬ æ€ç»´æ–¹å¼

1. **ç³»ç»Ÿæ€ç»´**ï¼šä»æ•´ä½“è§’åº¦è€ƒè™‘æ€§èƒ½ä¼˜åŒ–
2. **æ•°æ®é©±åŠ¨**ï¼šåŸºäºæµ‹é‡ç»“æœåšå†³ç­–
3. **æƒè¡¡æ„è¯†**ï¼šç†è§£ä¸åŒä¼˜åŒ–ç­–ç•¥çš„åˆ©å¼Š
4. **å·¥ç¨‹æ€ç»´**ï¼šåœ¨çº¦æŸæ¡ä»¶ä¸‹å¯»æ‰¾æœ€ä¼˜è§£

---

**ğŸ“ å¤‡æ³¨**: è¿™æ˜¯æ·±åº¦å­¦ä¹ ç³»ç»Ÿä¼˜åŒ–çš„åŸºç¡€ï¼Œåç»­çš„GPUæ¶æ„ã€åˆ†å¸ƒå¼è®­ç»ƒç­‰å†…å®¹éƒ½å»ºç«‹åœ¨è¿™äº›æ¦‚å¿µä¹‹ä¸Šã€‚