# Lecture 04: Mixture of Experts (MoE) - å­¦ä¹ æŒ‡å—

## ğŸ“š è¯¾ç¨‹æ¦‚è§ˆ

**è¯¾ç¨‹**: Stanford CS336 Spring 2025 - Lecture 04
**ä¸»é¢˜**: Mixture of Experts (MoE) æ¨¡å‹
**æ ¸å¿ƒä»·å€¼**: ç†è§£å¦‚ä½•é€šè¿‡ç¨€ç–æ¿€æ´»å®ç°é«˜æ•ˆçš„æ¨¡å‹æ‰©å±•
**å…ˆä¿®çŸ¥è¯†**: Lecture 03 (Transformeræ¶æ„)

---

## ğŸ¯ ä¸ºä»€ä¹ˆå­¦ä¹ MoEï¼Ÿ

### MoEè§£å†³çš„æ ¸å¿ƒé—®é¢˜

**Denseæ¨¡å‹çš„å›°å¢ƒ**:
```python
# Dense FFN: å‚æ•°é‡ = è®¡ç®—é‡
parameters = 2 Ã— d_model Ã— d_ff
compute = parameters  # æ‰€æœ‰å‚æ•°éƒ½è¦è®¡ç®—

# æƒ³è¦10å€å®¹é‡ï¼Ÿâ†’ éœ€è¦10å€è®¡ç®—ï¼
```

**MoEçš„çªç ´**:
```python
# MoE: å‚æ•°é‡ >> æ¿€æ´»å‚æ•°
total_parameters = num_experts Ã— parameters_per_expert
active_parameters = k Ã— parameters_per_expert  # k << num_experts

# 100å€å‚æ•°ï¼Œåªéœ€2å€è®¡ç®—ï¼
```

### ç°å®æ„ä¹‰

**æˆåŠŸæ¡ˆä¾‹**:
- **Switch Transformer**: 1.6Tå‚æ•°ï¼Œè®­ç»ƒ/æ¨ç†æ•ˆç‡æ¥è¿‘Dense
- **GLaM**: 1.2Tå‚æ•°ï¼Œè´¨é‡è¶…è¶ŠGPT-3ï¼Œæˆæœ¬æ›´ä½
- **GPT-4ä¼ é—»**: å¯èƒ½ä½¿ç”¨MoEæ¶æ„

**é€‚ç”¨åœºæ™¯**:
- âœ… éœ€è¦æå¤§æ¨¡å‹å®¹é‡
- âœ… æ¨ç†æˆæœ¬æ•æ„Ÿ
- âœ… ä»»åŠ¡æœ‰æ˜ç¡®å­é¢†åŸŸ
- âœ… å¤šä»»åŠ¡/å¤šè¯­è¨€åœºæ™¯

---

## ğŸ“– å­¦ä¹ èµ„æ–™

### 1. **00-æ•™å­¦å¤§çº².md** â­ é¦–å…ˆé˜…è¯»
**å†…å®¹ç»“æ„**:
- Part 1: MoEåŸºç¡€æ¦‚å¿µï¼ˆåŠ¨æœºã€ä¸“å®¶ã€é—¨æ§ï¼‰
- Part 2: é—¨æ§æœºåˆ¶ï¼ˆSoftmaxã€Noisy Top-Kã€è´Ÿè½½å‡è¡¡ï¼‰
- Part 3: ç°ä»£æ¶æ„ï¼ˆSwitchã€GLaMã€ST-MoEï¼‰
- Part 4: è®­ç»ƒä¼˜åŒ–ï¼ˆå¹¶è¡Œç­–ç•¥ã€é€šä¿¡ã€æ¨ç†ï¼‰
- Part 5: æ•°å­¦åˆ†æï¼ˆå‚æ•°é‡ã€è®¡ç®—é‡ã€å†…å­˜ï¼‰
- Part 6: å®ç°ä¸å®è·µ

**å­¦ä¹ å»ºè®®**: å…ˆé€šè¯»å»ºç«‹æ¡†æ¶ï¼Œé‡ç‚¹å…³æ³¨æ•°å­¦åˆ†æéƒ¨åˆ†

### 2. **01-æ·±åº¦é—®ç­”.md** â­â­â­ æ ¸å¿ƒå­¦ä¹ 
**24ä¸ªè‹æ ¼æ‹‰åº•å¼é—®é¢˜**:
- **Q1-Q6**: MoEåŸºç¡€ï¼ˆåŠ¨æœºã€ä¸“å®¶ã€é—¨æ§ã€Top-Kã€å‚æ•°é‡ã€è®¡ç®—é‡ï¼‰
- **Q7-Q12**: é—¨æ§æœºåˆ¶ï¼ˆSoftmaxé—®é¢˜ã€Noisy Top-Kã€è´Ÿè½½å‡è¡¡ã€Z-lossï¼‰
- **Q13-Q18**: ç°ä»£æ¶æ„ï¼ˆSwitchã€å¹¶è¡Œã€å¯¹æ¯”åˆ†æï¼‰
- **Q19-Q24**: è®­ç»ƒä¼˜åŒ–ï¼ˆç¨³å®šæ€§ã€é€šä¿¡ã€æ¨ç†ã€é‡åŒ–ã€æœªæ¥ï¼‰
- **4ä¸ªè¿›é˜¶æŒ‘æˆ˜**: è®¾è®¡ã€ç†è®ºã€ç³»ç»Ÿã€å¯è§£é‡Šæ€§

**å­¦ä¹ æ–¹å¼**:
1. æ¯å¤©å®Œæˆ4-6ä¸ªé—®é¢˜
2. ç‹¬ç«‹æ€è€ƒâ†’ç¼–ç¨‹éªŒè¯â†’æ·±åº¦è®¨è®º
3. è®°å½•åˆ°`02-æ·±åº¦è®¨è®ºè®°å½•.md`

### 3. å®˜æ–¹è¯¾ä»¶
**ä½ç½®**: `../../../nonexecutable/2025 Lecture 4 - MoEs.pdf`
**é…åˆä½¿ç”¨**: æŸ¥çœ‹è¯¦ç»†çš„æ¶æ„å›¾å’Œå®éªŒç»“æœ

---

## ğŸ—“ï¸ æ¨è4å¤©å­¦ä¹ è®¡åˆ’

### Day 1: MoEåŸºç¡€ (4-5å°æ—¶)

**ç†è®ºå­¦ä¹ **:
- [ ] é˜…è¯»æ•™å­¦å¤§çº² Part 1-2
- [ ] å›ç­”æ·±åº¦é—®ç­” Q1-Q6
- [ ] ç†è§£å‚æ•°é‡å’Œè®¡ç®—é‡çš„è§£è€¦

**ç¼–ç¨‹å®è·µ**:
- [ ] å®ç°åŸºç¡€Expertç±»
- [ ] å®ç°Softmaxé—¨æ§
- [ ] è®¡ç®—å‚æ•°é‡å’ŒFLOPs

**æ£€æŸ¥ç‚¹**: èƒ½æ¸…æ™°è§£é‡Šä¸ºä»€ä¹ˆéœ€è¦MoEï¼Œèƒ½æ‰‹ç®—å‚æ•°é‡

**ä»£ç ç¤ºä¾‹**:
```python
class Expert(nn.Module):
    def __init__(self, d_model, d_ff):
        super().__init__()
        self.w1 = nn.Linear(d_model, d_ff)
        self.w2 = nn.Linear(d_ff, d_model)

    def forward(self, x):
        return self.w2(F.relu(self.w1(x)))

class SimpleMoE(nn.Module):
    def __init__(self, d_model, d_ff, num_experts, k=2):
        super().__init__()
        self.experts = nn.ModuleList([
            Expert(d_model, d_ff) for _ in range(num_experts)
        ])
        self.gate = nn.Linear(d_model, num_experts)
        self.k = k

    def forward(self, x):
        # é—¨æ§
        gates = F.softmax(self.gate(x), dim=-1)
        top_k_gates, top_k_indices = torch.topk(gates, self.k)

        # å½’ä¸€åŒ–
        top_k_gates = top_k_gates / top_k_gates.sum(dim=-1, keepdim=True)

        # è®¡ç®—è¾“å‡º
        output = torch.zeros_like(x)
        for i in range(self.k):
            expert_id = top_k_indices[:, i]
            gate_value = top_k_gates[:, i].unsqueeze(-1)
            # ç®€åŒ–å®ç°ï¼šå®é™…éœ€è¦batchå¤„ç†
            for idx, exp_id in enumerate(expert_id):
                output[idx] += gate_value[idx] * self.experts[exp_id](x[idx:idx+1])

        return output
```

---

### Day 2: é—¨æ§ä¸è´Ÿè½½å‡è¡¡ (4-5å°æ—¶)

**ç†è®ºå­¦ä¹ **:
- [ ] é˜…è¯»æ•™å­¦å¤§çº² Part 2
- [ ] å›ç­”æ·±åº¦é—®ç­” Q7-Q12
- [ ] ç†è§£è´Ÿè½½å‡è¡¡çš„æ•°å­¦åŸç†

**ç¼–ç¨‹å®è·µ**:
- [ ] å®ç°Noisy Top-Ké—¨æ§
- [ ] å®ç°è¾…åŠ©æŸå¤±
- [ ] å®ç°Expert Capacityæœºåˆ¶
- [ ] å¯è§†åŒ–è´Ÿè½½åˆ†å¸ƒ

**æ£€æŸ¥ç‚¹**: ç†è§£è¾…åŠ©æŸå¤±å¦‚ä½•å·¥ä½œï¼Œèƒ½åˆ†æè´Ÿè½½ä¸å‡è¡¡çš„åŸå› 

**å…³é”®å®ç°**:
```python
def noisy_top_k_gating(x, W_gate, W_noise, k, training=True):
    # åŸºç¡€logits
    logits = x @ W_gate  # [batch, num_experts]

    if training:
        # å¯è®­ç»ƒå™ªå£°
        noise_stddev = F.softplus(x @ W_noise)
        noise = torch.randn_like(logits) * noise_stddev
        logits = logits + noise

    # Softmax
    gates = F.softmax(logits, dim=-1)

    # Top-Ké€‰æ‹©
    top_k_gates, top_k_indices = torch.topk(gates, k, dim=-1)

    # å½’ä¸€åŒ–
    top_k_gates = top_k_gates / top_k_gates.sum(dim=-1, keepdim=True)

    return top_k_gates, top_k_indices, gates

def load_balancing_loss(gates, expert_mask):
    """
    gates: [batch, seq_len, num_experts] - softmaxè¾“å‡º
    expert_mask: [batch, seq_len, num_experts] - top-k mask
    """
    # Importance: å¹³å‡é—¨æ§æƒé‡
    importance = gates.mean(dim=[0, 1])  # [num_experts]

    # Load: è¢«é€‰ä¸­çš„é¢‘ç‡
    load = expert_mask.float().mean(dim=[0, 1])  # [num_experts]

    # è¾…åŠ©æŸå¤±
    loss = (importance * load).sum() * gates.size(-1)  # ä¹˜ä»¥num_experts

    return loss
```

---

### Day 3: ç°ä»£MoEæ¶æ„ (4-5å°æ—¶)

**ç†è®ºå­¦ä¹ **:
- [ ] é˜…è¯»æ•™å­¦å¤§çº² Part 3-4
- [ ] å›ç­”æ·±åº¦é—®ç­” Q13-Q18
- [ ] å¯¹æ¯”Switchã€GLaMã€ST-MoE

**ç¼–ç¨‹å®è·µ**:
- [ ] å®ç°Switch Transformer MoEå±‚ï¼ˆk=1ï¼‰
- [ ] åˆ†æExpert Parallelism
- [ ] å®ç°Expert Capacityé™åˆ¶
- [ ] æ€§èƒ½åˆ†æï¼šMoE vs Dense

**æ£€æŸ¥ç‚¹**: ç†è§£ä¸ºä»€ä¹ˆSwitché€‰æ‹©k=1ï¼Œèƒ½åˆ†æé€šä¿¡å¼€é”€

**Switch Transformeræ ¸å¿ƒ**:
```python
class SwitchFFN(nn.Module):
    """Switch Transformer: k=1, simplified routing"""

    def __init__(self, d_model, d_ff, num_experts, capacity_factor=1.25):
        super().__init__()
        self.router = nn.Linear(d_model, num_experts)
        self.experts = nn.ModuleList([
            Expert(d_model, d_ff) for _ in range(num_experts)
        ])
        self.num_experts = num_experts
        self.capacity_factor = capacity_factor

    def forward(self, x):
        # x: [batch, seq_len, d_model]
        batch, seq_len, d_model = x.shape

        # è·¯ç”±å†³ç­–
        router_logits = self.router(x)  # [batch, seq_len, num_experts]
        router_probs = F.softmax(router_logits, dim=-1)

        # Top-1é€‰æ‹© (Switchçš„å…³é”®)
        expert_gate, expert_index = torch.max(router_probs, dim=-1)
        # expert_gate: [batch, seq_len]
        # expert_index: [batch, seq_len]

        # è®¡ç®—æ¯ä¸ªä¸“å®¶çš„capacity
        tokens_per_expert = (batch * seq_len) / self.num_experts
        capacity = int(tokens_per_expert * self.capacity_factor)

        # Dispatch tokens to experts
        output = torch.zeros_like(x)

        for expert_id in range(self.num_experts):
            # æ‰¾åˆ°è·¯ç”±åˆ°è¿™ä¸ªä¸“å®¶çš„tokens
            expert_mask = (expert_index == expert_id)  # [batch, seq_len]
            tokens_for_expert = x[expert_mask]  # [num_tokens, d_model]

            if tokens_for_expert.size(0) == 0:
                continue

            # Capacityé™åˆ¶
            if tokens_for_expert.size(0) > capacity:
                tokens_for_expert = tokens_for_expert[:capacity]
                expert_mask_limited = expert_mask.clone()
                # æ ‡è®°è¶…å‡ºcapacityçš„tokensï¼ˆå®é™…å®ç°æ›´å¤æ‚ï¼‰

            # ä¸“å®¶å¤„ç†
            expert_output = self.experts[expert_id](tokens_for_expert)

            # å†™å›output
            output[expert_mask[:tokens_for_expert.size(0)]] = \
                expert_output * expert_gate[expert_mask[:tokens_for_expert.size(0)]].unsqueeze(-1)

        return output
```

---

### Day 4: è®­ç»ƒä¸ä¼˜åŒ– (3-4å°æ—¶)

**ç†è®ºå­¦ä¹ **:
- [ ] é˜…è¯»æ•™å­¦å¤§çº² Part 5-6
- [ ] å›ç­”æ·±åº¦é—®ç­” Q19-Q24
- [ ] ç†è§£åˆ†å¸ƒå¼è®­ç»ƒæŒ‘æˆ˜

**ç¼–ç¨‹å®è·µ**:
- [ ] å®ç°Router Z-loss
- [ ] åˆ†æé€šä¿¡æ¨¡å¼
- [ ] è®¾è®¡æ¨ç†ä¼˜åŒ–ç­–ç•¥
- [ ] å®Œæ•´MoE Transformer Block

**æ£€æŸ¥ç‚¹**: èƒ½åˆ†æè®­ç»ƒä¸ç¨³å®šçš„åŸå› ï¼Œç†è§£é€šä¿¡ç“¶é¢ˆ

**å®Œæ•´MoE Block**:
```python
class MoETransformerBlock(nn.Module):
    """å®Œæ•´çš„MoE Transformer Block"""

    def __init__(self, d_model, num_heads, d_ff, num_experts, k=2):
        super().__init__()
        # Attention (ä¸Denseç›¸åŒ)
        self.attn = MultiHeadAttention(d_model, num_heads)
        self.norm1 = nn.LayerNorm(d_model)

        # MoE FFN (æ›¿æ¢Dense FFN)
        self.moe = MoELayer(d_model, d_ff, num_experts, k)
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, x):
        # Self-Attention
        x = x + self.attn(self.norm1(x))

        # MoE FFN
        moe_output, aux_loss = self.moe(self.norm2(x))
        x = x + moe_output

        return x, aux_loss

class MoELayer(nn.Module):
    """å®Œæ•´çš„MoEå±‚ï¼ŒåŒ…å«æ‰€æœ‰ä¼˜åŒ–"""

    def __init__(self, d_model, d_ff, num_experts, k,
                 capacity_factor=1.25, aux_loss_coef=0.01):
        super().__init__()
        self.num_experts = num_experts
        self.k = k
        self.capacity_factor = capacity_factor
        self.aux_loss_coef = aux_loss_coef

        # ä¸“å®¶
        self.experts = nn.ModuleList([
            Expert(d_model, d_ff) for _ in range(num_experts)
        ])

        # é—¨æ§
        self.gate = nn.Linear(d_model, num_experts)
        self.w_noise = nn.Linear(d_model, num_experts)

    def forward(self, x, training=True):
        # é—¨æ§
        top_k_gates, top_k_indices, all_gates = \
            noisy_top_k_gating(x, self.gate.weight, self.w_noise.weight,
                               self.k, training)

        # è´Ÿè½½å‡è¡¡æŸå¤±
        expert_mask = F.one_hot(top_k_indices, self.num_experts).float()
        aux_loss = load_balancing_loss(all_gates, expert_mask)

        # ä¸“å®¶è®¡ç®—
        output = self._dispatch_and_combine(x, top_k_gates, top_k_indices)

        return output, aux_loss * self.aux_loss_coef

    def _dispatch_and_combine(self, x, gates, indices):
        # é«˜æ•ˆå®ç°ï¼šbatchå¤„ç†æ¯ä¸ªä¸“å®¶
        output = torch.zeros_like(x)

        for expert_id in range(self.num_experts):
            # æ‰¾åˆ°è·¯ç”±åˆ°è¿™ä¸ªä¸“å®¶çš„tokenså’Œå¯¹åº”çš„gates
            expert_mask = (indices == expert_id)
            tokens = x[expert_mask]
            token_gates = gates[expert_mask]

            if tokens.size(0) > 0:
                expert_output = self.experts[expert_id](tokens)
                output[expert_mask] = expert_output * token_gates.unsqueeze(-1)

        return output
```

---

## ğŸ§  æ ¸å¿ƒæ¦‚å¿µé€ŸæŸ¥

### MoEä¸‰è¦ç´ 
```python
MoE_Components = {
    '1. Experts': 'å¤šä¸ªå¹¶è¡Œçš„FFNï¼Œå„è‡ªä¸“ä¸šåŒ–',
    '2. Router/Gate': 'å†³å®šæ¿€æ´»å“ªäº›ä¸“å®¶',
    '3. Top-K Selection': 'ç¨€ç–æ¿€æ´»ï¼Œåªç”¨kä¸ªä¸“å®¶'
}
```

### å…³é”®å…¬å¼
```
# MoEè¾“å‡º
y = Î£ G(x)_i Â· E_i(x)  for i in top_k

# è¾…åŠ©æŸå¤±
L_aux = Î± Â· Î£ (importance_i Ã— load_i)

# Router Z-loss
L_z = (log Î£ exp(logits))Â²
```

### å‚æ•°é‡ vs è®¡ç®—é‡
```python
å¯¹æ¯”åˆ†æ = {
    'Dense FFN': {
        'å‚æ•°': '2 Ã— d Ã— d_ff',
        'è®¡ç®—': '2 Ã— d Ã— d_ff FLOPs/token'
    },
    'MoE FFN': {
        'å‚æ•°': 'E Ã— 2 Ã— d Ã— d_ff  (Eå€!)',
        'è®¡ç®—': 'k Ã— 2 Ã— d Ã— d_ff  (çº¦kå€)',
        'å…³é”®': 'E=128, k=2 â†’ 128å€å‚æ•°, 2å€è®¡ç®—'
    }
}
```

---

## ğŸ’¡ å­¦ä¹ æŠ€å·§

### ç†è§£MoEçš„3ä¸ªè§†è§’

**1. æ¡ä»¶è®¡ç®—è§†è§’**:
- ä¸æ˜¯æ‰€æœ‰å‚æ•°éƒ½éœ€è¦æ¿€æ´»
- æ ¹æ®è¾“å…¥é€‰æ‹©æ€§è®¡ç®—
- ç±»æ¯”ï¼šCPUçš„åˆ†æ”¯é¢„æµ‹

**2. é›†æˆå­¦ä¹ è§†è§’**:
- å¤šä¸ªä¸“å®¶çš„ensemble
- æ¯ä¸ªä¸“å®¶ä¸“æ³¨å­é—®é¢˜
- é—¨æ§ç½‘ç»œå­¦ä¹ å¦‚ä½•ç»„åˆ

**3. ç³»ç»Ÿä¼˜åŒ–è§†è§’**:
- å‚æ•°åˆ†å¸ƒå¼å­˜å‚¨
- è®¡ç®—å±€éƒ¨åŒ–
- é€šä¿¡æœ€å°åŒ–

### å¸¸è§è¯¯åŒº

âŒ **è¯¯åŒº1**: "MoEå°±æ˜¯å¤šä¸ªæ¨¡å‹çš„ensemble"
âœ… **æ­£ç¡®**: MoEæ˜¯å•ä¸ªæ¨¡å‹ï¼Œä¸“å®¶å…±äº«æ¢¯åº¦æ›´æ–°ï¼Œç«¯åˆ°ç«¯è®­ç»ƒ

âŒ **è¯¯åŒº2**: "kä¸ªä¸“å®¶ = kå€è®¡ç®—é‡"
âœ… **æ­£ç¡®**: Routerä¹Ÿæœ‰è®¡ç®—ï¼Œä½†ç›¸å¯¹ä¸“å®¶å¾ˆå°ï¼›å…³é”®æ˜¯k << E

âŒ **è¯¯åŒº3**: "è´Ÿè½½å‡è¡¡ä¸é‡è¦"
âœ… **æ­£ç¡®**: è´Ÿè½½ä¸å‡è¡¡ä¼šå¯¼è‡´ä¸“å®¶é€€åŒ–ï¼Œè®­ç»ƒå¤±è´¥

âŒ **è¯¯åŒº4**: "MoEæ€»æ˜¯æ¯”Denseå¥½"
âœ… **æ­£ç¡®**: MoEæœ‰è®­ç»ƒå¤æ‚åº¦ã€é€šä¿¡å¼€é”€ç­‰trade-off

### è°ƒè¯•æŠ€å·§

**é—®é¢˜**: æ‰€æœ‰tokensè·¯ç”±åˆ°å°‘æ•°ä¸“å®¶
**è¯Šæ–­**:
```python
# æ£€æŸ¥é—¨æ§åˆ†å¸ƒ
gate_probs = F.softmax(router(x), dim=-1)
expert_usage = (gate_probs > threshold).sum(dim=0)
print(f"Experts usage: {expert_usage}")  # åº”è¯¥ç›¸å¯¹å‡åŒ€
```
**è§£å†³**: å¢å¤§aux_lossç³»æ•°ï¼Œæ£€æŸ¥åˆå§‹åŒ–

**é—®é¢˜**: Losséœ‡è¡
**è¯Šæ–­**: è§‚å¯Ÿrouter logitsçš„èŒƒå›´
**è§£å†³**: æ·»åŠ Router Z-lossï¼Œé™ä½å­¦ä¹ ç‡

**é—®é¢˜**: é€šä¿¡æˆä¸ºç“¶é¢ˆ
**è¯Šæ–­**: Profile All-to-Allæ—¶é—´
**è§£å†³**: å‡å°‘ä¸“å®¶æ•°ï¼Œæˆ–ç”¨Expert Parallelism

---

## ğŸ”— ä¸å…¶ä»–Lectureçš„è”ç³»

**â† Lecture 03 (Transformer)**:
- MoEæ›¿æ¢FFNï¼ŒAttentionä¸å˜
- æ®‹å·®è¿æ¥å’ŒLayerNormä¿æŒ
- æ¶æ„çš„æ¨¡å—åŒ–è®¾è®¡

**â†’ Lecture 06 (GPU Kernels)**:
- Expert batchingçš„kernelä¼˜åŒ–
- All-to-Allé€šä¿¡å®ç°
- å†…å­˜ç®¡ç†å’ŒTiling

**â†’ Lecture 10 (Inference)**:
- Expert offloadingç­–ç•¥
- KV cacheä¸MoEçš„äº¤äº’
- æ‰¹å¤„ç†ä¼˜åŒ–

**â†’ Lecture 12 (Serving)**:
- åˆ†å¸ƒå¼æ¨ç†ç³»ç»Ÿ
- è´Ÿè½½å‡è¡¡ä¸è·¯ç”±
- å¼¹æ€§æ‰©å±•

---

## ğŸ“Š å­¦ä¹ æˆæœæ£€éªŒ

### ç†è®ºæµ‹è¯• âœ…

1. **å‚æ•°é‡è®¡ç®—** (5åˆ†é’Ÿ):
   - Dense: d=4096, d_ff=16384
   - MoE: E=128, k=2
   - è®¡ç®—å‚æ•°é‡æ¯”ä¾‹

2. **å£å¤´è§£é‡Š** (æ¯ä¸ª2-3åˆ†é’Ÿ):
   - ä¸ºä»€ä¹ˆéœ€è¦MoEï¼Ÿ
   - è¾…åŠ©æŸå¤±å¦‚ä½•å·¥ä½œï¼Ÿ
   - Switch Transformerçš„k=1è®¾è®¡
   - Expert Parallelismçš„é€šä¿¡æ¨¡å¼

3. **æ¶æ„å¯¹æ¯”** (5åˆ†é’Ÿ):
   - Switch vs GLaM vs ST-MoE
   - å„è‡ªä¼˜åŠ£å’Œé€‚ç”¨åœºæ™¯

### ç¼–ç¨‹æµ‹è¯• ğŸ’»

```python
# 1. å®ç°æ ¸å¿ƒç»„ä»¶
class Expert(nn.Module):
    def __init__(self, d_model, d_ff):
        # ä½ çš„å®ç°
        pass

class MoELayer(nn.Module):
    def __init__(self, d_model, d_ff, num_experts, k):
        # ä½ çš„å®ç°
        pass

# 2. è®¡ç®—è´Ÿè½½å‡è¡¡
def load_balancing_loss(gates, expert_mask):
    # ä½ çš„å®ç°
    pass

# 3. å®Œæ•´æµ‹è¯•
x = torch.randn(2, 10, 512)  # [batch, seq, d_model]
moe = MoELayer(512, 2048, num_experts=8, k=2)
output, aux_loss = moe(x)

assert output.shape == x.shape
print(f"Auxiliary loss: {aux_loss.item()}")
```

### ç³»ç»Ÿæ€ç»´æµ‹è¯• ğŸ§ 

1. **æˆæœ¬åˆ†æ**: MoEè®­ç»ƒçš„ä¸»è¦æˆæœ¬åœ¨å“ªé‡Œï¼Ÿ
2. **ç“¶é¢ˆè¯†åˆ«**: ä»€ä¹ˆæƒ…å†µä¸‹é€šä¿¡æˆä¸ºç“¶é¢ˆï¼Ÿ
3. **ä¼˜åŒ–å†³ç­–**: å¦‚ä½•é€‰æ‹©ä¸“å®¶æ•°é‡å’Œkå€¼ï¼Ÿ
4. **æ¶æ„æƒè¡¡**: ä½•æ—¶é€‰MoEï¼Œä½•æ—¶é€‰Denseï¼Ÿ

---

## ğŸ¯ ä¸‹ä¸€æ­¥

å®ŒæˆLecture 04åï¼Œä½ åº”è¯¥ï¼š

âœ… **æŒæ¡**: MoEçš„æ ¸å¿ƒåŸç†å’Œæ•°å­¦
âœ… **ç†è§£**: è´Ÿè½½å‡è¡¡å’Œè®­ç»ƒæŒ‘æˆ˜
âœ… **èƒ½å¤Ÿ**: å®ç°åŸºç¡€MoEå±‚
âœ… **å…·å¤‡**: åˆ†æå’Œä¼˜åŒ–MoEçš„æ€ç»´

**å‡†å¤‡å¥½è¿›å…¥**:
- **Lecture 05**: Data & Training (å¦‚ä½•è®­ç»ƒå¤§è§„æ¨¡MoE)
- **Lecture 06**: GPU Kernels (MoEçš„åº•å±‚ä¼˜åŒ–)
- **Lecture 10**: Inference (MoEæ¨ç†ç³»ç»Ÿ)

---

## ğŸ†˜ è·å–å¸®åŠ©

**é‡åˆ°é—®é¢˜ï¼Ÿ**

1. **é‡è¯»æ•™å­¦å¤§çº²**: æŸ¥æ‰¾ç›¸å…³section
2. **æŸ¥çœ‹æ·±åº¦é—®ç­”**: å¼•å¯¼æ€§é—®é¢˜æç¤º
3. **å‚è€ƒå®˜æ–¹è¯¾ä»¶**: PDFä¸­çš„æ¶æ„å›¾
4. **æŸ¥çœ‹è®ºæ–‡**: Switch Transformer, GLaMåŸè®ºæ–‡
5. **å®éªŒä»£ç **: åŠ¨æ‰‹éªŒè¯ç†è§£

**è®¨è®ºæ¸ é“**:
- ä½¿ç”¨æ·±åº¦è®¨è®ºè®°å½•åŠŸèƒ½
- ä¸AIåŠ©æ‰‹è¿›è¡Œè‹æ ¼æ‹‰åº•å¼å¯¹è¯

---

## ğŸ“š å‚è€ƒèµ„æ–™

### å¿…è¯»è®ºæ–‡

1. **Shazeer et al. 2017**: [Outrageously Large Neural Networks](https://arxiv.org/abs/1701.06538)
   - åŸå§‹MoEè®ºæ–‡
   - Noisy Top-K gating
   - è´Ÿè½½å‡è¡¡æœºåˆ¶

2. **Fedus et al. 2021**: [Switch Transformers](https://arxiv.org/abs/2101.03961)
   - k=1è®¾è®¡
   - 1.6Tå‚æ•°æ‰©å±•
   - è®­ç»ƒç¨³å®šæ€§æŠ€å·§

3. **Du et al. 2021**: [GLaM](https://arxiv.org/abs/2112.06905)
   - 1.2T Decoder-only
   - æ•ˆç‡åˆ†æ
   - ä¸GPT-3å¯¹æ¯”

4. **Zoph et al. 2022**: [ST-MoE](https://arxiv.org/abs/2202.08906)
   - ç¨³å®šæ€§æ”¹è¿›
   - æ³›åŒ–æ€§èƒ½
   - æœ€ä½³å®è·µ

### ä»£ç å®ç°

- **Hugging Face**: `transformers` åº“çš„Switch Transformer
- **Fairseq**: Metaçš„MoEå®ç°
- **Megatron-LM**: NVIDIAçš„åˆ†å¸ƒå¼MoE
- **DeepSpeed**: Microsoftçš„MoEä¼˜åŒ–

---

**åˆ›å»ºæ—¥æœŸ**: 2025-01-12
**ç»´æŠ¤**: éšå­¦ä¹ è¿›åº¦æ›´æ–°
**çŠ¶æ€**: âœ… å®Œæ•´å­¦ä¹ æ¡†æ¶å·²å°±ç»ª

ğŸš€ **å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹MoEçš„æ·±åº¦å­¦ä¹ ä¹‹æ—…ï¼**
