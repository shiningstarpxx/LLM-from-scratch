# Lecture 04: Mixture of Experts (MoE) - æ•™å­¦å¤§çº²

## ğŸ“‹ è¯¾ç¨‹ä¿¡æ¯

**è¯¾ç¨‹**: Stanford CS336 Spring 2025 - Lecture 04
**ä¸»é¢˜**: Mixture of Experts (MoE) æ¨¡å‹
**æ ¸å¿ƒé—®é¢˜**: å¦‚ä½•é€šè¿‡ç¨€ç–æ¿€æ´»å®ç°æ¨¡å‹æ‰©å±•ï¼Ÿ
**å…ˆä¿®çŸ¥è¯†**: Lecture 03 (Transformeræ¶æ„)

---

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬è¯¾ç¨‹åï¼Œä½ å°†èƒ½å¤Ÿï¼š

1. âœ… ç†è§£MoEçš„æ ¸å¿ƒåŠ¨æœºå’Œè®¾è®¡å“²å­¦
2. âœ… æŒæ¡ä¸“å®¶è·¯ç”±æœºåˆ¶ï¼ˆExpert Routingï¼‰
3. âœ… åˆ†æMoEçš„å‚æ•°æ•ˆç‡ vs è®¡ç®—æ•ˆç‡
4. âœ… ç†è§£è´Ÿè½½å‡è¡¡æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆ
5. âœ… å¯¹æ¯”ç°ä»£MoEå˜ä½“ï¼ˆSwitchã€GLaMã€ST-MoEç­‰ï¼‰
6. âœ… å®ç°åŸºç¡€MoEå±‚

---

## ğŸ“š è¯¾ç¨‹å¤§çº²

### Part 1: MoEåŸºç¡€æ¦‚å¿µ (30åˆ†é’Ÿ)

#### 1.1 ä¸ºä»€ä¹ˆéœ€è¦MoEï¼Ÿ
**æ ¸å¿ƒåŠ¨æœº**:
- Denseæ¨¡å‹çš„æ‰©å±•ç“¶é¢ˆ
- å‚æ•°é‡ vs è®¡ç®—é‡çš„è§£è€¦
- æ¡ä»¶è®¡ç®—ï¼ˆConditional Computationï¼‰

**å…³é”®æ´å¯Ÿ**:
```python
# Dense FFN: æ‰€æœ‰å‚æ•°éƒ½å‚ä¸è®¡ç®—
output = FFN(input)  # ä½¿ç”¨100%å‚æ•°

# MoE: åªæ¿€æ´»éƒ¨åˆ†ä¸“å®¶
output = top_k_experts(input)  # ä½¿ç”¨10-20%å‚æ•°
```

#### 1.2 MoEçš„æ ¸å¿ƒç»„ä»¶
1. **ä¸“å®¶ç½‘ç»œ** (Experts): å¤šä¸ªå¹¶è¡Œçš„FFN
2. **é—¨æ§ç½‘ç»œ** (Gating/Router): å†³å®šæ¿€æ´»å“ªäº›ä¸“å®¶
3. **Top-Ké€‰æ‹©**: ç¨€ç–æ¿€æ´»æœºåˆ¶

**æ•°å­¦å®šä¹‰**:
```
y = Î£ G(x)_i Â· E_i(x)
å…¶ä¸­:
- G(x): é—¨æ§å‡½æ•°ï¼Œè¾“å‡ºæ¯ä¸ªä¸“å®¶çš„æƒé‡
- E_i(x): ç¬¬iä¸ªä¸“å®¶çš„è¾“å‡º
- åªæœ‰top-kä¸ªG(x)_iéé›¶
```

---

### Part 2: é—¨æ§æœºåˆ¶ (45åˆ†é’Ÿ)

#### 2.1 Softmaxé—¨æ§
**æœ€ç®€å•çš„é—¨æ§**:
```python
# è®¡ç®—é—¨æ§logits
logits = W_g @ x  # [num_experts]

# Softmaxå¾—åˆ°æƒé‡
gates = softmax(logits)  # [num_experts]

# Top-ké€‰æ‹©
top_k_gates, top_k_indices = topk(gates, k)

# å½’ä¸€åŒ–
top_k_gates = top_k_gates / sum(top_k_gates)
```

**é—®é¢˜**: è´Ÿè½½ä¸å‡è¡¡

#### 2.2 Noisy Top-Ké—¨æ§
**æ·»åŠ å™ªå£°ä¿ƒè¿›æ¢ç´¢**:
```python
# æ·»åŠ å¯è®­ç»ƒå™ªå£°
logits = W_g @ x
noise = StandardNormal() * softplus(W_noise @ x)
noisy_logits = logits + noise

# Top-ké€‰æ‹©
gates = softmax(noisy_logits)
top_k_gates, top_k_indices = topk(gates, k)
```

**ä¼˜åŠ¿**:
- è®­ç»ƒæ—¶æ¢ç´¢ä¸åŒä¸“å®¶
- æ¨ç†æ—¶å¯ä»¥å»æ‰å™ªå£°

#### 2.3 è´Ÿè½½å‡è¡¡
**æ ¸å¿ƒæŒ‘æˆ˜**: é˜²æ­¢æ‰€æœ‰tokenè·¯ç”±åˆ°å°‘æ•°ä¸“å®¶

**è§£å†³æ–¹æ¡ˆ1: è¾…åŠ©æŸå¤±**
```python
# é‡è¦æ€§: æ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„é¢‘ç‡
importance = mean(gates, over_tokens)

# è´Ÿè½½: æ¯ä¸ªä¸“å®¶å¤„ç†çš„tokenæ•°
load = sum(top_k_mask, over_tokens)

# è¾…åŠ©æŸå¤±: é¼“åŠ±å‡åŒ€åˆ†å¸ƒ
aux_loss = coefficient * sum(importance * load)
```

**è§£å†³æ–¹æ¡ˆ2: Expert Capacity**
```python
# é™åˆ¶æ¯ä¸ªä¸“å®¶æœ€å¤šå¤„ç†çš„tokenæ•°
capacity = (tokens_per_batch / num_experts) * capacity_factor

# è¶…è¿‡å®¹é‡çš„tokenè¢«ä¸¢å¼ƒæˆ–è·¯ç”±åˆ°å…¶ä»–ä¸“å®¶
```

---

### Part 3: ç°ä»£MoEæ¶æ„ (45åˆ†é’Ÿ)

#### 3.1 Switch Transformer (Google, 2021)
**å…³é”®åˆ›æ–°**: k=1ï¼Œæ¯ä¸ªtokenåªè·¯ç”±åˆ°1ä¸ªä¸“å®¶

**ä¼˜åŠ¿**:
- ç®€åŒ–è·¯ç”±é€»è¾‘
- æ›´å¥½çš„è®­ç»ƒç¨³å®šæ€§
- å¤§è§„æ¨¡æ‰©å±•ï¼ˆ1.6Tå‚æ•°ï¼‰

**æ¶æ„**:
```python
class SwitchFFN(nn.Module):
    def __init__(self, d_model, num_experts, expert_capacity):
        self.router = nn.Linear(d_model, num_experts)
        self.experts = nn.ModuleList([
            FFN(d_model) for _ in range(num_experts)
        ])
        self.capacity = expert_capacity

    def forward(self, x):
        # x: [batch, seq_len, d_model]

        # è·¯ç”±å†³ç­–
        router_logits = self.router(x)  # [batch, seq_len, num_experts]
        router_probs = softmax(router_logits, dim=-1)

        # Top-1é€‰æ‹©
        expert_gate, expert_index = torch.max(router_probs, dim=-1)

        # æ¯ä¸ªä¸“å®¶å¤„ç†å¯¹åº”çš„tokens
        # ï¼ˆå®ç°ç»†èŠ‚æ¶‰åŠcapacityé™åˆ¶å’Œbatchè°ƒåº¦ï¼‰
```

**é‡è¦å‚æ•°**:
- `num_experts`: é€šå¸¸ä¸º128-256
- `capacity_factor`: 1.0-1.5
- `expert_size`: ä¸dense FFNç›¸åŒ

#### 3.2 GLaM (Google, 2021)
**è§„æ¨¡**: 1.2Tå‚æ•°ï¼ŒDecoder-only

**åˆ›æ–°**:
- æ¯å±‚æ¿€æ´»top-2ä¸“å®¶
- 64ä¸ªä¸“å®¶/å±‚
- Expert capacityç®¡ç†

#### 3.3 ST-MoE (Google, 2022)
**Sparse Token MoE**

**å…³é”®æ”¹è¿›**:
- Token-levelè·¯ç”±
- æ›´ç»†ç²’åº¦çš„ä¸“å®¶åˆ†é…
- æ”¹è¿›çš„è´Ÿè½½å‡è¡¡

---

### Part 4: è®­ç»ƒä¸æ¨ç†ä¼˜åŒ– (30åˆ†é’Ÿ)

#### 4.1 è®­ç»ƒæŒ‘æˆ˜

**1. é€šä¿¡å¼€é”€**
```python
é€šä¿¡æ¨¡å¼ = {
    'é—®é¢˜': 'ä¸“å®¶åˆ†å¸ƒåœ¨ä¸åŒGPU/èŠ‚ç‚¹',
    'All-to-All': 'æ¯ä¸ªtokenå¯èƒ½éœ€è¦å‘é€åˆ°ä»»æ„ä¸“å®¶',
    'ç“¶é¢ˆ': 'è·¨èŠ‚ç‚¹é€šä¿¡å»¶è¿Ÿ'
}
```

**è§£å†³æ–¹æ¡ˆ**:
- Expert Parallelism: ä¸åŒä¸“å®¶æ”¾åœ¨ä¸åŒè®¾å¤‡
- Data Parallelism: æ¯ä¸ªè®¾å¤‡æœ‰å®Œæ•´ä¸“å®¶å‰¯æœ¬
- æ··åˆå¹¶è¡Œç­–ç•¥

**2. è´Ÿè½½ä¸å‡è¡¡**
- è¾…åŠ©æŸå¤±
- Expert capacityé™åˆ¶
- åŠ¨æ€ä¸“å®¶åˆ†é…

**3. è®­ç»ƒä¸ç¨³å®š**
- è¾ƒå°çš„å­¦ä¹ ç‡
- Router Z-lossï¼ˆç¨³å®šlogitsï¼‰
- æ¢¯åº¦è£å‰ª

#### 4.2 æ¨ç†ä¼˜åŒ–

**1. æ‰¹å¤„ç†ç­–ç•¥**
```python
# æŒ‰ä¸“å®¶åˆ†ç»„tokens
for expert_id in range(num_experts):
    tokens_for_expert = tokens[routed_to[expert_id]]
    if len(tokens_for_expert) > 0:
        expert_output = experts[expert_id](tokens_for_expert)
```

**2. ç¼“å­˜ç­–ç•¥**
- KV cacheä¸MoEçš„äº¤äº’
- ä¸“å®¶æ¿€æ´»æ¨¡å¼ç¼“å­˜

**3. é‡åŒ–**
- ä¸“å®¶æƒé‡é‡åŒ–
- åŠ¨æ€åŠ è½½/å¸è½½ä¸“å®¶

---

### Part 5: MoEçš„æ•°å­¦åˆ†æ (30åˆ†é’Ÿ)

#### 5.1 å‚æ•°é‡è®¡ç®—

**Dense Transformer**:
```
Parameters per layer:
- Attention: 4 * d_modelÂ²
- FFN: 2 * d_model * d_ff
- Total: 4dÂ² + 2dÂ·d_ff

For d=4096, d_ff=16384:
â‰ˆ 67M + 134M = 201M per layer
```

**MoE Transformer**:
```
Parameters per layer:
- Attention: 4 * d_modelÂ²  (unchanged)
- Router: d_model * num_experts
- Experts: num_experts * (2 * d_model * d_ff)
- Total: 4dÂ² + dÂ·E + EÂ·(2dÂ·d_ff)

For d=4096, d_ff=16384, E=128:
â‰ˆ 67M + 0.5M + 17B = 17B per layer!
```

**å…³é”®**: å‚æ•°é‡å¢åŠ çº¦100å€ï¼Œä½†æ¿€æ´»å‚æ•°åªå¢åŠ kå€ï¼ˆk=1-2ï¼‰

#### 5.2 è®¡ç®—é‡åˆ†æ

**FLOPs per token**:
```
Dense FFN:
- Forward: 2 * d_model * d_ff = 134M FLOPs

MoE FFN (k=1):
- Router: d_model * num_experts = 0.5M
- Expert: 2 * d_model * d_ff = 134M
- Total: â‰ˆ 134M FLOPs (same as dense!)
```

**æƒŠäººçš„ç»“è®º**: MoEçš„è®¡ç®—é‡ä¸Denseç›¸è¿‘ï¼Œä½†æ¨¡å‹å®¹é‡å¤§å¾—å¤šï¼

#### 5.3 å†…å­˜åˆ†æ

**Training**:
```
Memory per layer:
- Parameters: E * 2 * d * d_ff
- Activations: batch * seq_len * d
- Router logits: batch * seq_len * E

For E=128, d=4096, d_ff=16384:
Parameters â‰ˆ 17GB (fp32)
```

**Inference**:
```
- åªéœ€åŠ è½½æ¿€æ´»çš„kä¸ªä¸“å®¶
- å¯ä»¥offloadä¸æ´»è·ƒçš„ä¸“å®¶
- å†…å­˜ â‰ˆ k/E * total_memory
```

---

### Part 6: å®ç°ä¸å®è·µ (45åˆ†é’Ÿ)

#### 6.1 åŸºç¡€MoEå±‚å®ç°

**å®Œæ•´å®ç°è§åç»­ä»£ç section**

#### 6.2 æ€§èƒ½ä¼˜åŒ–æŠ€å·§

1. **Expert Batching**: å°†è·¯ç”±åˆ°åŒä¸€ä¸“å®¶çš„tokensåˆå¹¶å¤„ç†
2. **Capacity Factorè°ƒä¼˜**: å¹³è¡¡è®¡ç®—æ•ˆç‡å’Œè´¨é‡
3. **è¾…åŠ©æŸå¤±æƒé‡**: é€šå¸¸0.01-0.1
4. **Router Z-loss**: ç¨³å®šè®­ç»ƒ

#### 6.3 è°ƒè¯•å¸¸è§é—®é¢˜

**é—®é¢˜1**: æ‰€æœ‰tokensè·¯ç”±åˆ°å°‘æ•°ä¸“å®¶
**è§£å†³**: å¢å¤§è¾…åŠ©æŸå¤±ç³»æ•°ï¼Œæ£€æŸ¥åˆå§‹åŒ–

**é—®é¢˜2**: è®­ç»ƒlosséœ‡è¡
**è§£å†³**: é™ä½å­¦ä¹ ç‡ï¼Œæ·»åŠ Router Z-loss

**é—®é¢˜3**: æ¨ç†æ…¢äºé¢„æœŸ
**è§£å†³**: ä¼˜åŒ–expert batchingï¼Œè€ƒè™‘expert offloading

---

## ğŸ” æ ¸å¿ƒæ¦‚å¿µæ€»ç»“

### MoEçš„æœ¬è´¨
```python
æ ¸å¿ƒæ€æƒ³ = {
    'Conditional Computation': 'æ ¹æ®è¾“å…¥é€‰æ‹©æ€§æ¿€æ´»å‚æ•°',
    'Sparse Activation': 'å¤§æ¨¡å‹å®¹é‡ï¼Œå°æ¿€æ´»å¼€é”€',
    'Ensemble of Specialists': 'å¤šä¸ªä¸“å®¶å„å¸å…¶èŒ'
}
```

### å…³é”®æƒè¡¡
| ç»´åº¦ | Dense | MoE |
|------|-------|-----|
| **å‚æ•°é‡** | dÂ·d_ff | EÂ·dÂ·d_ff |
| **è®¡ç®—é‡** | 2dÂ·d_ff | kÂ·2dÂ·d_ff |
| **å†…å­˜(æ¨ç†)** | å›ºå®š | å¯åŠ¨æ€ |
| **é€šä¿¡** | ä½ | é«˜(åˆ†å¸ƒå¼) |
| **è®­ç»ƒç¨³å®šæ€§** | é«˜ | ä¸­ç­‰ |
| **ä¸“é•¿å­¦ä¹ ** | æ—  | æœ‰ |

### ä½•æ—¶ä½¿ç”¨MoEï¼Ÿ
âœ… **é€‚åˆ**:
- éœ€è¦æå¤§æ¨¡å‹å®¹é‡
- æ¨ç†æˆæœ¬æ•æ„Ÿ
- ä»»åŠ¡æœ‰æ˜ç¡®å­é¢†åŸŸ

âš ï¸ **æŒ‘æˆ˜**:
- åˆ†å¸ƒå¼è®­ç»ƒå¤æ‚
- è´Ÿè½½å‡è¡¡å›°éš¾
- é€šä¿¡å¼€é”€å¤§

---

## ğŸ“Š å­¦ä¹ æ£€æŸ¥æ¸…å•

### ç†è®ºç†è§£ âœ“
- [ ] èƒ½è§£é‡ŠMoEçš„æ ¸å¿ƒåŠ¨æœº
- [ ] ç†è§£é—¨æ§æœºåˆ¶çš„ä½œç”¨
- [ ] æŒæ¡Top-Ké€‰æ‹©çš„æ•°å­¦
- [ ] ç†è§£è´Ÿè½½å‡è¡¡é—®é¢˜
- [ ] èƒ½åˆ†æå‚æ•°é‡ vs è®¡ç®—é‡
- [ ] å¯¹æ¯”Dense vs MoEçš„æƒè¡¡

### ç¼–ç¨‹èƒ½åŠ› âœ“
- [ ] å®ç°åŸºç¡€é—¨æ§ç½‘ç»œ
- [ ] å®ç°Top-Kä¸“å®¶é€‰æ‹©
- [ ] å®ç°è¾…åŠ©æŸå¤±
- [ ] ç»„è£…å®Œæ•´MoEå±‚
- [ ] å¯¹æ¯”MoE vs Denseæ€§èƒ½

### ç³»ç»Ÿæ€ç»´ âœ“
- [ ] åˆ†æMoEçš„è®­ç»ƒç“¶é¢ˆ
- [ ] è®¾è®¡è´Ÿè½½å‡è¡¡ç­–ç•¥
- [ ] è¯„ä¼°é€šä¿¡å¼€é”€
- [ ] ä¼˜åŒ–æ¨ç†æ€§èƒ½

---

## ğŸ”— ä¸å…¶ä»–Lectureçš„è”ç³»

**â† Lecture 03 (Transformer)**:
- MoEæ›¿æ¢FFNå±‚
- Attentionå±‚ä¿æŒä¸å˜
- æ®‹å·®è¿æ¥ä¾ç„¶é‡è¦

**â†’ Lecture 06 (GPU Kernels)**:
- Expert batchingçš„kernelä¼˜åŒ–
- All-to-Allé€šä¿¡å®ç°

**â†’ Lecture 10 (Inference)**:
- Expert offloadingç­–ç•¥
- é‡åŒ–ä¸MoEçš„ç»“åˆ

---

## ğŸ“š å‚è€ƒèµ„æ–™

### æ ¸å¿ƒè®ºæ–‡
1. **Shazeer et al. 2017**: "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
2. **Fedus et al. 2021**: "Switch Transformers: Scaling to Trillion Parameter Models"
3. **Du et al. 2021**: "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"
4. **Zoph et al. 2022**: "ST-MoE: Designing Stable and Transferable Sparse Expert Models"

### å®ç°å‚è€ƒ
- Hugging Face Transformers (Switch Transformer)
- Fairseq (MoE layers)
- Megatron-LM (Expert Parallelism)

---

**åˆ›å»ºæ—¥æœŸ**: 2025-01-12
**çŠ¶æ€**: âœ… æ•™å­¦å¤§çº²å®Œæˆ
**ä¸‹ä¸€æ­¥**: æ·±åº¦é—®ç­”å’Œå®ç°ä»£ç 

---

## é™„å½•ï¼šå…³é”®å…¬å¼é€ŸæŸ¥

### é—¨æ§å‡½æ•°
```
G(x) = softmax(W_g Â· x + noise)
noise ~ N(0, softplus(W_noise Â· x)Â²)
```

### è¾…åŠ©æŸå¤±
```
L_aux = Î± Â· Î£(importance_i Ã— load_i)
importance_i = mean_tokens(G(x)_i)
load_i = sum_tokens(1[i in top_k(x)])
```

### Router Z-loss
```
L_z = log(Î£ exp(logits))Â²
ç›®çš„: é˜²æ­¢logitsè¿‡å¤§
```

### Expert Capacity
```
capacity = (tokens_per_batch / num_experts) Ã— capacity_factor
capacity_factor âˆˆ [1.0, 2.0]
```
