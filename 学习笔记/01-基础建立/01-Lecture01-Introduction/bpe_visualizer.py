#!/usr/bin/env python3
"""
ğŸ¯ BPE (Byte Pair Encoding) å½¢è±¡åŒ–æ¼”ç¤ºå·¥å…·
===========================================

è¿™ä¸ªè„šæœ¬é€šè¿‡å¯è§†åŒ–æ¼”ç¤ºå¸®åŠ©ç†è§£BPEç®—æ³•çš„å·¥ä½œåŸç†ã€‚
åŒ…å«é€æ­¥æ¼”ç¤ºã€åŠ¨ç”»æ•ˆæœå’Œè¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯ã€‚

ä½œè€…: Claude Code
æ—¥æœŸ: 2025-11-06
ç”¨é€”: Lecture 01 - TokenizationåŸç†ä¸å®è·µè¡¥å……ææ–™
"""

import json
import time
from collections import defaultdict
from typing import Dict, List, Tuple, Optional
import argparse


class BPEVisualizer:
    """BPEç®—æ³•å¯è§†åŒ–æ¼”ç¤ºå™¨"""

    def __init__(self, text: str, num_merges: int = 10, delay: float = 0.5):
        self.original_text = text
        self.num_merges = num_merges
        self.delay = delay

        # åˆå§‹åŒ–çŠ¶æ€
        self.reset()

    def reset(self):
        """é‡ç½®åˆ°åˆå§‹çŠ¶æ€"""
        # å°†æ–‡æœ¬è½¬æ¢ä¸ºå­—èŠ‚åºåˆ—
        self.indices = list(map(int, self.original_text.encode("utf-8")))
        self.vocab = {x: bytes([x]) for x in range(256)}  # 0-255 â†’ å•å­—èŠ‚
        self.merges = {}  # (token1, token2) â†’ new_token
        self.merge_history = []  # è®°å½•åˆå¹¶å†å²
        self.step = 0

    def print_separator(self, title: str = ""):
        """æ‰“å°åˆ†éš”çº¿"""
        print("\n" + "="*80)
        if title:
            print(f"ğŸ¯ {title}")
            print("="*80)

    def print_tokens(self, indices: List[int], label: str = ""):
        """ç¾åŒ–æ‰“å°tokenåºåˆ—"""
        if label:
            print(f"\nğŸ“ {label}:")

        # æ˜¾ç¤ºtokenåºåˆ—
        tokens_str = " â†’ ".join([f"[{idx}]" for idx in indices])
        print(f"   Tokens: {tokens_str}")

        # æ˜¾ç¤ºå¯¹åº”çš„å­—èŠ‚
        bytes_str = " â†’ ".join([f"{self.vocab[idx]!r}" for idx in indices])
        print(f"   Bytes:  {bytes_str}")

        # æ˜¾ç¤ºè§£ç åçš„æ–‡æœ¬ï¼ˆå¦‚æœå¯è¯»ï¼‰
        try:
            decoded = b"".join([self.vocab[idx] for idx in indices]).decode("utf-8")
            if decoded.isprintable():
                print(f"   Text:   {decoded!r}")
        except:
            print(f"   Text:   <éUTF-8åºåˆ—>")

    def find_most_frequent_pair(self, indices: List[int]) -> Tuple[Tuple[int, int], int]:
        """æ‰¾åˆ°æœ€é¢‘ç¹çš„ç›¸é‚»tokenå¯¹"""
        counts = defaultdict(int)

        # ç»Ÿè®¡ç›¸é‚»å¯¹é¢‘ç‡
        for i in range(len(indices) - 1):
            pair = (indices[i], indices[i + 1])
            counts[pair] += 1

        if not counts:
            return None, 0

        # æ‰¾åˆ°æœ€é¢‘ç¹çš„å¯¹
        most_frequent = max(counts.items(), key=lambda x: x[1])
        return most_frequent  # ((token1, token2), frequency)

    def merge_pair(self, indices: List[int], pair: Tuple[int, int], new_token: int) -> List[int]:
        """åˆå¹¶tokenåºåˆ—ä¸­çš„æŒ‡å®šå¯¹"""
        new_indices = []
        i = 0

        while i < len(indices):
            if (i + 1 < len(indices) and
                indices[i] == pair[0] and
                indices[i + 1] == pair[1]):
                new_indices.append(new_token)
                i += 2  # è·³è¿‡å·²åˆå¹¶çš„ä¸¤ä¸ªtoken
            else:
                new_indices.append(indices[i])
                i += 1

        return new_indices

    def visualize_pair_frequencies(self, indices: List[int]):
        """å¯è§†åŒ–tokenå¯¹é¢‘ç‡ç»Ÿè®¡"""
        counts = defaultdict(int)

        for i in range(len(indices) - 1):
            pair = (indices[i], indices[i + 1])
            counts[pair] += 1

        if not counts:
            print("   ğŸ“Š æ— ç›¸é‚»tokenå¯¹")
            return

        # æŒ‰é¢‘ç‡æ’åº
        sorted_pairs = sorted(counts.items(), key=lambda x: x[1], reverse=True)

        print(f"\nğŸ“Š Tokenå¯¹é¢‘ç‡ç»Ÿè®¡ (å…±{len(sorted_pairs)}ç§ä¸åŒçš„å¯¹):")
        print("   " + "-"*60)
        print("   æ’å | Tokenå¯¹          | é¢‘ç‡ | å­—èŠ‚è¡¨ç¤º")
        print("   " + "-"*60)

        for i, (pair, freq) in enumerate(sorted_pairs[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
            token1, token2 = pair
            bytes_repr = f"{self.vocab[token1]!r}+{self.vocab[token2]!r}"
            print(f"   #{i+1:2d}  | [{token1}]+[{token2:3d}]      | {freq:3d}  | {bytes_repr}")

        if len(sorted_pairs) > 10:
            print(f"   ... è¿˜æœ‰{len(sorted_pairs)-10}ä¸ªtokenå¯¹")

    def step_visualize(self):
        """å•æ­¥å¯è§†åŒ–BPEåˆå¹¶è¿‡ç¨‹"""
        self.step += 1

        self.print_separator(f"ç¬¬ {self.step} æ­¥åˆå¹¶")

        # æ˜¾ç¤ºå½“å‰çŠ¶æ€
        self.print_tokens(self.indices, f"å½“å‰tokenåºåˆ— (é•¿åº¦: {len(self.indices)})")

        # æ˜¾ç¤ºtokenå¯¹é¢‘ç‡ç»Ÿè®¡
        self.visualize_pair_frequencies(self.indices)

        # æ‰¾åˆ°æœ€é¢‘ç¹çš„å¯¹
        most_frequent_pair, frequency = self.find_most_frequent_pair(self.indices)

        if most_frequent_pair is None:
            print("\nâŒ æ²¡æœ‰å¯ä»¥åˆå¹¶çš„tokenå¯¹")
            return False

        token1, token2 = most_frequent_pair
        new_token = 256 + len(self.merges)  # æ–°tokenç´¢å¼•

        print(f"\nğŸ¯ é€‰æ‹©æœ€é¢‘ç¹çš„tokenå¯¹: [{token1}]+[{token2}] (å‡ºç°{frequency}æ¬¡)")
        print(f"ğŸ†• åˆ›å»ºæ–°token: [{new_token}] = {self.vocab[token1]!r}+{self.vocab[token2]!r}")

        # ç­‰å¾…ç”¨æˆ·ç¡®è®¤ï¼ˆå¦‚æœè®¾ç½®äº†å»¶è¿Ÿï¼‰
        if self.delay > 0:
            time.sleep(self.delay)

        # æ‰§è¡Œåˆå¹¶
        old_length = len(self.indices)
        self.indices = self.merge_pair(self.indices, most_frequent_pair, new_token)
        new_length = len(self.indices)

        # æ›´æ–°è¯æ±‡è¡¨å’Œåˆå¹¶è§„åˆ™
        self.merges[most_frequent_pair] = new_token
        self.vocab[new_token] = self.vocab[token1] + self.vocab[token2]

        # è®°å½•å†å²
        self.merge_history.append({
            'step': self.step,
            'pair': most_frequent_pair,
            'new_token': new_token,
            'frequency': frequency,
            'old_length': old_length,
            'new_length': new_length,
            'compression_ratio': old_length / new_length
        })

        # æ˜¾ç¤ºåˆå¹¶ç»“æœ
        self.print_tokens(self.indices, f"åˆå¹¶åtokenåºåˆ— (é•¿åº¦: {len(self.indices)})")

        # æ˜¾ç¤ºå‹ç¼©æ•ˆæœ
        compression = old_length / new_length
        print(f"\nğŸ“ˆ å‹ç¼©æ•ˆæœ:")
        print(f"   åˆå¹¶å‰é•¿åº¦: {old_length}")
        print(f"   åˆå¹¶åé•¿åº¦: {new_length}")
        print(f"   å‹ç¼©æ¯”: {compression:.3f}")

        return True

    def run_visualization(self):
        """è¿è¡Œå®Œæ•´çš„å¯è§†åŒ–è¿‡ç¨‹"""
        print("ğŸš€ BPEç®—æ³•å¯è§†åŒ–æ¼”ç¤ºå¼€å§‹")
        print(f"ğŸ“ åŸå§‹æ–‡æœ¬: {self.original_text!r}")
        print(f"ğŸ¯ ç›®æ ‡åˆå¹¶æ¬¡æ•°: {self.num_merges}")

        # æ˜¾ç¤ºåˆå§‹çŠ¶æ€
        self.print_separator("åˆå§‹çŠ¶æ€")
        self.print_tokens(self.indices, f"åˆå§‹tokenåºåˆ— (é•¿åº¦: {len(self.indices)})")

        print(f"\nğŸ’¡ åˆå§‹è¯æ±‡è¡¨å¤§å°: {len(self.vocab)} (0-255çš„å•å­—èŠ‚)")

        # é€æ­¥åˆå¹¶
        for i in range(self.num_merges):
            if not self.step_visualize():
                break

        # æœ€ç»ˆæ€»ç»“
        self.print_final_summary()

    def print_final_summary(self):
        """æ‰“å°æœ€ç»ˆæ€»ç»“"""
        self.print_separator("ğŸ‰ BPEè®­ç»ƒå®Œæˆæ€»ç»“")

        print(f"ğŸ“Š è®­ç»ƒç»Ÿè®¡:")
        print(f"   åŸå§‹æ–‡æœ¬é•¿åº¦: {len(self.original_text)} å­—ç¬¦")
        print(f"   åˆå§‹tokenæ•°: {len(list(map(int, self.original_text.encode('utf-8'))))}")
        print(f"   æœ€ç»ˆtokenæ•°: {len(self.indices)}")
        print(f"   æ€»åˆå¹¶æ­¥æ•°: {len(self.merge_history)}")
        print(f"   æœ€ç»ˆè¯æ±‡è¡¨å¤§å°: {len(self.vocab)}")

        # å‹ç¼©æ¯”
        initial_length = len(list(map(int, self.original_text.encode('utf-8'))))
        final_compression = initial_length / len(self.indices)
        print(f"   æ€»å‹ç¼©æ¯”: {final_compression:.3f}")

        # æ˜¾ç¤ºåˆå¹¶å†å²
        print(f"\nğŸ“œ åˆå¹¶å†å²:")
        print("   æ­¥éª¤ | åˆå¹¶å¯¹        | æ–°token | å‹ç¼©æ¯”")
        print("   " + "-"*40)

        for record in self.merge_history:
            pair = record['pair']
            print(f"   #{record['step']:2d}  | [{pair[0]}]+[{pair[1]:3d}]    | [{record['new_token']:3d}]   | {record['compression_ratio']:.3f}")

        # æ˜¾ç¤ºæœ€ç»ˆè¯æ±‡è¡¨ï¼ˆæ–°åˆ›å»ºçš„tokensï¼‰
        print(f"\nğŸ“š æ–°åˆ›å»ºçš„è¯æ±‡:")
        for record in self.merge_history:
            new_token = record['new_token']
            pair = record['pair']
            print(f"   [{new_token:3d}] = {self.vocab[pair[0]]!r}+{self.vocab[pair[1]]!r}")

    def encode_with_trained_bpe(self, text: str) -> List[int]:
        """ä½¿ç”¨è®­ç»ƒå¥½çš„BPEç¼–ç æ–°æ–‡æœ¬"""
        print(f"\nğŸ” ä½¿ç”¨è®­ç»ƒå¥½çš„BPEç¼–ç æ–°æ–‡æœ¬: {text!r}")

        # 1. è½¬æ¢ä¸ºå­—èŠ‚åºåˆ—
        indices = list(map(int, text.encode("utf-8")))
        self.print_tokens(indices, "åŸå§‹å­—èŠ‚åºåˆ—")

        # 2. åº”ç”¨æ‰€æœ‰åˆå¹¶è§„åˆ™
        for step, record in enumerate(self.merge_history):
            pair = record['pair']
            new_token = record['new_token']
            old_length = len(indices)

            indices = self.merge_pair(indices, pair, new_token)

            if len(indices) < old_length:  # åªæœ‰å®é™…åˆå¹¶æ—¶æ‰æ˜¾ç¤º
                print(f"   æ­¥éª¤{step+1}: åº”ç”¨åˆå¹¶ [{pair[0]}]+[{pair[1]}] â†’ [{new_token}] (é•¿åº¦: {old_length}â†’{len(indices)})")

        self.print_tokens(indices, "æœ€ç»ˆBPEç¼–ç ")
        return indices

    def decode_with_trained_bpe(self, indices: List[int]) -> str:
        """ä½¿ç”¨è®­ç»ƒå¥½çš„BPEè§£ç """
        print(f"\nğŸ”„ è§£ç tokenåºåˆ—: {indices}")

        # å°†tokenç´¢å¼•è½¬æ¢ä¸ºå­—èŠ‚
        bytes_list = [self.vocab[idx] for idx in indices]
        print(f"   å­—èŠ‚åºåˆ—: {b''.join(bytes_list)!r}")

        # è§£ç ä¸ºå­—ç¬¦ä¸²
        result = b"".join(bytes_list).decode("utf-8")
        print(f"   è§£ç ç»“æœ: {result!r}")

        return result


def demo_simple_example():
    """ç®€å•ç¤ºä¾‹æ¼”ç¤º"""
    print("ğŸ¯ ç®€å•ç¤ºä¾‹: 'aaabdaaabac'")

    text = "aaabdaaabac"
    visualizer = BPEVisualizer(text, num_merges=5, delay=0)
    visualizer.run_visualization()

    # æµ‹è¯•ç¼–ç æ–°æ–‡æœ¬
    visualizer.encode_with_trained_bpe("aaaa")
    visualizer.decode_with_trained_bpe([256, 256, 100, 256, 256, 99])


def demo_chinese_example():
    """ä¸­æ–‡ç¤ºä¾‹æ¼”ç¤º"""
    print("ğŸ¯ ä¸­æ–‡ç¤ºä¾‹: 'ä½ å¥½ä¸–ç•Œï¼Œä½ å¥½Python'")

    text = "ä½ å¥½ä¸–ç•Œï¼Œä½ å¥½Python"
    visualizer = BPEVisualizer(text, num_merges=8, delay=0)
    visualizer.run_visualization()

    # æµ‹è¯•ç¼–ç æ–°æ–‡æœ¬
    visualizer.encode_with_trained_bpe("ä½ å¥½")


def demo_english_example():
    """è‹±æ–‡ç¤ºä¾‹æ¼”ç¤º"""
    print("ğŸ¯ è‹±æ–‡ç¤ºä¾‹: 'hello world hello'")

    text = "hello world hello"
    visualizer = BPEVisualizer(text, num_merges=10, delay=0)
    visualizer.run_visualization()


def interactive_mode():
    """äº¤äº’æ¨¡å¼"""
    print("ğŸ® äº¤äº’å¼BPEæ¼”ç¤º")
    print("è¾“å…¥æ–‡æœ¬å’Œåˆå¹¶æ¬¡æ•°ï¼Œè§‚å¯ŸBPEç®—æ³•çš„å·¥ä½œè¿‡ç¨‹")

    while True:
        text = input("\nğŸ“ è¯·è¾“å…¥æ–‡æœ¬ (æˆ– 'quit' é€€å‡º): ").strip()
        if text.lower() == 'quit':
            break

        try:
            num_merges = int(input("ğŸ¯ åˆå¹¶æ¬¡æ•° (é»˜è®¤10): ").strip() or "10")
        except ValueError:
            num_merges = 10

        try:
            delay = float(input("â±ï¸ æ¼”ç¤ºå»¶è¿Ÿç§’æ•° (é»˜è®¤0.5): ").strip() or "0.5")
        except ValueError:
            delay = 0.5

        visualizer = BPEVisualizer(text, num_merges, delay)
        visualizer.run_visualization()

        # æµ‹è¯•ç¼–ç 
        test_text = input(f"\nğŸ” æµ‹è¯•ç¼–ç æ–‡æœ¬ (é»˜è®¤ '{text[:5]}'): ").strip() or text[:5]
        visualizer.encode_with_trained_bpe(test_text)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="BPEç®—æ³•å¯è§†åŒ–æ¼”ç¤ºå·¥å…·")
    parser.add_argument("--text", type=str, help="è¦å¤„ç†çš„æ–‡æœ¬")
    parser.add_argument("--merges", type=int, default=10, help="åˆå¹¶æ¬¡æ•°")
    parser.add_argument("--delay", type=float, default=0.5, help="æ¼”ç¤ºå»¶è¿Ÿ")
    parser.add_argument("--demo", choices=["simple", "chinese", "english"], help="é¢„è®¾æ¼”ç¤º")
    parser.add_argument("--interactive", action="store_true", help="äº¤äº’æ¨¡å¼")

    args = parser.parse_args()

    if args.interactive:
        interactive_mode()
    elif args.demo == "simple":
        demo_simple_example()
    elif args.demo == "chinese":
        demo_chinese_example()
    elif args.demo == "english":
        demo_english_example()
    elif args.text:
        visualizer = BPEVisualizer(args.text, args.merges, args.delay)
        visualizer.run_visualization()
    else:
        print("ğŸ¯ BPEå¯è§†åŒ–æ¼”ç¤ºå·¥å…·")
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("1. é¢„è®¾æ¼”ç¤º: python bpe_visualizer.py --demo simple")
        print("2. äº¤äº’æ¨¡å¼: python bpe_visualizer.py --interactive")
        print("3. è‡ªå®šä¹‰æ–‡æœ¬: python bpe_visualizer.py --text 'your text' --merges 5")
        print("\nğŸš€ å¯åŠ¨ç®€å•æ¼”ç¤º...")
        demo_simple_example()


if __name__ == "__main__":
    main()