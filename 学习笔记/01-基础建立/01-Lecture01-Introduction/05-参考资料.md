# Lecture 01: Introduction & Tokenization - å‚è€ƒèµ„æ–™

## ğŸ“– æ ¸å¿ƒè®ºæ–‡

### å†å²åŸºç¡€è®ºæ–‡

#### [Shannon (1950)](https://dl.acm.org/doi/10.1109/JRPROC.1950.233823)
**æ ‡é¢˜**: Prediction and Entropy of Printed English
**è´¡çŒ®**:
- é¦–æ¬¡æå‡ºç”¨è¯­è¨€æ¨¡å‹æµ‹é‡è‹±è¯­ç†µ
- å¥ å®šäº†ä¿¡æ¯è®ºåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åŸºç¡€
- å¼•å…¥äº†n-gramè¯­è¨€æ¨¡å‹çš„æ¦‚å¿µ

#### [Bengio et al. (2003)](https://www.jmlr.org/papers/v3/bengio03a.html)
**æ ‡é¢˜**: A Neural Probabilistic Language Model
**è´¡çŒ®**:
- ç¬¬ä¸€ä¸ªç¥ç»è¯­è¨€æ¨¡å‹
- å¼•å…¥äº†è¯åµŒå…¥çš„æ¦‚å¿µ
- å¥ å®šäº†ç°ä»£ç¥ç»è¯­è¨€å¤„ç†çš„åŸºç¡€

### Transformeræ—¶ä»£è®ºæ–‡

#### [Bahdanau et al. (2015)](https://arxiv.org/abs/1409.0473)
**æ ‡é¢˜**: Neural Machine Translation by Jointly Learning to Align and Translate
**è´¡çŒ®**:
- å¼•å…¥äº†æ³¨æ„åŠ›æœºåˆ¶
- è§£å†³äº†åºåˆ—åˆ°åºåˆ—æ¨¡å‹çš„ç“¶é¢ˆé—®é¢˜
- ä¸ºTransformeræ¶æ„å¥ å®šåŸºç¡€

#### [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762)
**æ ‡é¢˜**: Attention Is All You Need
**è´¡çŒ®**:
- æå‡ºäº†Transformeræ¶æ„
- å®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ‘’å¼ƒRNN/CNN
- æˆä¸ºç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„åŸºç¡€æ¶æ„

#### [Shazeer et al. (2017)](https://arxiv.org/abs/1701.06538)
**æ ‡é¢˜**: Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer
**è´¡çŒ®**:
- æå‡ºäº†æ··åˆä¸“å®¶(MoE)æ¨¡å‹
- å…è®¸æ¨¡å‹è§„æ¨¡å¤§å¹…æ‰©å±•è€Œä¸å¢åŠ è®¡ç®—é‡
- ç°ä»£å¤§æ¨¡å‹çš„é‡è¦æŠ€æœ¯

### æ‰©å±•æ³•åˆ™ç›¸å…³

#### [Kaplan et al. (2020)](https://arxiv.org/abs/2001.08361)
**æ ‡é¢˜**: Scaling Laws for Neural Language Models
**è´¡çŒ®**:
- ç³»ç»Ÿç ”ç©¶äº†æ¨¡å‹è§„æ¨¡ã€æ•°æ®é‡ã€è®¡ç®—é‡ä¹‹é—´çš„å…³ç³»
- æå‡ºäº†æ‰©å±•æ³•åˆ™çš„æ•°å­¦å…¬å¼
- ä¸ºå¤§æ¨¡å‹è®­ç»ƒæä¾›äº†ç†è®ºæŒ‡å¯¼

#### [Hoffmann et al. (2022)](https://arxiv.org/abs/2203.15556)
**æ ‡é¢˜**: Training Compute-Optimal Large Language Models
**è´¡çŒ®**:
- æå‡ºäº†Chinchillaæ‰©å±•æ³•åˆ™
- é‡æ–°æ ¡å‡†äº†æ¨¡å‹å¤§å°å’Œæ•°æ®é‡çš„æœ€ä¼˜æ¯”ä¾‹
- æŒ‡å‡ºä¹‹å‰çš„æ¨¡å‹éƒ½è®­ç»ƒä¸è¶³

### é‡è¦æ¨¡å‹è®ºæ–‡

#### [Radford et al. (2019)](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
**æ ‡é¢˜**: Language Models are Unsupervised Multitask Learners (GPT-2)
**è´¡çŒ®**:
- å±•ç¤ºäº†å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬èƒ½åŠ›
- å¼•å‘äº†å¯¹æ¨¡å‹èƒ½åŠ›å’Œå®‰å…¨çš„è®¨è®º
- é‡‡ç”¨äº†åˆ†é˜¶æ®µå‘å¸ƒçš„ç­–ç•¥

#### [Brown et al. (2020)](https://arxiv.org/abs/2005.14165)
**æ ‡é¢˜**: Language Models are Few-Shot Learners (GPT-3)
**è´¡çŒ®**:
- å±•ç¤ºäº†ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›
- è¯æ˜äº†è§„æ¨¡æ‰©å±•å¸¦æ¥çš„è´¨å˜
- å¼€å¯äº†å¤§æ¨¡å‹æ—¶ä»£

#### [Brown et al. (2024)](https://arxiv.org/abs/2303.08774)
**æ ‡é¢˜**: GPT-4 Technical Report
**è´¡çŒ®**:
- æœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§æ¨¡å‹
- å±•ç¤ºäº†å¤§è§„æ¨¡è®­ç»ƒçš„æŠ€æœ¯æŒ‘æˆ˜
- ä½†ç¼ºä¹æŠ€æœ¯ç»†èŠ‚çš„å…¬å¼€

### å¼€æ”¾æ¨¡å‹ç›¸å…³

#### [GPT-J (2021)](https://github.com/EleutherAI/gpt-j)
**ç»„ç»‡**: EleutherAI
**è´¡çŒ®**:
- ç¬¬ä¸€ä¸ªçœŸæ­£å¼€æ”¾çš„GPT-3çº§åˆ«æ¨¡å‹
- æä¾›äº†å®Œæ•´çš„è®­ç»ƒä»£ç å’Œæƒé‡
- æ¨åŠ¨äº†å¼€æ”¾æ¨¡å‹è¿åŠ¨

#### [LLaMA (2023)](https://arxiv.org/abs/2302.13971)
**ç»„ç»‡**: Meta AI
**è´¡çŒ®**:
- è¯æ˜äº†è¾ƒå°æ¨¡å‹ä¹Ÿèƒ½è¾¾åˆ°å¾ˆå¥½æ•ˆæœ
- æ¿€å‘äº†å¼€æ”¾æ¨¡å‹ç”Ÿæ€ç³»ç»Ÿ
- æ¨åŠ¨äº†æ¨¡å‹ä¼˜åŒ–ç ”ç©¶

#### [OLMo (2024)](https://allenai.org/llm)
**ç»„ç»‡**: AI2
**è´¡çŒ®**:
- çœŸæ­£çš„å¼€æºæ¨¡å‹(æƒé‡+æ•°æ®+ä»£ç )
- æä¾›äº†å®Œæ•´çš„è®­ç»ƒç»†èŠ‚
- ä¸ºå­¦æœ¯ç ”ç©¶æä¾›äº†é€æ˜åº¦

### Tokenizationç›¸å…³

#### [Sennrich et al. (2016)](https://arxiv.org/abs/1508.07909)
**æ ‡é¢˜**: Neural Machine Translation of Rare Words with Subword Units
**è´¡çŒ®**:
- å°†BPEå¼•å…¥ç¥ç»æœºå™¨ç¿»è¯‘
- è§£å†³äº†ç½•è§è¯é—®é¢˜
- æˆä¸ºç°ä»£tokenizerçš„æ ‡å‡†æ–¹æ³•

#### [Gage (1994)](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)
**æ ‡é¢˜**: A New Algorithm for Data Compression
**è´¡çŒ®**:
- æœ€åˆæå‡ºBPEç®—æ³•ç”¨äºæ•°æ®å‹ç¼©
- åæ¥è¢«NLPé¢†åŸŸé‡‡ç”¨

## ğŸ› ï¸ é‡è¦å·¥å…·å’Œåº“

### Tokenizationå·¥å…·
- [tiktoken](https://github.com/openai/tiktoken): OpenAIçš„å¿«é€Ÿtokenizeråº“
- [sentencepiece](https://github.com/google/sentencepiece): Googleçš„subword tokenizer
- [huggingface/tokenizers]: Hugging Faceçš„é«˜æ€§èƒ½tokenizeråº“

### äº¤äº’å·¥å…·
- [tiktokenizer](https://tiktokenizer.vercel.app/): åœ¨çº¿tokenizerå¯è§†åŒ–å·¥å…·
- [OpenAI Tokenizer](https://platform.openai.com/tokenizer): OpenAIå®˜æ–¹å·¥å…·

## ğŸ“š æ¨èé˜…è¯»ææ–™

### æ•™ç¨‹å’Œåšå®¢
- [Andrej Karpathyçš„Tokenizationè§†é¢‘](https://www.youtube.com/watch?v=zduSFxRajkE): æ·±å…¥æµ…å‡ºçš„tokenizationè§£é‡Š
- [The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html): Suttonçš„ç»å…¸æ–‡ç« 
- [Hugging Face Tokenization Course](https://huggingface.co/course/chapter2/4): å®ç”¨çš„tokenizationæ•™ç¨‹

### æŠ€æœ¯æŠ¥å‘Š
- [ChinchillaæŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2203.15556): è¯¦ç»†çš„æ‰©å±•æ³•åˆ™åˆ†æ
- [GPT-4æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2303.08774): è™½ç„¶ç¼ºä¹ç»†èŠ‚ï¼Œä½†ä»æ˜¯é‡è¦å‚è€ƒ
- [DeepSeekæŠ€æœ¯æŠ¥å‘Š](https://github.com/deepseek-ai/DeepSeek-V2): å¼€æ”¾æ¨¡å‹çš„æŠ€æœ¯ç»†èŠ‚

### æ•°æ®é›†ç›¸å…³
- [The Pile](https://pile.eleuther.ai/): å¤§è§„æ¨¡å¼€æ”¾æ–‡æœ¬æ•°æ®é›†
- [Common Crawl](https://commoncrawl.org/): ç½‘é¡µçˆ¬å–æ•°æ®çš„ä¸»è¦æ¥æº
- [OpenWebText](https://github.com/jcpeterson/openwebtext): å¼€æ”¾çš„ç½‘é¡µæ–‡æœ¬æ•°æ®é›†

## ğŸŒ é‡è¦èµ„æºé“¾æ¥

### è¯¾ç¨‹ç›¸å…³
- [CS336å®˜æ–¹ç½‘ç«™](https://stanford-cs336.github.io/spring2025/)
- [è¯¾ç¨‹YouTubeé¢‘é“](https://www.youtube.com/@stanfordcs336)
- [Together AIé›†ç¾¤æŒ‡å—](https://docs.google.com/document/d/1BSSig7zInyjDKcbNGftVxubiHlwJ-ZqahQewIzBmBOo/edit)

### æ¨¡å‹æ’è¡Œæ¦œ
- [Hugging Face Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
- [AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/): æŒ‡ä»¤è·Ÿéšèƒ½åŠ›è¯„ä¼°
- [Chatbot Arena](https://chat.lmsys.org/): äººç±»åå¥½è¯„ä¼°

### ç ”ç©¶å·¥å…·
- [Weights & Biases](https://wandb.ai/): å®éªŒè·Ÿè¸ªå’Œå¯è§†åŒ–
- [TensorBoard](https://www.tensorflow.org/tensorboard): è®­ç»ƒç›‘æ§
- [PyTorch](https://pytorch.org/): æ·±åº¦å­¦ä¹ æ¡†æ¶

## ğŸ“Š æ‰©å±•é˜…è¯»

### æ•ˆç‡ä¼˜åŒ–ç›¸å…³
- [DeepSpeed](https://www.deepspeed.ai/): å¾®è½¯çš„åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶
- [FairScale](https://fairscale.readthedocs.io/): Facebookçš„åˆ†å¸ƒå¼è®­ç»ƒåº“
- [Megatron-LM](https://github.com/NVIDIA/Megatron-LM): NVIDIAçš„å¤§æ¨¡å‹è®­ç»ƒæ¡†æ¶

### ç¡¬ä»¶ç›¸å…³
- [NVIDIA A100 Specification](https://www.nvidia.com/en-us/data-center/a100/): GPUç¡¬ä»¶è§„æ ¼
- [GPU Architecture Analysis](https://horace.io/img/perf_intro/factory_bandwidth.png): ç¡¬ä»¶æ€§èƒ½åˆ†æ

### ä¼¦ç†å’Œå®‰å…¨ç›¸å…³
- [On the Dangers of Stochastic Parrots](https://dl.acm.org/doi/10.1145/3442188.3445922): è¯­è¨€æ¨¡å‹ä¼¦ç†è®¨è®º
- [Model Cards for Model Reporting](https://arxiv.org/abs/1810.03993): æ¨¡å‹é€æ˜åº¦æŠ¥å‘Šæ¡†æ¶

---

**ğŸ’¡ æç¤º**: è¿™äº›å‚è€ƒèµ„æ–™æŒ‰ç…§é‡è¦æ€§å’Œç›¸å…³æ€§æ’åºï¼Œå»ºè®®ä¼˜å…ˆé˜…è¯»æ ‡è®°ä¸ºæ ¸å¿ƒè®ºæ–‡çš„æ–‡çŒ®ã€‚å¯¹äºå®è·µéƒ¨åˆ†ï¼Œå·¥å…·å’Œåº“çš„æ–‡æ¡£æ›´åŠ å®ç”¨ã€‚