#!/usr/bin/env python3
"""
ğŸ¯ BPE (Byte Pair Encoding) æ ¸å¿ƒç®—æ³•å®ç°
===========================================

ç®€æ´æ¸…æ™°çš„BPEå®ç°ï¼Œä¸“æ³¨äºç®—æ³•æœ¬è´¨ï¼Œä¾¿äºç†è§£å’Œå­¦ä¹ ã€‚

ä½œè€…: Claude Code
æ—¥æœŸ: 2025-11-06
ç”¨é€”: Lecture 01 - TokenizationåŸç†ä¸å®è·µæ ¸å¿ƒä»£ç ç¤ºä¾‹
"""

from collections import defaultdict
from typing import Dict, List, Tuple, Optional


class BPETokenizer:
    """BPE Tokenizeræ ¸å¿ƒå®ç°"""

    def __init__(self):
        self.vocab: Dict[int, bytes] = {}
        self.merges: Dict[Tuple[int, int], int] = {}
        self._init_vocab()

    def _init_vocab(self):
        """åˆå§‹åŒ–è¯æ±‡è¡¨ä¸ºå•å­—èŠ‚"""
        self.vocab = {i: bytes([i]) for i in range(256)}

    def train(self, text: str, num_merges: int) -> None:
        """
        è®­ç»ƒBPE tokenizer

        Args:
            text: è®­ç»ƒæ–‡æœ¬
            num_merges: åˆå¹¶æ¬¡æ•°
        """
        print(f"ğŸš€ å¼€å§‹BPEè®­ç»ƒ: '{text}' (åˆå¹¶{num_merges}æ¬¡)")

        # 1. å°†æ–‡æœ¬è½¬æ¢ä¸ºå­—èŠ‚åºåˆ—
        indices = list(map(int, text.encode("utf-8")))
        print(f"ğŸ“ åˆå§‹tokenåºåˆ—: {indices}")

        # 2. è¿­ä»£åˆå¹¶
        for i in range(num_merges):
            # ç»Ÿè®¡ç›¸é‚»tokenå¯¹é¢‘ç‡
            pair_counts = self._count_pairs(indices)
            if not pair_counts:
                break

            # æ‰¾åˆ°æœ€é¢‘ç¹çš„å¯¹
            most_frequent_pair = max(pair_counts.items(), key=lambda x: x[1])[0]
            new_token = 256 + i

            print(f"æ­¥éª¤{i+1}: åˆå¹¶ {most_frequent_pair} â†’ {new_token} "
                  f"(é¢‘ç‡: {pair_counts[most_frequent_pair]})")

            # æ‰§è¡Œåˆå¹¶
            indices = self._merge(indices, most_frequent_pair, new_token)

            # è®°å½•åˆå¹¶è§„åˆ™å’Œæ›´æ–°è¯æ±‡è¡¨
            self.merges[most_frequent_pair] = new_token
            self.vocab[new_token] = self.vocab[most_frequent_pair[0]] + self.vocab[most_frequent_pair[1]]

            print(f"  åˆå¹¶å: {indices}")

        print(f"âœ… è®­ç»ƒå®Œæˆ! æœ€ç»ˆåºåˆ—: {indices}")
        print(f"ğŸ“Š è¯æ±‡è¡¨å¤§å°: {len(self.vocab)}, åˆå¹¶è§„åˆ™æ•°: {len(self.merges)}")

    def _count_pairs(self, indices: List[int]) -> Dict[Tuple[int, int], int]:
        """ç»Ÿè®¡ç›¸é‚»tokenå¯¹é¢‘ç‡"""
        counts = defaultdict(int)
        for i in range(len(indices) - 1):
            pair = (indices[i], indices[i + 1])
            counts[pair] += 1
        return counts

    def _merge(self, indices: List[int], pair: Tuple[int, int], new_token: int) -> List[int]:
        """åˆå¹¶æŒ‡å®šçš„tokenå¯¹"""
        new_indices = []
        i = 0
        while i < len(indices):
            if (i + 1 < len(indices) and
                indices[i] == pair[0] and
                indices[i + 1] == pair[1]):
                new_indices.append(new_token)
                i += 2
            else:
                new_indices.append(indices[i])
                i += 1
        return new_indices

    def encode(self, text: str) -> List[int]:
        """ç¼–ç æ–‡æœ¬ä¸ºtokenåºåˆ—"""
        # 1. è½¬æ¢ä¸ºå­—èŠ‚åºåˆ—
        indices = list(map(int, text.encode("utf-8")))

        # 2. åº”ç”¨æ‰€æœ‰åˆå¹¶è§„åˆ™
        for pair, new_token in self.merges.items():
            indices = self._merge(indices, pair, new_token)

        return indices

    def decode(self, indices: List[int]) -> str:
        """è§£ç tokenåºåˆ—ä¸ºæ–‡æœ¬"""
        # 1. å°†tokenè½¬æ¢ä¸ºå­—èŠ‚
        bytes_list = [self.vocab[idx] for idx in indices]

        # 2. è§£ç ä¸ºå­—ç¬¦ä¸²
        return b"".join(bytes_list).decode("utf-8")

    def print_vocab(self):
        """æ‰“å°è¯æ±‡è¡¨"""
        print("\nğŸ“š è¯æ±‡è¡¨:")
        print("Token | Bytes")
        print("-" * 30)

        # åªæ˜¾ç¤ºæ–°åˆ›å»ºçš„tokens
        for token in sorted(self.vocab.keys()):
            if token >= 256:  # æ–°åˆ›å»ºçš„tokens
                print(f"{token:5d} | {self.vocab[token]!r}")

    def print_merges(self):
        """æ‰“å°åˆå¹¶è§„åˆ™"""
        print("\nğŸ”— åˆå¹¶è§„åˆ™:")
        print("æ­¥éª¤ | åˆå¹¶å¯¹      â†’ æ–°Token | Bytes")
        print("-" * 50)

        for i, (pair, new_token) in enumerate(self.merges.items()):
            bytes_repr = f"{self.vocab[pair[0]]!r}+{self.vocab[pair[1]]!r}"
            print(f"{i+1:4d} | [{pair[0]}]+[{pair[1]:3d}]   â†’ [{new_token:3d}]   | {bytes_repr}")


def simple_example():
    """ç®€å•ç¤ºä¾‹ï¼šaaabdaaabac"""
    print("ğŸ¯ ç¤ºä¾‹1: aaabdaaabac")
    print("-" * 50)

    tokenizer = BPETokenizer()
    tokenizer.train("aaabdaaabac", num_merges=5)

    print("\nğŸ“‹ è®­ç»ƒç»“æœ:")
    tokenizer.print_merges()
    tokenizer.print_vocab()

    # æµ‹è¯•ç¼–ç 
    test_text = "aaaa"
    encoded = tokenizer.encode(test_text)
    decoded = tokenizer.decode(encoded)

    print(f"\nğŸ” æµ‹è¯•ç¼–ç : '{test_text}'")
    print(f"   ç¼–ç : {encoded}")
    print(f"   è§£ç : '{decoded}'")
    print(f"   âœ… æˆåŠŸ: {test_text == decoded}")


def english_example():
    """è‹±æ–‡ç¤ºä¾‹ï¼šhello world hello"""
    print("\nğŸ¯ ç¤ºä¾‹2: hello world hello")
    print("-" * 50)

    tokenizer = BPETokenizer()
    tokenizer.train("hello world hello", num_merges=8)

    print("\nğŸ“‹ è®­ç»ƒç»“æœ:")
    tokenizer.print_merges()

    # æµ‹è¯•ç¼–ç 
    test_text = "hello"
    encoded = tokenizer.encode(test_text)
    decoded = tokenizer.decode(encoded)

    print(f"\nğŸ” æµ‹è¯•ç¼–ç : '{test_text}'")
    print(f"   ç¼–ç : {encoded}")
    print(f"   è§£ç : '{decoded}'")
    print(f"   âœ… æˆåŠŸ: {test_text == decoded}")


def chinese_example():
    """ä¸­æ–‡ç¤ºä¾‹ï¼šä½ å¥½ä¸–ç•Œ"""
    print("\nğŸ¯ ç¤ºä¾‹3: ä½ å¥½ä¸–ç•Œ")
    print("-" * 50)

    tokenizer = BPETokenizer()
    tokenizer.train("ä½ å¥½ä¸–ç•Œ", num_merges=6)

    print("\nğŸ“‹ è®­ç»ƒç»“æœ:")
    tokenizer.print_merges()
    tokenizer.print_vocab()

    # æµ‹è¯•ç¼–ç 
    test_text = "ä½ å¥½"
    encoded = tokenizer.encode(test_text)
    decoded = tokenizer.decode(encoded)

    print(f"\nğŸ” æµ‹è¯•ç¼–ç : '{test_text}'")
    print(f"   ç¼–ç : {encoded}")
    print(f"   è§£ç : '{decoded}'")
    print(f"   âœ… æˆåŠŸ: {test_text == decoded}")


def compression_analysis():
    """å‹ç¼©æ•ˆæœåˆ†æ"""
    print("\nğŸ“Š å‹ç¼©æ•ˆæœåˆ†æ")
    print("-" * 50)

    texts = [
        "aaaaaa",           # é‡å¤å­—ç¬¦
        "hello world",      # è‹±æ–‡
        "ä½ å¥½ä¸–ç•Œä½ å¥½",      # ä¸­æ–‡
        "abcabcabc",        # é‡å¤æ¨¡å¼
    ]

    for text in texts:
        print(f"\nğŸ“ æ–‡æœ¬: '{text}'")

        # åŸå§‹å­—èŠ‚é•¿åº¦
        original_bytes = len(text.encode("utf-8"))
        print(f"   åŸå§‹å­—èŠ‚: {original_bytes}")

        # è®­ç»ƒBPE
        tokenizer = BPETokenizer()
        tokenizer.train(text, num_merges=min(10, original_bytes // 2))

        # ç¼–ç åé•¿åº¦
        encoded = tokenizer.encode(text)
        compressed_length = len(encoded)
        print(f"   BPEç¼–ç : {compressed_length}")

        # å‹ç¼©æ¯”
        compression_ratio = original_bytes / compressed_length
        print(f"   å‹ç¼©æ¯”: {compression_ratio:.3f}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ BPEæ ¸å¿ƒç®—æ³•æ¼”ç¤º")
    print("=" * 60)

    # è¿è¡Œç¤ºä¾‹
    simple_example()
    english_example()
    chinese_example()
    compression_analysis()

    print("\n" + "=" * 60)
    print("âœ… æ¼”ç¤ºå®Œæˆ!")
    print("\nğŸ’¡ å…³é”®æ´å¯Ÿ:")
    print("1. BPEä»å­—èŠ‚å¼€å§‹ï¼Œé€æ­¥åˆå¹¶é«˜é¢‘å¯¹")
    print("2. å¸¸è§æ¨¡å¼è¢«å‹ç¼©ä¸ºå•ä¸ªtoken")
    print("3. å‹ç¼©æ•ˆæœå–å†³äºæ–‡æœ¬çš„é‡å¤æ¨¡å¼")
    print("4. è¯æ±‡è¡¨å¤§å°å¯é€šè¿‡åˆå¹¶æ¬¡æ•°æ§åˆ¶")


if __name__ == "__main__":
    main()