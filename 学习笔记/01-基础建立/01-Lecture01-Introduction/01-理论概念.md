# Lecture 01: Introduction & Tokenization - 理论概念

## 📚 核心概念

### 🎯 课程存在的意义

#### 问题背景
- **研究者与技术的脱离**: 现在的研究者越来越脱离底层技术
- **历史演进**:
  - 8年前: 研究者自己实现和训练模型
  - 6年前: 研究者下载预训练模型(如BERT)进行微调
  - 现在: 研究者只是调用专有模型的API(如GPT-4/Claude/Gemini)

#### 抽象层次的问题
- **抽象是泄漏的**: 与编程语言或操作系统不同，LLM的抽象层次存在泄漏
- **基础研究需求**: 仍然需要深入底层进行基础研究
- **完整理解必要性**: 基础研究需要对技术的完整理解

### 🏭 语言模型的工业化

#### 规模的挑战
- **GPT-4**: 1.8万亿参数，训练成本1亿美元
- **xAI**: 20万个H100 GPU集群训练Grok
- **Stargate项目**: OpenAI、NVIDIA、Oracle投资5000亿美元

#### 透明度问题
- **缺乏公开细节**: 前沿模型的构建细节没有公开
- **技术报告限制**: GPT-4技术报告缺乏实现细节

### 📈 "More is Different"现象

#### 规模带来的变化
- **计算分配变化**: 注意力与MLP的FLOPs比例随规模变化
- **行为涌现**: 随着规模增大，新的能力会涌现

#### 可转移的知识类型
1. **Mechanics (机制)**: 事物如何工作
   - Transformer是什么
   - 模型并行如何利用GPU
2. **Mindset (思维模式)**:
   - 充分利用硬件
   - 认真对待规模问题
   - 扩展法则思维
3. **Intuitions (直觉)**:
   - 数据和建模决策如何产生好的准确性
   - 部分可转移，不一定跨规模适用

### 🍂 "The Bitter Lesson"理念

#### 核心观点
- **错误解读**: 规模就是一切，算法不重要
- **正确解读**: 能够扩展的算法才是重要的

#### 基本公式
```
accuracy = efficiency × resources
```

#### 效率的重要性
- **大规模下效率更重要**: 无法承受浪费
- **算法效率提升**: ImageNet上2012-2019年算法效率提升44倍
- **核心问题**: 给定计算和数据预算，如何构建最佳模型？

### 📊 语言模型发展历史

#### 前神经网络时代 (2010年代之前)
- **Shannon (1950)**: 测量英语熵的语言模型
- **N-gram语言模型**: 用于机器翻译、语音识别

#### 神经网络组件 (2010年代)
- **Bengio et al. (2003)**: 第一个神经语言模型
- **Sutskever et al. (2014)**: 序列到序列建模
- **Adam优化器 (2014)**: 现代优化算法基础
- **Bahdanau et al. (2015)**: 注意力机制
- **Vaswani et al. (2017)**: Transformer架构
- **Shazeer et al. (2017)**: 混合专家模型
- **模型并行 (2018-2019)**: GPipe, Zero, Megatron-LM

#### 早期基础模型 (2010年代末)
- **ELMo**: LSTM预训练，微调提升任务性能
- **BERT**: Transformer预训练，微调提升任务性能
- **T5 (11B)**: 将所有任务转换为文本到文本格式

#### 拥抱扩展，更加封闭 (2020年代初)
- **GPT-2 (1.5B)**: 流畅文本生成，零样本能力初步迹象
- **扩展法则**: 为扩展提供希望/可预测性
- **GPT-3 (175B)**: 上下文学习，封闭模型
- **PaLM (540B)**: 大规模，训练不足
- **Chinchilla (70B)**: 计算最优扩展法则

#### 开放模型时代
- **EleutherAI**: 开放数据集(The Pile)和模型(GPT-J)
- **Meta OPT (175B)**: GPT-3复现，硬件问题众多
- **BLOOM**: 专注于数据源
- **Llama系列**: Meta的开源模型
- **Qwen系列**: 阿里巴巴的模型
- **DeepSeek系列**: 高性能开源模型
- **OLMo 2**: AI2的完全开源模型

#### 开放程度分类
1. **封闭模型** (如GPT-4o): 仅API访问
2. **开放权重模型** (如DeepSeek): 权重可用，架构细节论文，部分训练细节，无数据细节
3. **开源模型** (如OLMo): 权重和数据可用，大部分细节公开

#### 当今前沿模型 (2025年)
- OpenAI o3
- Anthropic Claude Sonnet 3.7
- xAI Grok 3
- Google Gemini 2.5
- Meta Llama 3.3
- DeepSeek r1
- Alibaba Qwen 2.5 Max
- Tencent Hunyuan-T1

## 🎓 课程特色

### 可执行讲座 (Executable Lectures)
- **定义**: 程序执行即可传递讲座内容
- **优势**:
  - 查看和运行代码
  - 看到讲座的层次结构
  - 跳转到定义和概念

### 课程物流
- **学分**: 5学分课程
- **工作量**: 作业量相当于CS224n所有作业加期末项目
- **计算资源**: Together AI提供计算集群

### 适合人群
#### 应该选修的原因
- 有强迫性需求理解事物如何工作
- 想要建立研究工程能力

#### 不应该选修的原因
- 本季度想完成研究工作
- 对AI最新技术(多模态、RAG等)感兴趣
- 想要在自己应用领域获得好结果

## 🔧 课程核心组件

### 效率驱动
- **资源**: 数据 + 硬件 (计算、内存、通信带宽)
- **核心问题**: 给定固定资源集，如何训练最佳模型？

### 设计决策维度
- 数据处理
- Tokenization
- 模型架构
- 训练策略
- 扩展法则
- 对齐技术

### 课程结构
1. **基础 (Basics)**: 基本版本的全流程
2. **系统 (Systems)**: 充分利用硬件
3. **扩展法则 (Scaling Laws)**: 小规模实验预测大规模
4. **数据 (Data)**: 数据策划和处理
5. **对齐 (Alignment)**: 使模型有用

## 💡 关键洞察

### 效率思维
- **当前约束**: 计算受限
- **设计反映**: 挤压给定硬件的最大性能
- **未来变化**: 明天将变成数据受限

### 实践哲学
- **从零开始**: 通过构建来理解
- **效率优先**: 在每个决策中都要考虑效率
- **规模意识**: 即使在小模型上也要保持大规模思维

---

**📝 备注**: 这是CS336课程的开篇，建立了整个课程的理念框架和历史背景。理解这些核心概念对于后续学习至关重要。